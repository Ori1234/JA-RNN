{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ori1234/JA-RNN/blob/master/Thesis_8_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY",
        "colab_type": "text"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i)\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window.\n",
        "\n",
        "\n",
        "Note to self: Make sure you dont run anything for more than 12 hrs on Colab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE",
        "colab_type": "text"
      },
      "source": [
        "**SUMMERY**\n",
        "\n",
        "say somthing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTdNSHUmCvD",
        "colab_type": "text"
      },
      "source": [
        "#IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"IMPORTS\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import math\n",
        "from termcolor import colored, cprint\n",
        "import editdistance\n",
        "\n",
        "\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrDNbLhC7H",
        "colab_type": "text"
      },
      "source": [
        "#GLOBAL VARS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fPaxl8g_BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "this_time=str(datetime.now())\n",
        "GLOBAL_epoch=0\n",
        "DEBUG=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AndYz46gsER",
        "colab_type": "text"
      },
      "source": [
        "#LOGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldvhaPtVF0Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######To clean logs\n",
        "######!rm log*.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxiE-PU7A-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "566c28ff-9be5-4919-aa5f-f255647fafcc"
      },
      "source": [
        "CELL_NAME=\"START LOG\"\n",
        "\n",
        "\n",
        "log_file=\"LOG___\"+this_time+\".txt\"\n",
        "f_logg= open(log_file,\"w\")\n",
        "print(\"logging to file (will be added to mail)\",log_file)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _print_log(also_print,*txts):  \n",
        "  txt=\"\"\n",
        "  for t in txts:\n",
        "    txt+=\" \"+str(t)\n",
        "  f_logg.write(CELL_NAME+\" \"+str(datetime.now())+\": \"+txt+'\\n') #TODO ADD TIME!!!!\n",
        "  if also_print:\n",
        "    print(*txts)\n",
        "\n",
        "def print_log(*txts):\n",
        "  if (DEBUG):\n",
        "    print(*txts)\n",
        "  _print_log(False,*txts)\n",
        "\n",
        "\n",
        "def print_log_screen(*txts):\n",
        "  _print_log(True,*txts)\n",
        "\n",
        "\n",
        "def log_flush():\n",
        "  f_logg.flush()\n",
        "\n",
        "def close_log():\n",
        "  f_logg.close()\n",
        "\n",
        "\n",
        "\n",
        "#DON'T FORGET TO CLOSE THE FILE AT THE END\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logging to file (will be added to mail) LOG___2020-05-13 14:27:12.030576.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh93f3m4fb3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37d08878-5420-4681-88f1-aa2b550cebc6"
      },
      "source": [
        "\n",
        "#PAIST HERE TO SEE PROPERLY THE SHOW DIFF PART IN LOGS\n",
        "text='''MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
        "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
        "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
        "MAIN:  LER (label error rate):  0.033400332030791034'''\n",
        "print(text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
            "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
            "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
            "MAIN:  LER (label error rate):  0.033400332030791034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp",
        "colab_type": "text"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0637e118-764e-4def-e928-c0e3bd646f21"
      },
      "source": [
        "CELL_NAME=\"MOUNT DRIVE\"\n",
        "print(\"mounting to drive at /gdrive\")\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mounting to drive at /gdrive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY",
        "colab_type": "text"
      },
      "source": [
        "#MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cf669160-fbee-4f0d-efa1-6b6831185e7a"
      },
      "source": [
        "CELL_NAME=\"BATCH_SIZE and STATEFUL\"\n",
        "\n",
        "STATEFUL=False\n",
        "BATCH_SIZE = 128\n",
        "TO_SHUFFLE=True\n",
        "\n",
        "embedding_dim = 8\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "print_log_screen(\"set batch size to \"+str(BATCH_SIZE))\n",
        "print_log_screen(\"STATEFUL: \"+str(STATEFUL))\n",
        "print_log_screen(\"embedding_dim: \"+str(embedding_dim))\n",
        "print_log_screen(\"rnn_units: \"+str(rnn_units))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set batch size to 128\n",
            "STATEFUL: False\n",
            "embedding_dim: 8\n",
            "rnn_units: 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw",
        "colab_type": "text"
      },
      "source": [
        "# INIT RANDOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "\n",
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "def init_random():\n",
        "  print_log_screen(\"init random to 1\")\n",
        "  np.random.seed(1)\n",
        "  tf.compat.v1.set_random_seed(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUbUyZx3i1OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57c4b2bf-d0f8-4472-dc8b-ee0cc85017bf"
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "init_random()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init random to 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g",
        "colab_type": "text"
      },
      "source": [
        "#CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n",
        "\n",
        "LTRchar=u'\\u202B'   #align rtl symbole\n",
        "BLANK=\"_\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK",
        "colab_type": "text"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXctuG0ZIwCT",
        "colab_type": "text"
      },
      "source": [
        "##show diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF show diff\"\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "# #USEAGE:\n",
        "# s1=\"لامة النصارى واستحقو\" \n",
        "# s2=\"لأمّة النصارى واستحقوّا\"\n",
        "# a,b=show_diff(s1,s2,'blue')\n",
        "# #print_log(\"\".join(b))\n",
        "# print_log(a,\"|\",b)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF",
        "colab_type": "text"
      },
      "source": [
        "##send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"SEND MAIL\"\n",
        "\n",
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  files=[log_file,\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP",
        "colab_type": "text"
      },
      "source": [
        "##plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PLOT\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "#my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "##letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LETTER MAPPING\"\n",
        "\n",
        "tag=\"'\"\n",
        "\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"+tag\n",
        "\n",
        "#\"א\": \"اإآٱأ\", with wasla\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآٱأ\",\n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "#######################################################\n",
        "for c in additional_letters:\n",
        "  letter_dict[c]=c\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print_log(\"arab_heb_maping\",arab_heb_maping)\n",
        "print_log(\"length:\",len(arab_heb_maping))\n",
        "print_log(\"heb_arab_maping\",heb_arab_maping)\n",
        "print_log(\"length:\",len(heb_arab_maping))\n",
        "\n",
        "#################################################################3\n",
        "#FUNCTIONS:\n",
        "\n",
        "def remove_chars_not_in_map(phrase,map):\n",
        "  res=[]\n",
        "  for c in phrase:  \n",
        "    if c in map:\n",
        "      res.append(c)\n",
        "    else:\n",
        "      print_log(LTRchar+\"Skipping char not in predefined map\\n ( \"+c+\" )\\nin sentences:\\n\"+phrase+'\\n')      \n",
        "  return \"\".join(res)\n",
        "\n",
        "def remove_chars_not_in_JA_map(ja):\n",
        "  return remove_chars_not_in_map(ja,heb_arab_maping)\n",
        "\n",
        "extended_arab_chars=list(arab_heb_maping.keys())\n",
        "extended_arab_chars+=tanween\n",
        "extended_arab_chars.append(shada)\n",
        "extended_arab_chars.append(hamza_on_line)\n",
        "\n",
        "def remove_chars_not_in_arab_map(arr):\n",
        "  return remove_chars_not_in_map(arr,extended_arab_chars)\n",
        "\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n",
        "\n",
        "\n",
        "def reverse_simple_map(arr_str):\n",
        "  ja_str=[]\n",
        "  for c in arr_str:\n",
        "    if c in arab_heb_maping:\n",
        "      ja_str.append(arab_heb_maping[c])\n",
        "    #else:\n",
        "      #print_log(\"reverse_simple_map: char ( \"+c+\" ) not in letter mapping and will be skiped\")\n",
        "      #print_log(arr_str)\n",
        "  return \"\".join(ja_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtRTahEBmLU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DATA PATHS\"\n",
        "\n",
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "kfir_kuzari_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_ORIGINAL.txt\"\n",
        "kfir_rasag_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_ORIGINAL.txt\"\n",
        "kfir_kuzari_test_SWITCH=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\"\n",
        "kfir_rasag_test_SWITCH=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31",
        "colab_type": "text"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"HELPERS\"\n",
        "\n",
        "##HELPERS\n",
        "\n",
        "\n",
        "# def view_data(data):\n",
        "#   for i,j,l1,l2 in data.take(3):\n",
        "#     print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[0],l1[0])),\" | \",decode_arr(j[0],l2[0]))\n",
        "\n",
        "def view_data(data):\n",
        "  print_log(\"=\"*200)\n",
        "  for i,j,l1,l2 in data.take(1):\n",
        "    for t in range(5):\n",
        "      print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[t],l1[t])),\" | \",decode_arr(j[t],l2[t]))\n",
        "  print_log(\"=\"*200)\n",
        "\n",
        "\n",
        "def clear_blank(s):\n",
        "  return s.replace(BLANK,\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PREPROCESS SENTENCES\"\n",
        "\n",
        "\n",
        "def normalize_unicode(s):\n",
        "    s = s.strip()\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "#ARABIC:\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def replace_arab_style_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "#CHECK\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "#JUDEO-ARABIC\n",
        "# def preprocess_JA(w):\n",
        "#     w = normalize_unicode(w.strip())\n",
        "#     w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "#     w = w.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "#     return w\n",
        "\n",
        "\n",
        "def dropout(ja_str,keep):\n",
        "  res=[]\n",
        "  for c in ja_str:\n",
        "    if c==\" \":\n",
        "      res.append(c)\n",
        "    elif (np.random.binomial(1,keep)):\n",
        "      res.append(c)\n",
        "    else:\n",
        "      res.append(BLANK)\n",
        "  return \"\".join(res)\n",
        "\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "\n",
        "def undouble_hebrew(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq",
        "colab_type": "text"
      },
      "source": [
        "##languageIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print_log(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print_log(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print_log(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print_log(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Ch9lK2o-Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "# inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys())) this was a bug!!!!\n",
        "# targ_lang = LanguageIndex(\"\".join(extended_arab_chars))\n",
        "inp_lang = LanguageIndex(heb_arab_maping.keys())\n",
        "targ_lang = LanguageIndex(extended_arab_chars)\n",
        "\n",
        "\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print_log(len(idx))\n",
        "      if leng==-1:\n",
        "       # print_log(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "      return result\n",
        "\n",
        "def decode_JA(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,inp_lang.idx2char,leng)\n",
        "\n",
        "def decode_arr(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,targ_lang.idx2char,leng)\n",
        "\n",
        "\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print_log_screen(\"VECTORIZE: char not in dict - need to call preprocess_lines before calling produce_dataset \")      \n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def encode_JA(ja):\n",
        "  return vectorize(ja,inp_lang.char2idx)\n",
        "\n",
        "def encode_arr(arr):\n",
        "  return vectorize(arr,targ_lang.char2idx)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7wKGxCzxuE",
        "colab_type": "text"
      },
      "source": [
        "##FUCNTION FOR GEN DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xjn9m6nyHv7",
        "colab_type": "text"
      },
      "source": [
        "###SPLIT TEST TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62bZRaH7L8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST TRAIN SPLIT\"\n",
        "\n",
        "def split_test_train(input_file,output_train,output_test,test_percent=0.2):\n",
        "  with open(input_file, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')  \n",
        "  text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  test_size=math.floor(num_of_lines*test_percent)\n",
        "  idx=[0]*test_size + [1]*(num_of_lines-test_size)\n",
        "  random.shuffle(idx)\n",
        "  print(idx)\n",
        "  \n",
        "  f_train=open(output_train,'w+')\n",
        "  f_test=open(output_test,'w+')\n",
        "  \n",
        "  train = []\n",
        "  test = []\n",
        "  for a,i in zip(lines,idx):\n",
        "    if i:\n",
        "      train.append(a)\n",
        "      f_train.write(a+'\\n')\n",
        "    else:\n",
        "      test.append(a)\n",
        "      f_test.write(a+'\\n')\n",
        "  \n",
        "  f_train.close()\n",
        "  f_test.close()\n",
        "\n",
        "  print(len(train),train[0:10])\n",
        "  print(len(test),test[0:10])\n",
        "\n",
        "#split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58vNbHqdTly_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SHOLD ONLY BE CALLED TO RESPLIT TEST AND TRAIN\n",
        "#split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")\n",
        "#split_test_train(haemunot,haemunot+\".train.txt\",haemunot+\".test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_YqhOHyNUs",
        "colab_type": "text"
      },
      "source": [
        "###load_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LOAD_LINES\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  print_log_screen(\"loading text: \"+input_file)\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    #text=text.replace('ֿ',\"'\")   ##allread doing it inside preprocess_JA()\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  print_log(\"first lines:\",lines[0:100])\n",
        "  print_log_screen(\"len(lines)\", len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyI8c-qU-DEe",
        "colab_type": "text"
      },
      "source": [
        "###preprocess_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro14w9OP-Dt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_JA(ja):\n",
        "    ja=replace_arab_style_punctuation(ja)\n",
        "    ja=clear_blank(ja) #acctually should be taken care of by remove_chars_not_in_map ...\n",
        "    ja = normalize_unicode(ja.strip())\n",
        "    ja = ja.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    ja = ja.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "    ja=remove_chars_not_in_JA_map(ja)\n",
        "    return ja\n",
        "\n",
        "def preprocess_arr(arr):\n",
        "    arr=remove_arab_nikud(arr)\n",
        "    arr=standard_nunization(arr)\n",
        "    arr=replace_arab_style_punctuation(arr)\n",
        "    arr=clear_blank(arr) #acctually should be taken care of by remove_chars_not_in_map ...\n",
        "    arr=normalize_unicode(arr)     \n",
        "    arr=remove_chars_not_in_arab_map(arr)\n",
        "    return arr\n",
        "\n",
        "def preprocess_lines(ja_arr_lines):\n",
        "  res=[]\n",
        "  for l in ja_arr_lines:\n",
        "    ja,arr=l.split('\\t')\n",
        "\n",
        "    ja=preprocess_JA(ja)\n",
        "      \n",
        "    arr=preprocess_arr(arr)\n",
        "    \n",
        "    res.append(ja+'\\t'+arr)\n",
        "  return res\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jWF3taWySyb",
        "colab_type": "text"
      },
      "source": [
        "###create_parralele_phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvF0XWGTzZ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Takes a file of <heb, arab> phrases separated by tab\n",
        "# Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_parralel_phrases(lines,keep=1):\n",
        "    phrase_pairs=[]\n",
        "    for l in lines:\n",
        "      heb,arr=l.split('\\t')\n",
        "      \n",
        "      # heb=clear_blank(replace_arab_style_punctuation(heb)) #TODO needed?\n",
        "      # heb=preprocess_JA(heb)\n",
        "      heb=dropout(heb,keep)\n",
        "      heb=double_hebrew(heb)      \n",
        "\n",
        "      # arr=remove_arab_nikud(arr)\n",
        "      # arr=standard_nunization(arr)\n",
        "      # arr=clear_blank(replace_arab_style_punctuation(arr))\n",
        "      # arr=normalize_unicode(arr) \n",
        "      \n",
        "      phrase_pairs.append([heb,arr])        \n",
        "    return phrase_pairs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT",
        "colab_type": "text"
      },
      "source": [
        "###produce_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuiTkrWRLYG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"produce_dataset\"\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def produce_dataset(parallel_phrase,to_shuffle=False):\n",
        "  \n",
        "    input_tensor = [encode_JA(heb) for heb, arr in parallel_phrase]\n",
        "    input_lengths=[len(heb) for heb,arr in parallel_phrase]\n",
        "  \n",
        "    target_tensor = [encode_arr(arr) for heb, arr in parallel_phrase]\n",
        "    target_lengths = [len(arr)  for heb,arr in parallel_phrase]\n",
        "  \n",
        "\n",
        "    print_log(\"VECTORIZE EXAMPLE\")\n",
        "    print_log(LTRchar,parallel_phrase[0])\n",
        "    print_log(input_lengths[0])\n",
        "    print_log(target_lengths[0])\n",
        "    print_log(input_tensor[0])\n",
        "    print_log(target_tensor[0])\n",
        "    print_log(\"\\n\")\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                 value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    print_log(len(input_tensor), \n",
        "        len(target_tensor))      \n",
        "    BUFFER_SIZE = len(input_tensor)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, \n",
        "                                                  target_tensor,\n",
        "                                                  input_lengths,\n",
        "                                                  target_lengths))\n",
        "    if to_shuffle:\n",
        "      dataset=dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "                                                  \n",
        "    dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset_double\n",
        "    \n",
        "    return dataset_double\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF create_data_tensors\"\n",
        "\n",
        "# def max_length(tensor):\n",
        "#     return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# def create_data_tensors(pairs):\n",
        "  \n",
        "#     input_tensor = [encode_JA(heb) for heb, arr in pairs]\n",
        "#     input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "  \n",
        "#     target_tensor = [encode_arr(arr) for heb, arr in pairs]\n",
        "#     target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "  \n",
        "\n",
        "#     print_log()\n",
        "#     print_log(LTRchar,pairs[0])\n",
        "#     print_log(input_lenghts[0])\n",
        "#     print_log(target_lengths[0])\n",
        "#     print_log(input_tensor[0])\n",
        "#     print_log(target_tensor[0])\n",
        "\n",
        "#     # Padding the input and output tensor to the maximum length\n",
        "#     max=max_length(input_tensor)\n",
        "#     if max>70:\n",
        "#       max=70\n",
        "#     input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "#                                                                  maxlen=max,\n",
        "#                                                                  padding='post',\n",
        "#                                                                   value=inp_lang.char2idx[BLANK])\n",
        "#     max=max_length(target_tensor)\n",
        "#     if max>70:\n",
        "#       max=70    \n",
        "#     target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "#                                                                   maxlen=max,\n",
        "#                                                                   padding='post',\n",
        "#                                                                   value=targ_lang.char2idx[BLANK])\n",
        "#     return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN",
        "colab_type": "text"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF GEN DATA\"\n",
        "\n",
        "# def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths,_test_size=0.2):  \n",
        "#   input_tensor_train, input_tensor_val, \\\n",
        "#   target_tensor_train, target_tensor_val, \\\n",
        "#   input_lengths_train, input_lengths_val, \\\n",
        "#   target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "#                                                               target_tensor,\n",
        "#                                                               input_lenghts,\n",
        "#                                                               target_lengths, test_size=_test_size)\n",
        "  \n",
        "#   print_log(len(input_tensor_train), \n",
        "#         len(target_tensor_train), \n",
        "#         len(input_tensor_val), \n",
        "#         len(target_tensor_val))\n",
        "  \n",
        "#   BUFFER_SIZE = len(input_tensor_train)\n",
        "  \n",
        "#   dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "#                                                 target_tensor_train,\n",
        "#                                                 input_lengths_train,\n",
        "#                                                 target_lengths_train)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "#   test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "#                                                     target_tensor_val,\n",
        "#                                                     input_lengths_val,\n",
        "#                                                     target_lengths_val)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)                                                  \n",
        "  \n",
        "#   dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   return dataset_double,test_dataset_double\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0elYUMb8Ls",
        "colab_type": "text"
      },
      "source": [
        "##activate gen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izFnzJAMynm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e689528a-8908-4778-ca5f-4e827a4719fd"
      },
      "source": [
        "CELL_NAME=\"LOAD DATASET NEW\"\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_train=preprocess_lines(\n",
        "    load_lines(hakuzari+\".train.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "train_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_train,1),to_shuffle=TO_SHUFFLE)\n",
        "\n",
        "print_log_screen(\"train_dataset_double_kuzari\")\n",
        "view_data(train_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_test=preprocess_lines(\n",
        "    load_lines(hakuzari+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_kuzari\")\n",
        "view_data(test_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "rasag_lines_test=preprocess_lines(\n",
        "    load_lines(haemunot+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_rasag= produce_dataset(create_parralel_phrases(rasag_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_rasag\")\n",
        "view_data(test_dataset_double_rasag)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.train.txt\n",
            "len(lines) 8739\n",
            "train_dataset_double_kuzari\n",
            "‫ אעט'מ מנ אעתקאדהמ אלחד'ת  |  أعظم من اعتقادهم الحدث\n",
            "‫ אלי הד'ה אלדרג'ה אלרוחאניה  |  إلى هذه الدرجة الروحانية\n",
            "‫ אלאלאהי אלד'י ראמ קרבה  |  الإلهيّ الذي رام قربه\n",
            "‫ . ולא אלתקלל מנ אלמאל  |  . ولا التقلّل من المال\n",
            "‫ , ותלתד' ברויה' אלנור  |  , وتلتذّ برؤية النور\n",
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.test.txt\n",
            "len(lines) 2184\n",
            "test_dataset_double_kuzari\n",
            "‫ ואהל אלאדיאנ ת'מ עלי  |  وأهل الأديان ثمّ على\n",
            "‫ , אלד'י כאנ ענד מלכ אלכ'זר  |  , الذي كان عند ملك الخزر\n",
            "‫ אלדאכ'ל פי דינ אליהוד  |  الداخل في دين اليهود\n",
            "‫ כתאב אלתואריכ' , אנה  |  كتاب التواريخ , أنه\n",
            "‫ תכרר עליה רויא , כאנ  |  تكرّر عليه رؤيا , كأنّ\n",
            "loading text: /gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt.test.txt\n",
            "len(lines) 2071\n",
            "test_dataset_double_rasag\n",
            "‫ באנ קאל תבארכ אללה אלאה  |  بأن قال تبارك الله إله\n",
            "‫ אמא עלי את'ר מא אפתתחנא  |  أمّا على إثر ما افتتحنا\n",
            "‫ פי מטאלבהמ וענ וג'ה זואלהא  |  في مطالبهم وعن وجه زوالها\n",
            "‫ H H H . וארי אנ אג'על  |  H H H . وأرى أن أجعل\n",
            "‫ אלשבה באי סבב כאנ מנ  |  الشبه بأي سبب كان من\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"LOAD DATASET\"\n",
        "\n",
        "\n",
        "# lines=load_lines(hakuzari)\n",
        "# pairs = create_parralel_phrases(lines,1)  \n",
        "# input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "# print_log_screen(\"test_dataset_double_kuzari\")\n",
        "# view_data(test_dataset_double_kuzari)\n",
        "\n",
        "# lines1=load_lines(haemunot)\n",
        "# pairs1 = create_parralel_phrases(lines1,1)  \n",
        "# input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = create_data_tensors(pairs1)\n",
        "# dataset_double_rasag,test_dataset_double_rasag=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "# print_log_screen(\"test_dataset_double_rasag\")\n",
        "# view_data(test_dataset_double_rasag)\n",
        "\n",
        "\n",
        "# # def gen kuzari_drop(lines,keep):\n",
        "# #   pairs = create_parralel_phrases(lines,0.90)  \n",
        "# #   input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# #   dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8",
        "colab_type": "text"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpKhndwM10b0",
        "colab_type": "text"
      },
      "source": [
        "###load_lines_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOU-bSFtpEgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"load_lines_synth\"\n",
        "\n",
        "#TODO edit when time permits\n",
        "\n",
        "def load_lines_synth(file_path=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(file_path, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  text=replace_arab_style_punctuation(text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)    \n",
        "\n",
        "  words=text.split()  \n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      #sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "      sentences.append(\" \".join(res))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "  return sentences\n",
        "\n",
        "# res=load_lines_synth()\n",
        "# print(len(res))\n",
        "#res[-5:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9CAcJjeyB_E",
        "colab_type": "text"
      },
      "source": [
        "###preprocess_synth_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdHcLRT0uhA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_synth_lines(arab_sentences):\n",
        "  res_sentences=[]\n",
        "  for arr in arab_sentences:    \n",
        "    arr=preprocess_arr(arr)\n",
        "    res_sentences.append(arr)\n",
        "  return res_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"GEN SYNTH\"\n",
        "\n",
        "# #TODO edit when time permits\n",
        "\n",
        "# def load_lines_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "#   with open(ibnsina, 'rb') as f:\n",
        "#       ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "#   ibnsina_text=replace_arab_style_punctuation(ibnsina_text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "#   #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "#   #add space before and after punctuation signs\n",
        "#   ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "#   ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "#   return ibnsina_text\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF gen_synth_sentences\"\n",
        "\n",
        "# def gen_synth_sentences(sina_words):\n",
        "#   #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "#   SENTENCE_LIMIT=20\n",
        "#   sentences=[]\n",
        "#   char_count=0\n",
        "#   res=[]\n",
        "#   for w in sina_words:\n",
        "#   #  w=w.rstrip(\" \").strip(\" \")\n",
        "#     char_count+=len(w)+1 #len of word + space afterwards\n",
        "#     res.append(w)\n",
        "#     if char_count>SENTENCE_LIMIT:    \n",
        "#       sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "#       res=[]\n",
        "#       char_count=0\n",
        "#     #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "#   return sentences\n",
        "\n",
        "# # #ACTIVATE\n",
        "# # sentences=gen_synth_sentences(sina_words)\n",
        "# # len(sentences)\n",
        "# # sentences[:5]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AqGe0R1_a_",
        "colab_type": "text"
      },
      "source": [
        "###create_parralel_phrases_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF gen_dropout\"\n",
        "\n",
        "def create_parralel_phrases_synth(sentences,keep=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)           \n",
        "    heb=reverse_simple_map(arr)\n",
        "    heb=dropout(heb,keep)\n",
        "    heb_sentences.append(double_hebrew(heb))\n",
        "\n",
        "  print_log(heb_sentences[:5]) \n",
        "  print_log(arab_setences[:5])\n",
        "\n",
        "  print_log(len(arab_setences))\n",
        "  \n",
        "  return list(zip(heb_sentences,arab_setences))\n",
        "\n",
        "  #input_tensor_synth, target_tensor_synth, \\  \n",
        "  # input_lenghts_synth,target_lengths_synth = create_data_tensors(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # # Show length\n",
        "  # print_log(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  # print_log(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  # BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  # dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "  #                                                     target_tensor_synth,\n",
        "  #                                                     input_lenghts_synth,\n",
        "  #                                                     target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  # dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  # return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF gen_dropout_all\"\n",
        "\n",
        "# def gen_dropout_all(sntcs1,*sentences,keep=1):\n",
        "#   result_dataset=gen_dropout(sntcs1,keep)\n",
        "#   for sntcs in sentences:\n",
        "#     result_dataset.concatenate(gen_dropout(sntcs,keep))\n",
        "#   return result_dataset.shuffle(1000)\n",
        "\n",
        "# #   BUFFER_SIZE=1000 #TODO get size\n",
        "# #   return dataset_double_synt3.concatenate(dataset_double_kuzari).shuffle(BUFFER_SIZE)  ##TODO: this is without dropout. \n",
        "# # #view_data(gen_dropout_all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auU5EkGq5fW9",
        "colab_type": "text"
      },
      "source": [
        "##activate gen synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WTc9QYMawy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "84737c2d-fda6-4ab1-f393-c1b354390ae8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "synth_path0=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"\n",
        "synth_sentences0=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path0)\n",
        ")\n",
        "\n",
        "\n",
        "synth_phrase_pairs0=create_parralel_phrases_synth(synth_sentences0,\n",
        "                                                  0.9)\n",
        "synth_dataset0=produce_dataset(synth_phrase_pairs0,\n",
        "                               to_shuffle=TO_SHUFFLE)\n",
        "view_data(synth_dataset0)\n",
        "\n",
        "#####################3\n",
        "synth_path1=\"/gdrive/My Drive/JUDEO-ARAB/daruri-IR.txt\"\n",
        "synth_sentences1=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path1)\n",
        ")\n",
        "synth_phrase_pairs1=create_parralel_phrases_synth(synth_sentences1,\n",
        "                                                  0.9)\n",
        "synth_dataset1=produce_dataset(synth_phrase_pairs1,\n",
        "                               to_shuffle=TO_SHUFFLE)\n",
        "view_data(synth_dataset1)\n",
        "\n",
        "\n",
        "#######################33\n",
        "synth_path2=\"/gdrive/My Drive/JUDEO-ARAB/farabi-tahsil.txt\"\n",
        "synth_sentences2=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path2)\n",
        ")\n",
        "synth_phrase_pairs2=create_parralel_phrases_synth(synth_sentences2,\n",
        "                                                  0.9)\n",
        "synth_dataset2=produce_dataset(synth_phrase_pairs2,\n",
        "                               to_shuffle=TO_SHUFFLE)\n",
        "view_data(synth_dataset2)\n",
        "\n",
        "##########################3\n",
        "synth_path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "synth_sentences3=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path3)\n",
        ")\n",
        "\n",
        "synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                  0.9)\n",
        "synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‫ _א_ת וו_דה' לחקת _ פתכונ  |  كانت ووحدة لحقت , فتكون\n",
            "‫ ת_ונ וחדה' אלת'נאייה' גיר  |  تكون وحدة الثنائية غير\n",
            "‫ _חרכה' או אתצאל פיכונ  |  بحركة أو اتصال فيكون\n",
            "‫ אלאמכאנ וח_יקתה , והו  |  الإمكان وحقيقته , وهو\n",
            "‫ ענה כת'רה' מתפקה' אלנו_  |  عنه كثرة متفقة النوع\n",
            "‫ מעהמ __'א אלכ'לאפ לאנה_  |  معهم هذا الخلاف لأنهم\n",
            "‫ ומנהא חאלה' א__ריצ' ו_למסאפר  |  ومنها حالة المريض والمسافر\n",
            "‫ אלי שהר או שה__נ פ_'איז  |  إلى شهر أو شهرين فجائز\n",
            "‫ איה' פי אלתמסכ באלא_'מאע  |  آية في التمسك بالإجماع\n",
            "‫ , פאנ ד'לכ ממתנ_ . 254  |  , فإن ذلك ممتنع . 254\n",
            "‫ אלת_לימ פי הד_א אלג'נס  |  التعليم في هذا الجنس\n",
            "‫ מא יל_מסה מנ אעטא שי  |  ما يلتمسه من إعطاء شيء\n",
            "‫ נפוס _למד_יינ ותד'_ ותרק  |  نفوس المدنيين وتذل وترق\n",
            "‫ אלעלמ אלאנסא_י ואלעל_  |  العلم الإنساني والعلم\n",
            "‫ איצ'א מבד_ ותכונ לה אשיא  |  أيضا مبدأ وتكون له أشياء\n",
            "‫ כתאב רסאלה' אלחרופ ללפילסופ  |  كتاب رسالة الحروف للفيلسوف\n",
            "‫ אבי נצר אלפאראבי בסמ  |  أبي نصر الفارابيّ بسم\n",
            "‫ אללה אלרחמנ אלרחי_ ובה  |  الله الرحمن الرحيم وبه\n",
            "‫ נס_עינ אלחמד ללה _ב  |  نستعين الحمد لله ربّ\n",
            "‫ _לעאלמינ ו_לסל_מ עלי  |  العالمين والسلام على\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT4q_5EwyowP",
        "colab_type": "text"
      },
      "source": [
        "##gen_all_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbugLUuw2Ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "69a34904-03ed-435d-cee7-93f68dca4c1b"
      },
      "source": [
        "CELL_NAME=\"GEN_ALL_SYNTH\"\n",
        "import glob\n",
        "\n",
        "#all_synth_lines=synth_sentences0+synth_sentences1+synth_sentences2+synth_sentences3\n",
        "all_synth_lines=[]\n",
        "\n",
        "print(len(all_synth_lines))\n",
        "\n",
        "for file_path in glob.glob(\"/gdrive/My Drive/JUDEO-ARAB/for_synth/_*.txt\"):\n",
        "  print(file_path)\n",
        "  print(len(all_synth_lines))\n",
        "  all_synth_lines+=preprocess_synth_lines(\n",
        "    load_lines_synth(file_path))\n",
        "\n",
        "def gen_all_synth(keep):\n",
        "  all_synth_phrase_pairs=create_parralel_phrases_synth(all_synth_lines,\n",
        "                                                      keep)\n",
        "  all_synth_dataset=produce_dataset(all_synth_phrase_pairs,\n",
        "                                   to_shuffle=TO_SHUFFLE)\n",
        "  view_data(all_synth_dataset)\n",
        "  return all_synth_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39146\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000844.txt\n",
            "39146\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000300.txt\n",
            "39182\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000875.txt\n",
            "52628\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000120.txt\n",
            "81044\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_010699.txt\n",
            "92389\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000426.txt\n",
            "106574\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_009356.txt\n",
            "110219\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000950.txt\n",
            "111877\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_009343.txt\n",
            "115062\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_009342.txt\n",
            "118953\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_009355.txt\n",
            "119122\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_010672.txt\n",
            "119963\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_001285.txt\n",
            "140994\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000499.txt\n",
            "142285\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_007573.txt\n",
            "174384\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000572.txt\n",
            "176519\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000405.txt\n",
            "178724\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000574.txt\n",
            "181235\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_007593.txt\n",
            "184686\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000662.txt\n",
            "189958\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_001043.txt\n",
            "191207\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_010565.txt\n",
            "197048\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_006908.txt\n",
            "213651\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_010570.txt\n",
            "238690\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000153.txt\n",
            "241795\n",
            "/gdrive/My Drive/JUDEO-ARAB/OpenArab/JK_000346.txt\n",
            "303716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4",
        "colab_type": "text"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "befce2f3-ab8c-471b-c5d3-985761b18dd3"
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL\"\n",
        "\n",
        "#BASED ON THE MODEL FROM https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units, \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-40-fe0c13a5ddfd>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL1\"\n",
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "#model=rebuild()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mdbbN5XLlGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "be2d5d52-4f1c-4caa-fe26-c0ee513021df"
      },
      "source": [
        "model=rebuild()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 8)            384       \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (128, None, 2048)         6352896   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 67)           137283    \n",
            "=================================================================\n",
            "Total params: 63,150,531\n",
            "Trainable params: 63,150,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE",
        "colab_type": "text"
      },
      "source": [
        "##CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEFINE CHECKPOINT\"\n",
        "\n",
        "checkpoint_path='/gdrive/My Drive/checkpoints/'+this_time+\"/ckpt\"\n",
        "def save_checkpoint(massage,ckp_path=checkpoint_path):\n",
        "  print_log_screen(\"saving checkpoing at \"+checkpoint_path)\n",
        "  model.save_weights(checkpoint_path)\n",
        "  f_check= open(checkpoint_path+\".txt\",\"a+\")  \n",
        "  f_check.write(\"saving chekcpoing at epoch\"+ str(GLOBAL_epoch) +'\\n')\n",
        "  f_check.write(massage+'\\n')\n",
        "  f_check.close()\n",
        "  \n",
        "def load_checkpoint(checkpoint_path=checkpoint_path):\n",
        "  checkpoint_path=\"/gdrive/My Drive/checkpoints/\"+checkpoint_path+\"/ckpt\"\n",
        "  print_log_screen(\"loading checkpoing from \"+checkpoint_path)\n",
        "  model1=rebuild()\n",
        "  model1.load_weights(checkpoint_path)\n",
        "  return model1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG",
        "colab_type": "text"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo",
        "colab_type": "text"
      },
      "source": [
        "## forward run single letters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST single letters\"\n",
        "def test_single_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print_log(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "       print(\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print(\"####max is the blank symbole\")\n",
        "      print_log('-'*10)\n",
        "  \n",
        "#test_single_letters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B8G14-fL8Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98747b86-689a-4d2e-c5b7-cc8d68de7ea0"
      },
      "source": [
        "model=load_checkpoint(\"2020-05-07 15:51:26.100845\")\n",
        "test_single_letters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading checkpoing from /gdrive/My Drive/checkpoints/2020-05-07 15:51:26.100845/ckpt\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (128, None, 8)            384       \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (128, None, 2048)         6352896   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (128, None, 67)           137283    \n",
            "=================================================================\n",
            "Total params: 63,150,531\n",
            "Trainable params: 63,150,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "candidate:***( )***\n",
            "1.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.000 <blank>\n",
            "prediction***( )***1.000\n",
            "candidate:***(!)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  1.000 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(\")***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.001(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.997 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(')***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  1.000 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(()***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.161(()  0.001())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.838 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***())***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.038())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.001(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.961 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(,)***\n",
            "0.022( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.608(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.001(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.367 <blank>\n",
            "prediction***(,)***0.608\n",
            "candidate:***(-)***\n",
            "0.001( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.999(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.000 <blank>\n",
            "prediction***(-)***0.999\n",
            "candidate:***(.)***\n",
            "0.002( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.001(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.996 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(0)***\n",
            "0.002( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.001(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.995 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(1)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.001(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.998 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(2)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.999 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(3)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.001(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.998 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(4)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.001(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.006(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.993 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(5)***\n",
            "0.001( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.001(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.997 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(6)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.004(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.994 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(7)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.003(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.001(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.002(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.001(و)  0.000(ى)  0.002(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.002(ّ)  0.000(ٱ)  0.987 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(8)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.500(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.499 <blank>\n",
            "prediction***(8)***0.500\n",
            "candidate:***(9)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.023(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.977 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(:)***\n",
            "0.005( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.069(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.924 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(;)***\n",
            "0.033( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.966(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.001 <blank>\n",
            "prediction***(;)***0.966\n",
            "candidate:***(?)***\n",
            "0.001( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.002(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.103(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.003(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.001(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.888 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(H)***\n",
            "0.529( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.468(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.002(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.002 <blank>\n",
            "prediction***( )***0.529\n",
            "candidate:***([)***\n",
            "0.001( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.001(8)  0.000(9)  0.000(:)  0.001(;)  0.000(?)  0.000(H)  0.347([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.647 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(])***\n",
            "0.029( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.512(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.001(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.002(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.454 <blank>\n",
            "prediction***(])***0.512\n",
            "candidate:***(א)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.004(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.996 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ב)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.353(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.647 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ג)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.002(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.009(غ)  0.000(ف)  0.002(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.001(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.984 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ד)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.002(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.004(ّ)  0.000(ٱ)  0.994 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ה)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  1.000 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ו)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.436(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.563 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ז)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.002(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.395(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.007(ّ)  0.000(ٱ)  0.594 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ח)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.595(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.002(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.001(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.001(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.004(ّ)  0.000(ٱ)  0.397 <blank>\n",
            "prediction***(ح)***0.595\n",
            "candidate:***(ט)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.005(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.994 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(י)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.002(ّ)  0.000(ٱ)  0.997 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(כ)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.005(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.994 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ל)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.009(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.991 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(מ)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.045(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.036(ّ)  0.000(ٱ)  0.919 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(נ)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.052(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.003(ّ)  0.000(ٱ)  0.944 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ס)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.960(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.039 <blank>\n",
            "prediction***(س)***0.960\n",
            "candidate:***(ע)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.998(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.002 <blank>\n",
            "prediction***(ع)***0.998\n",
            "candidate:***(פ)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.644(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.355 <blank>\n",
            "prediction***(ف)***0.644\n",
            "candidate:***(צ)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.015(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.004(ّ)  0.000(ٱ)  0.981 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ק)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.980(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.020 <blank>\n",
            "prediction***(ق)***0.980\n",
            "candidate:***(ר)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.003(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.001(?)  0.000(H)  0.000([)  0.000(])  0.001(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.705(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.002(ّ)  0.000(ٱ)  0.287 <blank>\n",
            "prediction***(ر)***0.705\n",
            "candidate:***(ש)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.000(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.000(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.101(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.000(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.001(ّ)  0.000(ٱ)  0.897 <blank>\n",
            "####max is the blank symbole\n",
            "candidate:***(ת)***\n",
            "0.000( )  0.000(!)  0.000(\")  0.000(')  0.000(()  0.000())  0.000(,)  0.000(-)  0.000(.)  0.000(0)  0.000(1)  0.000(2)  0.000(3)  0.000(4)  0.000(5)  0.000(6)  0.000(7)  0.000(8)  0.000(9)  0.000(:)  0.000(;)  0.000(?)  0.001(H)  0.000([)  0.000(])  0.000(ء)  0.000(آ)  0.000(أ)  0.000(ؤ)  0.000(إ)  0.000(ئ)  0.000(ا)  0.000(ب)  0.000(ة)  0.006(ت)  0.000(ث)  0.000(ج)  0.000(ح)  0.000(خ)  0.000(د)  0.000(ذ)  0.000(ر)  0.000(ز)  0.000(س)  0.000(ش)  0.000(ص)  0.000(ض)  0.000(ط)  0.000(ظ)  0.000(ع)  0.000(غ)  0.000(ف)  0.000(ق)  0.000(ك)  0.000(ل)  0.000(م)  0.001(ن)  0.000(ه)  0.000(و)  0.000(ى)  0.000(ي)  0.000(ً)  0.000(ٌ)  0.000(ٍ)  0.000(ّ)  0.000(ٱ)  0.992 <blank>\n",
            "####max is the blank symbole\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF",
        "colab_type": "text"
      },
      "source": [
        "##forward run text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM2_GVYmOwdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e3b9bee-ed23-4759-e8d6-7a37621e27a1"
      },
      "source": [
        "CELL_NAME=\"DEF test__CTC_word_multiline\"\n",
        "\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def forward_text(lines,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_JA(l)\n",
        "    l = double_hebrew(l)\n",
        "    v= encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "\n",
        "  #PADD HORIZANTALY REST OF LINES TO FILL BATCH\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "  res=[]\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  \n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "\n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print_log(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_arr(dense[t])\n",
        "      print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''כמאלא\n",
        "כמאלא'''\n",
        "\n",
        "print_log_screen(forward_text(lines.split('\\n'),3,BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "كمالا\n",
            "كمالا\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K6i5QWGO4DS",
        "colab_type": "text"
      },
      "source": [
        "##TEST KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "69509182-bc3b-42e8-8976-335534cd2108"
      },
      "source": [
        "CELL_NAME=\"DEF TEST KFIR\"\n",
        "#JA_lines should be allready with doubling\n",
        "#indexes - word index to be tested (if testing only on a certain word in the every line)\n",
        "def test_text_kfir(JA_lines,arr_lines,indexes=None,num_of_paths=1,SHOW_PRINT=False):   \n",
        "  num_of_lines=len(JA_lines)\n",
        "  num_of_letters= 0\n",
        "  assert(num_of_lines==len(arr_lines))\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in JA_lines:\n",
        "    l = preprocess_JA(l)\n",
        "    #l = double_hebrew(l)\n",
        "    v=encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "\n",
        "\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "  predict_ltrs=model(inputs)\n",
        "\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  \n",
        "  total_res=[]\n",
        "  total_edit_dist=0\n",
        "  total_normalized_edit_dist=0\n",
        "  line_counter=1\n",
        "  for t in range(num_of_lines):\n",
        "    real=arr_lines[t]\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_arr(dense[t]).strip() \n",
        "      # print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "      total_res.append(prediction)\n",
        "    if indexes:\n",
        "      real=real.split()[indexes[t]]\n",
        "      # print(real)          \n",
        "      # print(indexes[t])\n",
        "      # print(prediction)\n",
        "      prediction=prediction.split()[indexes[t]]\n",
        "    ed_dist=editdistance.eval(real, prediction)\n",
        "    num_of_letters+=len(real)\n",
        "\n",
        "    normalized_ed_dist=ed_dist/len(real)\n",
        "    real,prediction=show_diff(real,prediction,'red')\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"({0})\".format(line_counter),LTRchar,undouble_hebrew(JA_lines[t]),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(ed_dist))\n",
        "    line_counter+=1\n",
        "    total_normalized_edit_dist+=normalized_ed_dist\n",
        "    total_edit_dist+=ed_dist\n",
        "  return total_res,total_normalized_edit_dist,num_of_lines,total_edit_dist,num_of_letters\n",
        "  \n",
        "JA_lines='''ייככ''אאללףף\n",
        "עעלליי\n",
        "ההדד''הה\n",
        "אאללאאממאאננהה\n",
        "ווללאא'''\n",
        "\n",
        "JA_lines='''ייככ''אאללףף\n",
        "אאללאאממאאננהה ששללווםם'''\n",
        "\n",
        "# arr_lines='''يخالف\n",
        "# على\n",
        "# هذه\n",
        "# الأمانة\n",
        "# ولا'''\n",
        "\n",
        "arr_lines='''يخالف\n",
        "الأمانة الأمانة'''\n",
        "\n",
        "indexes=[0,1]\n",
        "indexes=None\n",
        "\n",
        "\n",
        "test_text_kfir(JA_lines.split('\\n'),arr_lines.split('\\n'),indexes,SHOW_PRINT=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1) ‫ יכ'אלף | يخالف | يخالف | 0.0000\n",
            "(2) ‫ אלאמאנה שלום | الأمانة \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mأ\u001b[0mم\u001b[1m\u001b[31mانة\u001b[0m | الأمانة \u001b[1m\u001b[31mع\u001b[0mل\u001b[1m\u001b[31mو\u001b[0mم | 5.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['يخالف', 'الأمانة علوم'], 0.3333333333333333, 2, 5, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5e5-59jCxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "35d0e055-ff60-4f43-aad8-9c4525ea3e4f"
      },
      "source": [
        "CELL_NAME=\"DEF test_kfir1\"\n",
        "#data_path : kfir_kuzari_test/kfir_rasag_test\n",
        "def test_kfir(data_path,SHOW_PRINT=False,indexes_filepath=None,replace_GAIN=False):\n",
        "  lines=preprocess_lines(load_lines(data_path))\n",
        "  pairs = create_parralel_phrases(lines)\n",
        "  \n",
        "  if indexes_filepath:\n",
        "    indexes=[]\n",
        "    f_indexes=open(indexes_filepath,'r')\n",
        "    ind_lines=f_indexes.readlines()\n",
        "    assert(len(ind_lines)==len(lines))\n",
        "    for il in ind_lines:\n",
        "      indexes.append(int(il)) #need to iterate beacuase of the casting to int\n",
        "    f_indexes.close()    \n",
        "  if replace_GAIN:    \n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs]\n",
        "  \n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "\n",
        "  num_of_lines=len(pairs)\n",
        "  index=0\n",
        "  total_num_of_examples=0\n",
        "  total_num_of_letters=0\n",
        "  total_sum_e_d_normalized=0\n",
        "  total_sum_e_d=0\n",
        "  while index<=num_of_lines:\n",
        "    batch_hebrew=hebrew_lines[index:index+BATCH_SIZE]\n",
        "    batch_arab=arab_lines[index:index+BATCH_SIZE]\n",
        "    if indexes_filepath:\n",
        "      batch_indexes=indexes[index:index+BATCH_SIZE]\n",
        "    else:\n",
        "      batch_indexes=None\n",
        "    _,sum_of_e_d_normalized,num_of_examples,sum_of_e_d,num_of_letters=test_text_kfir(batch_hebrew,\n",
        "                                                                                     batch_arab,\n",
        "                                                                                     batch_indexes,\n",
        "                                                                                     SHOW_PRINT=SHOW_PRINT)\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"BATCH (sum_of_e_d_normalized,num_of_examples): \",sum_of_e_d_normalized,num_of_examples)\n",
        "      print_log_screen(\"BATCH (sum_of_e_d,num_of_letters): \",sum_of_e_d,num_of_letters)\n",
        "    total_num_of_examples+=num_of_examples\n",
        "    total_num_of_letters+=num_of_letters\n",
        "    total_sum_e_d+=sum_of_e_d\n",
        "    total_sum_e_d_normalized+=sum_of_e_d_normalized\n",
        "    index+=BATCH_SIZE\n",
        "\n",
        "  print_log_screen(\"#examples:\",total_num_of_examples,\", accuracy:\",1-total_sum_e_d_normalized/total_num_of_examples)\n",
        "  print_log_screen(\"#letters:\",total_num_of_letters,\", accuracy1:\",1-total_sum_e_d/total_num_of_letters)\n",
        "  return total_sum_e_d_normalized/total_num_of_examples,total_sum_e_d/total_num_of_letters\n",
        "\n",
        "\n",
        "test_kfir(kfir_kuzari_test_SWITCH,SHOW_PRINT=False)\n",
        "test_kfir(kfir_kuzari_test,SHOW_PRINT=False)\n",
        "test_kfir(kfir_rasag_test_SWITCH,SHOW_PRINT=False)\n",
        "test_kfir(kfir_rasag_test,SHOW_PRINT=False)\n",
        "#test_kfir(kfir_rasag_test,SHOW_PRINT=True,replace_GAIN=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 500\n",
            "#examples: 500 , accuracy: 0.917111038961039\n",
            "#letters: 2325 , accuracy1: 0.916989247311828\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_ORIGINAL.txt\n",
            "len(lines) 500\n",
            "#examples: 500 , accuracy: 0.9063816738816739\n",
            "#letters: 2325 , accuracy1: 0.9058064516129032\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 50\n",
            "#examples: 50 , accuracy: 0.9283333333333333\n",
            "#letters: 192 , accuracy1: 0.9114583333333334\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_ORIGINAL.txt\n",
            "len(lines) 50\n",
            "#examples: 50 , accuracy: 0.9473333333333334\n",
            "#letters: 192 , accuracy1: 0.9375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.05266666666666667, 0.0625)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyZ4mSjvzKxX",
        "colab_type": "text"
      },
      "source": [
        "##COMPARE with/without CONTEXT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qhhb8OaD1QT",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35ffdd8d-94b2-4b6b-b9ad-be5e6d7062a4"
      },
      "source": [
        "CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR\"\n",
        "\n",
        "def switch_gim_ghayn(ja):\n",
        "  return ja.replace(\"ג\",\"ג'\").replace(\"ג''\",\"ג\")\n",
        "\n",
        "#adjust test kfir to comp edits only for index word!\n",
        "EMUNOT=True\n",
        "\n",
        "if EMUNOT:\n",
        "  file_all = haemunot\n",
        "  file_singels = kfir_rasag_test\n",
        "else:\n",
        "  file_all = haemunot  #there's no use in testing with lines from kuzari since I'm training on it.\n",
        "  file_singels = kfir_kuzari_test_SWITCH\n",
        "\n",
        "full_lines=preprocess_lines(load_lines(file_all))\n",
        "single_words=preprocess_lines(load_lines(file_singels))\n",
        "\n",
        "\n",
        "def grep(reg,ja):\n",
        "  random.shuffle(full_lines) \n",
        "  JA_dont_match=False\n",
        "  for line in full_lines:\n",
        "      line=remove_arab_nikud(line) #TODO: do preprocessing jointly    \n",
        "      JA_line,arab_line=line.split('\\t')\n",
        "      if re.search(reg, arab_line):\n",
        "          full_line=line\n",
        "          arab_words=arab_line.split()\n",
        "          JA_words=JA_line.split()\n",
        "          reg_ind=arab_words.index(reg.strip(\" $^\"))\n",
        "          single_words=ja+'\\t'+arab_words[reg_ind]\n",
        "          assert(len(JA_words)==len(arab_words)) #MAYBE NOT ALWAYS THE CASE!!!          \n",
        "          if (not JA_words[reg_ind]==ja):\n",
        "            JA_dont_match=True\n",
        "            print(\"ja:\"+ja+ \"JA_words[reg_ind]:\" +JA_words[reg_ind])            \n",
        "            print(\"continue searching...\")\n",
        "            continue          \n",
        "          return full_line,single_words,reg_ind\n",
        "  if JA_dont_match:\n",
        "    return full_line.replace(JA_words[reg_ind],ja),single_words,reg_ind\n",
        "  return\n",
        "          \n",
        "def highlight(text,word_index):\n",
        "  words=text.split()\n",
        "  res=[]  \n",
        "  for i in range(len(words)):\n",
        "    if i==word_index:\n",
        "      res.append(colored(words[i],'red'))\n",
        "    else:\n",
        "      res.append(words[i])      \n",
        "  return ' '.join(res)\n",
        "\n",
        "def highlight_grep_output(ja_arr_line,word_index):\n",
        "  ja,arr=ja_arr_line.split('\\t')\n",
        "  return highlight(ja,word_index)+'\\t'+highlight(arr,word_index)\n",
        "\n",
        "\n",
        "not_found_counter=0\n",
        "\n",
        "full_f=open(\"with_context.txt\",'w+')\n",
        "single_f=open(\"no_context.txt\",'w+')\n",
        "index_f=open(\"context_idexes.txt\",'w+')\n",
        "\n",
        "for l in single_words:  \n",
        "  arab=l.split('\\t')[1].strip()\n",
        "  ja=l.split('\\t')[0].strip()\n",
        "  print(l)\n",
        "  # if not DO_EMUNOT:\n",
        "  #   ja=ja.replace(\"ג\",\"ג'\").replace(\"ג''\",\"ג\")    ##only for kuzari - change to regular (or won't grep)\n",
        "  arab=' '+arab+' ' #first try to find greped in a middle of a line if exists\n",
        "  found=grep(arab,ja)\n",
        "  if not found:\n",
        "    print(colored(\"###SEARCH FIRST WORD\",\"red\",\"on_yellow\"))\n",
        "    found=grep(\"^\"+arab.lstrip(),ja)  #TODO regex for both this and next reg by | , so I don't discremanate start of line to end of line\n",
        "    if not found:\n",
        "      print(colored(\"###SEARCH LAST WORD\",\"red\",\"on_yellow\"))\n",
        "      found=grep(arab.rstrip()+\"$\",ja)\n",
        "      if not found:\n",
        "        not_found_counter+=1\n",
        "  if not found:\n",
        "    print(colored(\"not found:\"+arab,\"blue\",\"on_green\"))\n",
        "  else:\n",
        "    print(highlight_grep_output(found[0],found[2]))\n",
        "    single_f.write(found[1]+'\\n')\n",
        "    full_f.write(found[0]+'\\n')\n",
        "    index_f.write(str(found[2])+'\\n')\n",
        "\n",
        "single_f.close()\n",
        "full_f.close()\n",
        "index_f.close()\n",
        "\n",
        "\n",
        "\n",
        "print(\"#not founds:\"+str(not_found_counter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\n",
            "len(lines) 10358\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_ORIGINAL.txt\n",
            "len(lines) 50\n",
            "דאר\tدار\n",
            "פי \u001b[31mדאר\u001b[0m אלדניא עלי צ'רבינ\tفي \u001b[31mدار\u001b[0m الدنيا على ضربين\n",
            "אלג'זא\tالجزاء\n",
            "וקת \u001b[31mאלג'זא\u001b[0m אלא פריקאנ\tوقت \u001b[31mالجزاء\u001b[0m إلاّ فريقان\n",
            "וקבל\tوقبل\n",
            ". \u001b[31mוקבל\u001b[0m ד'לכ מא ראי אנ\t. \u001b[31mوقبل\u001b[0m ذلك ما رأى أن\n",
            "ד'לכ\tذلك\n",
            "אמ גיר \u001b[31mד'לכ\u001b[0m פאנ כאנת\tأم غير \u001b[31mذلك\u001b[0m فإن كانت\n",
            "מא\tما\n",
            "צ'רורה \u001b[31mמא\u001b[0m אלד'י נקול\tضرورة \u001b[31mما\u001b[0m الذي نقول\n",
            "ראי\tرأى\n",
            "או אנה \u001b[31mראי\u001b[0m אלמות יג'מע\tأو أنّه \u001b[31mرأى\u001b[0m الموت يجمع\n",
            "אנ\tأن\n",
            "פיג'ב \u001b[31mאנ\u001b[0m נעתקד אנ אלארבע\tفيجب \u001b[31mأن\u001b[0m نعتقد أنّ الأربع\n",
            "יפרק\tيفرّق\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[31mיפרק\u001b[0m בינ רוחה וג'סמה\t\u001b[31mيفرّق\u001b[0m بين روحه وجسمه\n",
            "בינ\tبين\n",
            "יפרק \u001b[31mבינ\u001b[0m רוחה וג'סמה\tيفرّق \u001b[31mبين\u001b[0m روحه وجسمه\n",
            "רוחה\tروحه\n",
            "וטארת \u001b[31mרוחה\u001b[0m כמא קאל H\tوطارت \u001b[31mروحه\u001b[0m كما قال H\n",
            "וג'סמה\tوجسمه\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[43m\u001b[31m###SEARCH LAST WORD\u001b[0m\n",
            "יפרק בינ רוחה \u001b[31mוג'סמה\u001b[0m\tيفرّق بين روحه \u001b[31mوجسمه\u001b[0m\n",
            "אלי\tإلى\n",
            "להמ והמ ממהלונ \u001b[31mאלי\u001b[0m אנ\tلهم وهم ممهلون \u001b[31mإلى\u001b[0m أن\n",
            "וקת\tوقت\n",
            "אי \u001b[31mוקת\u001b[0m טלעתה כד'אכ ואשד\tأي \u001b[31mوقت\u001b[0m طلعته كذاك وأشد\n",
            "אסתכמאל\tاستكمال\n",
            "אלי וקת \u001b[31mאסתכמאל\u001b[0m אלנפוס\tإلى وقت \u001b[31mاستكمال\u001b[0m النفوس\n",
            "אלנפוס\tالنفوس\n",
            "באנ עיוב \u001b[31mאלנפוס\u001b[0m כאלד'נוב\tبأنّ عيوب \u001b[31mالنفوس\u001b[0m كالذنوب\n",
            "חתי\tحتى\n",
            "פי אלבחר , \u001b[31mחתי\u001b[0m תעאד באסרהא\tفي البحر , \u001b[31mحتى\u001b[0m تعاد بأسرها\n",
            "יג'מעהא\tيجمعها\n",
            "חתי \u001b[31mיג'מעהא\u001b[0m אלג'מיע עלי\tحتى \u001b[31mيجمعها\u001b[0m الجميع على\n",
            "אלג'מיע\tالجميع\n",
            "מנ \u001b[31mאלג'מיע\u001b[0m וד'לכ לאנהמ\tمن \u001b[31mالجميع\u001b[0m وذلك لأنهم\n",
            "עלי\tعلى\n",
            ", יחת'המ \u001b[31mעלי\u001b[0m חפט' אלסבת\t, يحثهم \u001b[31mعلى\u001b[0m حفظ السبت\n",
            "מא\tما\n",
            "ועלי \u001b[31mמא\u001b[0m סנד'כר פי בעצ'\tوعلى \u001b[31mما\u001b[0m سنذكر في بعض\n",
            "בינת\tبيّنت\n",
            ". פאד' קד \u001b[31mבינת\u001b[0m הד'ה אלת'לת'\t. فإذ قد \u001b[31mبيّنت\u001b[0m هذه الثلث\n",
            "פלא\tفلا\n",
            "אלאול ומע ד'לכ \u001b[31mפלא\u001b[0m בד\tالأول ومع ذلك \u001b[31mفلا\u001b[0m بدّ\n",
            "נעלמ\tنعلم\n",
            "? . פינבגי אנ \u001b[31mנעלמ\u001b[0m אנ\t? . فينبغي أن \u001b[31mنعلم\u001b[0m أنّ\n",
            "יהודיא\tيهوديّاً\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[43m\u001b[31m###SEARCH LAST WORD\u001b[0m\n",
            "מא בינת . פלא נעלמ \u001b[31mיהודיא\u001b[0m\tما بيّنت . فلا نعلم \u001b[31mيهوديّاً\u001b[0m\n",
            "יכ'אלפ\tيخالف\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[31mיכ'אלפ\u001b[0m עלי הד'ה אלאמאנה\t\u001b[31mيخالف\u001b[0m على هذه الأمانة\n",
            "עלי\tعلى\n",
            "מעה פעל \u001b[31mעלי\u001b[0m טול אלזמאנ\tمعه فعل \u001b[31mعلى\u001b[0m طول الزمان\n",
            "הד'ה\tهذه\n",
            ". פאד'א ג'מענא \u001b[31mהד'ה\u001b[0m אלארבעה\t. فإذا جمعنا \u001b[31mهذه\u001b[0m الأربعة\n",
            "אלאמאנה\tالأمانة\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[43m\u001b[31m###SEARCH LAST WORD\u001b[0m\n",
            "למנ כ'אלפנא פי הד'ה \u001b[31mאלאמאנה\u001b[0m\tلمن خالفنا في هذه \u001b[31mالأمانة\u001b[0m\n",
            "ולא\tولا\n",
            ", \u001b[31mולא\u001b[0m מנ ליס הו מכ'תארא\t, \u001b[31mولا\u001b[0m من ليس هو مختاراً\n",
            "יסתצעב\tيستصعب\n",
            "ולא \u001b[31mיסתצעב\u001b[0m ענד עקלה כיפ\tولا \u001b[31mيستصعب\u001b[0m عند عقله كيف\n",
            "ענד\tعند\n",
            "אלגמאמ \u001b[31mענד\u001b[0m מואפאתכ ,\tالغمام \u001b[31mعند\u001b[0m موافاتك ,\n",
            "עקלה\tعقله\n",
            ", יג'וז פי \u001b[31mעקלה\u001b[0m אנ יאמרה\t, يجوز في \u001b[31mعقله\u001b[0m أن يأمره\n",
            "כיפ\tكيف\n",
            "אג'מעינ \u001b[31mכיפ\u001b[0m תסעהמ אלארצ'\tأجمعين \u001b[31mكيف\u001b[0m تسعهم الأرض\n",
            "יחיי\tيحيي\n",
            "ללעקאב אד' ליס \u001b[31mיחיי\u001b[0m פי\tللعقاب إذ ليس \u001b[31mيحيي\u001b[0m في\n",
            "רבה\tربّه\n",
            "יגלב קוה' \u001b[31mרבה\u001b[0m , פאד'א\tيغلب قوّة \u001b[31mربّه\u001b[0m , فإذا\n",
            "אלמותי\tالموتى\n",
            "פי אחיה \u001b[31mאלמותי\u001b[0m . אלמקאלה\tفي إحياء \u001b[31mالموتى\u001b[0m . المقالة\n",
            "אד'\tإذ\n",
            "אלמאל \u001b[31mאד'\u001b[0m קאל : H H H\tالمال \u001b[31mإذ\u001b[0m قال : H H H\n",
            "קד\tقد\n",
            "יקאל להא כואכב , \u001b[31mקד\u001b[0m קטעת\tيقال لها كواكب , \u001b[31mقد\u001b[0m قطعت\n",
            "צח\tصحّ\n",
            "פקד אבטל מעני קד \u001b[31mצח\u001b[0m ענדה\tفقد أبطل معنى قد \u001b[31mصحّ\u001b[0m عنده\n",
            "לה\tله\n",
            "אנ ליס \u001b[31mלה\u001b[0m שי מנ הד'ה\tأنّ ليس \u001b[31mله\u001b[0m شيء من هذه\n",
            "אנה\tأنه\n",
            "לא יעלמ \u001b[31mאנה\u001b[0m דאכ'ל H קנאדיל\tلا يعلم \u001b[31mأنه\u001b[0m داخل H قناديل\n",
            "כ'לק\tخلق\n",
            "שרא , ואנמא \u001b[31mכ'לק\u001b[0m אלאשיא\tشرّاً , وإنما \u001b[31mخلق\u001b[0m الأشياء\n",
            "שיא\tشيئاً\n",
            "אנ \u001b[31mשיא\u001b[0m מא כאנ פיה בה\tأنّ \u001b[31mشيئاً\u001b[0m ما كان فيه به\n",
            "לא\tلا\n",
            "ט'אהרה , אלא מא \u001b[31mלא\u001b[0m יג'וז\tظاهره , إلاّ ما \u001b[31mلا\u001b[0m يجوز\n",
            "מנ\tمن\n",
            "בל הי אדונ \u001b[31mמנ\u001b[0m חאל אלאפלאכ\tبل هي أدون \u001b[31mمن\u001b[0m حال الأفلاك\n",
            "שי\tشيء\n",
            "הל יקדר עלי לא \u001b[31mשי\u001b[0m בל\tهل يقدر على لا \u001b[31mشيء\u001b[0m بل\n",
            "פלא\tفلا\n",
            "אכ'יר \u001b[31mפלא\u001b[0m יכונ וראה מעלומ\tأخير \u001b[31mفلا\u001b[0m يكون وراءه معلوم\n",
            "יג'וז\tيجوز\n",
            ", \u001b[31mיג'וז\u001b[0m פי עקלה אנ יאמרה\t, \u001b[31mيجوز\u001b[0m في عقله أن يأمره\n",
            "אנ\tأن\n",
            ", כד'אכ וג'ב \u001b[31mאנ\u001b[0m תעמ דלאילה\t, كذاك وجب \u001b[31mأن\u001b[0m تعمّ دلائله\n",
            "יסתעסר\tيستعسر\n",
            "\u001b[43m\u001b[31m###SEARCH FIRST WORD\u001b[0m\n",
            "\u001b[31mיסתעסר\u001b[0m לה אנ יעיד שיא\t\u001b[31mيستعسر\u001b[0m له أن يعيد شيئاً\n",
            "#not founds:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNrQSIk2f9iG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "81b204b9-64f6-48d5-94e0-33bdf6c669b0"
      },
      "source": [
        "CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR 1\"\n",
        "\n",
        "PRINT_DETAILED=True\n",
        "\n",
        "#model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-04 12:32:58.607342/ckpt')  #THIS IS THE FIRST ACTIVATIION, BY WHICH I PRODUCED THE EXCEL FOR NACHUM\n",
        "\n",
        "#model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-05 13:01:42.636420/ckpt')\n",
        "\n",
        "#model=load_checkpoint(\"2020-05-07 15:51:26.100845\") THIS IS NOW\n",
        "\n",
        "if model.stateful:\n",
        "  model.reset_states()\n",
        "\n",
        "test_kfir(\"with_context.txt\",SHOW_PRINT=PRINT_DETAILED,indexes_filepath=\"context_idexes.txt\",replace_GAIN=True)\n",
        "\n",
        "#print(\"=\"*200)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: with_context.txt\n",
            "len(lines) 50\n",
            "(1) ‫ פי דאר אלדניא עלי צ'רבינ | دار | دار | 0.0000\n",
            "(2) ‫ וקת אלגזא אלא פריקאנ | الجزاء | الجزاء | 0.0000\n",
            "(3) ‫ . וקבל ד'לכ מא ראי אנ | وقبل | وقبل | 0.0000\n",
            "(4) ‫ אמ ג'יר ד'לכ פאנ כאנת | ذلك | ذلك | 0.0000\n",
            "(5) ‫ צ'רורה מא אלד'י נקול | ما | ما | 0.0000\n",
            "(6) ‫ או אנה ראי אלמות יגמע | رأ\u001b[1m\u001b[31mى\u001b[0m | رأ\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(7) ‫ פיגב אנ נעתקד אנ אלארבע | أن | أن | 0.0000\n",
            "(8) ‫ יפרק בינ רוחה וגסמה | يفر\u001b[1m\u001b[31mّ\u001b[0mق | يفرق | 1.0000\n",
            "(9) ‫ יפרק בינ רוחה וגסמה | بين | بين | 0.0000\n",
            "(10) ‫ וטארת רוחה כמא קאל H | روحه | روحه | 0.0000\n",
            "(11) ‫ יפרק בינ רוחה וגסמה | وجسمه | وجسمه | 0.0000\n",
            "(12) ‫ להמ והמ ממהלונ אלי אנ | إلى | إلى | 0.0000\n",
            "(13) ‫ אי וקת טלעתה כד'אכ ואשד | وقت | وقت | 0.0000\n",
            "(14) ‫ אלי וקת אסתכמאל אלנפוס | استكمال | استكمال | 0.0000\n",
            "(15) ‫ באנ עיוב אלנפוס כאלד'נוב | النفوس | النفوس | 0.0000\n",
            "(16) ‫ פי אלבחר , חתי תעאד באסרהא | حتى | حت\u001b[1m\u001b[31mّ\u001b[0mى | 1.0000\n",
            "(17) ‫ חתי יגמעהא אלגמיע עלי | يجمعها | يجمعها | 0.0000\n",
            "(18) ‫ מנ אלגמיע וד'לכ לאנהמ | الجميع | الجميع | 0.0000\n",
            "(19) ‫ , יחת'המ עלי חפט' אלסבת | على | على | 0.0000\n",
            "(20) ‫ ועלי מא סנד'כר פי בעצ' | ما | ما | 0.0000\n",
            "(21) ‫ . פאד' קד בינת הד'ה אלת'לת' | بيّنت | بيّنت | 0.0000\n",
            "(22) ‫ אלאול ומע ד'לכ פלא בד | فلا | فلا | 0.0000\n",
            "(23) ‫ ? . פינבג'י אנ נעלמ אנ | نعلم | نعلم | 0.0000\n",
            "(24) ‫ מא בינת . פלא נעלמ יהודיא | يهوديّا\u001b[1m\u001b[31mً\u001b[0m | يهوديّا | 1.0000\n",
            "(25) ‫ יכ'אלפ עלי הד'ה אלאמאנה | يخالف | يخالف | 0.0000\n",
            "(26) ‫ מעה פעל עלי טול אלזמאנ | على | على | 0.0000\n",
            "(27) ‫ . פאד'א גמענא הד'ה אלארבעה | هذه | هذه | 0.0000\n",
            "(28) ‫ למנ כ'אלפנא פי הד'ה אלאמאנה | الأمانة | الأمانة | 0.0000\n",
            "(29) ‫ , ולא מנ ליס הו מכ'תארא | ولا | ولا | 0.0000\n",
            "(30) ‫ ולא יסתצעב ענד עקלה כיפ | يستصعب | يستصعب | 0.0000\n",
            "(31) ‫ אלג'מאמ ענד מואפאתכ , | عند | عند | 0.0000\n",
            "(32) ‫ , יגוז פי עקלה אנ יאמרה | عقله | عقله | 0.0000\n",
            "(33) ‫ אגמעינ כיפ תסעהמ אלארצ' | كيف | كيف | 0.0000\n",
            "(34) ‫ ללעקאב אד' ליס יחיי פי | يحيي | يحيي | 0.0000\n",
            "(35) ‫ יג'לב קוה' רבה , פאד'א | ربّه | ربّه | 0.0000\n",
            "(36) ‫ פי אחיה אלמותי . אלמקאלה | الموتى | الموتى | 0.0000\n",
            "(37) ‫ אלמאל אד' קאל : H H H | إذ | إذ | 0.0000\n",
            "(38) ‫ יקאל להא כואכב , קד קטעת | قد | قد | 0.0000\n",
            "(39) ‫ פקד אבטל מעני קד צח ענדה | صحّ | صحّ | 0.0000\n",
            "(40) ‫ אנ ליס לה שי מנ הד'ה | له | له | 0.0000\n",
            "(41) ‫ לא יעלמ אנה דאכ'ל H קנאדיל | أنه | أنه | 0.0000\n",
            "(42) ‫ שרא , ואנמא כ'לק אלאשיא | خلق | خلق | 0.0000\n",
            "(43) ‫ אנ שיא מא כאנ פיה בה | ش\u001b[1m\u001b[31mي\u001b[0mئاً | شئاً | 1.0000\n",
            "(44) ‫ ט'אהרה , אלא מא לא יגוז | لا | لا | 0.0000\n",
            "(45) ‫ בל הי אדונ מנ חאל אלאפלאכ | من | من | 0.0000\n",
            "(46) ‫ הל יקדר עלי לא שי בל | شيء | شيء | 0.0000\n",
            "(47) ‫ אכ'יר פלא יכונ וראה מעלומ | فلا | فلا | 0.0000\n",
            "(48) ‫ , יגוז פי עקלה אנ יאמרה | يجوز | يجوز | 0.0000\n",
            "(49) ‫ , כד'אכ וגב אנ תעמ דלאילה | أن | أن | 0.0000\n",
            "(50) ‫ יסתעסר לה אנ יעיד שיא | يستع\u001b[1m\u001b[31mس\u001b[0mر | يستع\u001b[1m\u001b[31mب\u001b[0mر | 1.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  1.3583333333333334 50\n",
            "BATCH (sum_of_e_d,num_of_letters):  6 192\n",
            "#examples: 50 , accuracy: 0.9728333333333333\n",
            "#letters: 192 , accuracy1: 0.96875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.02716666666666667, 0.03125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGDdAym7t8m7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "4f26110a-5788-4c69-b7ed-1e145c7b6a9c"
      },
      "source": [
        "if model.stateful:\n",
        "  model.reset_states()\n",
        "\n",
        "test_kfir(\"no_context.txt\",SHOW_PRINT=PRINT_DETAILED,replace_GAIN=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: no_context.txt\n",
            "len(lines) 50\n",
            "(1) ‫ דאר | دار | دار | 0.0000\n",
            "(2) ‫ אלגזא | الجزا\u001b[1m\u001b[31mء\u001b[0m | الجزا | 1.0000\n",
            "(3) ‫ וקבל | وقبل | وقبل | 0.0000\n",
            "(4) ‫ ד'לכ | ذلك | ذلك | 0.0000\n",
            "(5) ‫ מא | ما | ما | 0.0000\n",
            "(6) ‫ ראי | رأى | رأى | 0.0000\n",
            "(7) ‫ אנ | أن | أن\u001b[1m\u001b[31mّ\u001b[0m | 1.0000\n",
            "(8) ‫ יפרק | يفر\u001b[1m\u001b[31mّ\u001b[0mق | يفرق | 1.0000\n",
            "(9) ‫ בינ | بين | بين | 0.0000\n",
            "(10) ‫ רוחה | روحه | روحه | 0.0000\n",
            "(11) ‫ וגסמה | وجسمه | وجسمه | 0.0000\n",
            "(12) ‫ אלי | إلى | إلى | 0.0000\n",
            "(13) ‫ וקת | وقت | وقت | 0.0000\n",
            "(14) ‫ אסתכמאל | استكمال | استكمال | 0.0000\n",
            "(15) ‫ אלנפוס | النفوس | النفوس | 0.0000\n",
            "(16) ‫ חתי | حتى | حت\u001b[1m\u001b[31mّ\u001b[0mى | 1.0000\n",
            "(17) ‫ יגמעהא | يجمعها | يجمعها | 0.0000\n",
            "(18) ‫ אלגמיע | الجميع | الجميع | 0.0000\n",
            "(19) ‫ עלי | على | على | 0.0000\n",
            "(20) ‫ מא | ما | ما | 0.0000\n",
            "(21) ‫ בינת | بي\u001b[1m\u001b[31mّ\u001b[0mنت | بينت | 1.0000\n",
            "(22) ‫ פלא | فلا | فلا | 0.0000\n",
            "(23) ‫ נעלמ | نعلم | نعلم | 0.0000\n",
            "(24) ‫ יהודיא | يهوديّا\u001b[1m\u001b[31mً\u001b[0m | يهوديّا | 1.0000\n",
            "(25) ‫ יכ'אלפ | يخالف | يخالف | 0.0000\n",
            "(26) ‫ עלי | على | على | 0.0000\n",
            "(27) ‫ הד'ה | هذه | هذه | 0.0000\n",
            "(28) ‫ אלאמאנה | الأمانة | الأمانة | 0.0000\n",
            "(29) ‫ ולא | ولا | ولا | 0.0000\n",
            "(30) ‫ יסתצעב | يستصعب | يستصعب | 0.0000\n",
            "(31) ‫ ענד | عند | عند | 0.0000\n",
            "(32) ‫ עקלה | عقله | عقله | 0.0000\n",
            "(33) ‫ כיפ | كيف | كيف | 0.0000\n",
            "(34) ‫ יחיי | يحيي | يحيي | 0.0000\n",
            "(35) ‫ רבה | ربّه | ربّه | 0.0000\n",
            "(36) ‫ אלמותי | الموتى | الموتى | 0.0000\n",
            "(37) ‫ אד' | إذ | إذ | 0.0000\n",
            "(38) ‫ קד | قد | قد | 0.0000\n",
            "(39) ‫ צח | صحّ | صحّ | 0.0000\n",
            "(40) ‫ לה | له | له | 0.0000\n",
            "(41) ‫ אנה | أنه | أن\u001b[1m\u001b[31mّ\u001b[0mه | 1.0000\n",
            "(42) ‫ כ'לק | خلق | خلق | 0.0000\n",
            "(43) ‫ שיא | شيئاً | شيئاً | 0.0000\n",
            "(44) ‫ לא | لا | لا | 0.0000\n",
            "(45) ‫ מנ | من | من | 0.0000\n",
            "(46) ‫ שי | شيء | شيء | 0.0000\n",
            "(47) ‫ פלא | فلا | فلا | 0.0000\n",
            "(48) ‫ יגוז | يجوز | يجوز | 0.0000\n",
            "(49) ‫ אנ | أن | أن\u001b[1m\u001b[31mّ\u001b[0m | 1.0000\n",
            "(50) ‫ יסתעסר | يستعسر | يستعسر | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  2.3583333333333334 50\n",
            "BATCH (sum_of_e_d,num_of_letters):  8 192\n",
            "#examples: 50 , accuracy: 0.9528333333333333\n",
            "#letters: 192 , accuracy1: 0.9583333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.04716666666666667, 0.041666666666666664)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L",
        "colab_type": "text"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST_BASELINE\"\n",
        "\n",
        "def baseline(this_dataset=test_dataset_double_kuzari,print_only_first_in_bath=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_batch, target_batch, inputs_len,targets_len in this_dataset:\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_batch[i])\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=decode_arr(target_batch[i],targets_len[i].numpy()).strip(BLANK)\n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if print_only_first_in_bath and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"baseline accuracy: \",1-total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 1-total_accuracy\n",
        "#baseline(limit=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WehJRO5foBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "762b259d-a988-4898-bd7c-1a1041d70646"
      },
      "source": [
        "baseline(limit=3)\n",
        "# print_log_screen(\"KUZARI TEST\")\n",
        "# baseline(test_dataset_double_kuzari)\n",
        "# print_log_screen(\"RASAG TEST\")\n",
        "# baseline(test_dataset_double_rasag)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1) ‫ ואהל אלאדיאנ ת'מ עלי | و\u001b[1m\u001b[31mأ\u001b[0mهل ال\u001b[1m\u001b[31mأ\u001b[0mديان ثم\u001b[1m\u001b[31mّ\u001b[0m عل\u001b[1m\u001b[31mى\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mهل ال\u001b[1m\u001b[31mا\u001b[0mديان ثم عل\u001b[1m\u001b[31mي\u001b[0m | 0.2000\n",
            "(2) ‫ הכד'א כאנ קומה מעה , | هكذا كان قومه معه , | هكذا كان قومه معه , | 0.0000\n",
            "(3) ‫ תקתצ'י אלמעאני אלתי יריד | تقتضي المعاني التي يريد | تقتضي المعاني التي يريد | 0.0000\n",
            "baseline accuracy:  0.9019730996119918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9019730996119918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWMVLTywwIW",
        "colab_type": "text"
      },
      "source": [
        "##TEST BASELINE KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrcOFoswu-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21a32d44-3fec-4e51-82b7-0e66b123934f"
      },
      "source": [
        "CELL_NAME=\"DEF TEST_BASELINE_KFIR\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#only once per file\n",
        "kfir_rasag_lines=preprocess_lines(\n",
        "    load_lines(kfir_rasag_test_SWITCH)\n",
        ")\n",
        "kfir_rasag_phrases=create_parralel_phrases(kfir_rasag_lines,1)\n",
        "\n",
        "kfir_kuzari_lines=preprocess_lines(\n",
        "    load_lines(kfir_kuzari_test_SWITCH)\n",
        ")\n",
        "kfir_kuzari_phrases=create_parralel_phrases(kfir_kuzari_lines,1)\n",
        "\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def baseline_kfir(pairs,replace_GAIN=False,SHOW_PRINT=False): \n",
        "  if replace_GAIN:\n",
        "    print_log_screen(\"replaceing gimel with jain to match my train convention\")\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs] #ייננבבגג''יי\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "  \n",
        "  total_examples=len(pairs)\n",
        "  total_loss=0\n",
        "  sum_of_e_dist=0\n",
        "  num_of_letters=0\n",
        "  for l in arab_lines:\n",
        "    num_of_letters+=len(l)\n",
        "  total_accuracy=0\n",
        "  line_counter=1\n",
        "  for heb_input,real in zip(hebrew_lines,arab_lines):\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)                \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                normalized_accuracy=accuracy/len(real)\n",
        "                total_accuracy+=normalized_accuracy\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                sum_of_e_dist+=accuracy\n",
        "                if SHOW_PRINT:\n",
        "                  print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",str(accuracy))\n",
        "                line_counter+=1\n",
        "\t\t\t\t\t\t\t   \n",
        "  total_accuracy/=total_examples\n",
        "  sum_of_e_dist/num_of_letters\n",
        "  print_log_screen(\"accuracy: \",1-total_accuracy)\n",
        "  print_log_screen(\"accuracy1: \",1-sum_of_e_dist/num_of_letters)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy,sum_of_e_dist/num_of_letters\n",
        "#baseline(limit=3)\n",
        "#baseline_kfir(kfir_kuzari_phrases)\n",
        "baseline_kfir(kfir_kuzari_phrases,False,True)\n",
        "#baseline_kfir(kfir_rasag_phrases)\n",
        "baseline_kfir(kfir_rasag_phrases,False,True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 50\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 500\n",
            "(1) ‫ לא | لا | لا | 0\n",
            "(2) ‫ תכ'אפ | تخاف | تخاف | 0\n",
            "(3) ‫ אלפנא | الفنا\u001b[1m\u001b[31mء\u001b[0m | الفنا | 1\n",
            "(4) ‫ אבדא | \u001b[1m\u001b[31mإ\u001b[0mبدا\u001b[1m\u001b[31mً\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mبدا | 2\n",
            "(5) ‫ פתציר | فتصير | فتصير | 0\n",
            "(6) ‫ נפס | نفس | نفس | 0\n",
            "(7) ‫ אלאנסאנ | ال\u001b[1m\u001b[31mإ\u001b[0mنسان | ال\u001b[1m\u001b[31mا\u001b[0mنسان | 1\n",
            "(8) ‫ אלכאמל | الكامل | الكامل | 0\n",
            "(9) ‫ וד'לכ | وذلك | وذلك | 0\n",
            "(10) ‫ אלעקל | العقل | العقل | 0\n",
            "(11) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا | 2\n",
            "(12) ‫ ואחדא | واحدا\u001b[1m\u001b[31mً\u001b[0m | واحدا | 1\n",
            "(13) ‫ פלא | فلا | فلا | 0\n",
            "(14) ‫ יבאלי | ي\u001b[1m\u001b[31mخ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | ي\u001b[1m\u001b[31mبا\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m | 3\n",
            "(15) ‫ בפנא | بفنا\u001b[1m\u001b[31mء\u001b[0m | بفنا | 1\n",
            "(16) ‫ ג'סדה | جسده | جسده | 0\n",
            "(17) ‫ ואלאתה | و\u001b[1m\u001b[31mآ\u001b[0mلاته | و\u001b[1m\u001b[31mا\u001b[0mلاته | 1\n",
            "(18) ‫ אד' | \u001b[1m\u001b[31mإ\u001b[0mذ | \u001b[1m\u001b[31mا\u001b[0mذ | 1\n",
            "(19) ‫ קד | قد | قد | 0\n",
            "(20) ‫ צאר | صار | صار | 0\n",
            "(21) ‫ וד'לכ | وذلك | وذلك | 0\n",
            "(22) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا | 2\n",
            "(23) ‫ ואחדא | واحدا\u001b[1m\u001b[31mً\u001b[0m | واحدا | 1\n",
            "(24) ‫ וטאבת | وطابت | وطابت | 0\n",
            "(25) ‫ נפסה | نفسه | نفسه | 0\n",
            "(26) ‫ פי | في | في | 0\n",
            "(27) ‫ אלחיאה | الحيا\u001b[1m\u001b[31mة\u001b[0m | الحيا\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(28) ‫ אד | \u001b[1m\u001b[31mإذ\u001b[0m | \u001b[1m\u001b[31mاد\u001b[0m | 2\n",
            "(29) ‫ צאר | صار | صار | 0\n",
            "(30) ‫ פי | في | في | 0\n",
            "(31) ‫ זמרה | زمر\u001b[1m\u001b[31mة\u001b[0m | زمر\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(32) ‫ הרמס | هرمس | هرمس | 0\n",
            "(33) ‫ ואסקלאביוס | و\u001b[1m\u001b[31mإ\u001b[0mسقلابيوس | و\u001b[1m\u001b[31mا\u001b[0mسقلابيوس | 1\n",
            "(34) ‫ וסקראט | وسقراط | وسقراط | 0\n",
            "(35) ‫ ואפלאטונ | و\u001b[1m\u001b[31mإ\u001b[0mفلاطون | و\u001b[1m\u001b[31mا\u001b[0mفلاطون | 1\n",
            "(36) ‫ וארסטוטאליס | و\u001b[1m\u001b[31mإ\u001b[0mرسطوطاليس | و\u001b[1m\u001b[31mا\u001b[0mرسطوطاليس | 1\n",
            "(37) ‫ בל | بل | بل | 0\n",
            "(38) ‫ הו | هو | هو | 0\n",
            "(39) ‫ והמ | وهم | وهم | 0\n",
            "(40) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1\n",
            "(41) ‫ מנ | من | من | 0\n",
            "(42) ‫ כאנ | كان | كان | 0\n",
            "(43) ‫ פי | في | في | 0\n",
            "(44) ‫ דרג'תהמ | درجتهم | درجتهم | 0\n",
            "(45) ‫ ואלעקל | والعقل | والعقل | 0\n",
            "(46) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1\n",
            "(47) ‫ שי | شي\u001b[1m\u001b[31mء\u001b[0m | شي | 1\n",
            "(48) ‫ ואחד | واحد | واحد | 0\n",
            "(49) ‫ פהד'א | فهذا | فهذا | 0\n",
            "(50) ‫ אלד'י | الذي | الذي | 0\n",
            "(51) ‫ יכני | يكن\u001b[1m\u001b[31mّى\u001b[0m | يكن\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(52) ‫ ענה | عنه | عنه | 0\n",
            "(53) ‫ ברצ'א | برضا | برضا | 0\n",
            "(54) ‫ אללה | الله | الله | 0\n",
            "(55) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(56) ‫ סביל | سبيل | سبيل | 0\n",
            "(57) ‫ אללגז | اللغز | اللغز | 0\n",
            "(58) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mا\u001b[0mو | 1\n",
            "(59) ‫ אלתקריב | التقريب | التقريب | 0\n",
            "(60) ‫ פאתבעה | فات\u001b[1m\u001b[31mّ\u001b[0mبعه | فاتبعه | 1\n",
            "(61) ‫ ואתבע | وات\u001b[1m\u001b[31mّ\u001b[0mبع | واتبع | 1\n",
            "(62) ‫ אלעלמ | العلم | العلم | 0\n",
            "(63) ‫ בחקאיק | بحقا\u001b[1m\u001b[31mئ\u001b[0mق | بحقا\u001b[1m\u001b[31mي\u001b[0mق | 1\n",
            "(64) ‫ אלאמור | ال\u001b[1m\u001b[31mإ\u001b[0mمور | ال\u001b[1m\u001b[31mا\u001b[0mمور | 1\n",
            "(65) ‫ ליציר | ليصير | ليصير | 0\n",
            "(66) ‫ עקלכ | عقلك | عقلك | 0\n",
            "(67) ‫ פעלא | ف\u001b[1m\u001b[31mا\u001b[0mعلا\u001b[1m\u001b[31mً\u001b[0m | فعلا | 2\n",
            "(68) ‫ לא | لا | لا | 0\n",
            "(69) ‫ מנפעלא | منفعلا\u001b[1m\u001b[31mً\u001b[0m | منفعلا | 1\n",
            "(70) ‫ ואלזמ | والزم | والزم | 0\n",
            "(71) ‫ אעדל | \u001b[1m\u001b[31mإ\u001b[0mعدل | \u001b[1m\u001b[31mا\u001b[0mعدل | 1\n",
            "(72) ‫ אלטרק | الطرق | الطرق | 0\n",
            "(73) ‫ פי | في | في | 0\n",
            "(74) ‫ אלאכ'לאק | ال\u001b[1m\u001b[31mإ\u001b[0mخلاق | ال\u001b[1m\u001b[31mا\u001b[0mخلاق | 1\n",
            "(75) ‫ ואלאעמאל | وال\u001b[1m\u001b[31mإ\u001b[0mعمال | وال\u001b[1m\u001b[31mا\u001b[0mعمال | 1\n",
            "(76) ‫ לאנה | ل\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mه | ل\u001b[1m\u001b[31mا\u001b[0mنه | 2\n",
            "(77) ‫ מעונה | معون\u001b[1m\u001b[31mة\u001b[0m | معون\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(78) ‫ פי | في | في | 0\n",
            "(79) ‫ תצור | تصو\u001b[1m\u001b[31mّ\u001b[0mر | تصور | 1\n",
            "(80) ‫ אלחק | الحق\u001b[1m\u001b[31mّ\u001b[0m | الحق | 1\n",
            "(81) ‫ ולזומ | ولزوم | ولزوم | 0\n",
            "(82) ‫ אלתעלמ | التعل\u001b[1m\u001b[31mّ\u001b[0mم | التعلم | 1\n",
            "(83) ‫ ואלתשבה | والتشب\u001b[1m\u001b[31mّ\u001b[0mه | والتشبه | 1\n",
            "(84) ‫ בד'לכ | بذلك | بذلك | 0\n",
            "(85) ‫ אלעקל | العقل | العقل | 0\n",
            "(86) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1\n",
            "(87) ‫ ויתבע | ويتبع | ويتبع | 0\n",
            "(88) ‫ הד'א | هذا | هذا | 0\n",
            "(89) ‫ אלקנוע | القنوع | القنوع | 0\n",
            "(90) ‫ ואלכ'צ'וע | والخضوع | والخضوع | 0\n",
            "(91) ‫ ואלכ'שוע | والخشوع | والخشوع | 0\n",
            "(92) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1\n",
            "(93) ‫ כ'לק | خلق | خلق | 0\n",
            "(94) ‫ פאצ'ל | فاضل | فاضل | 0\n",
            "(95) ‫ מע | مع | مع | 0\n",
            "(96) ‫ אלתעט'ימ | التعظيم | التعظيم | 0\n",
            "(97) ‫ ללסבב | للسبب | للسبب | 0\n",
            "(98) ‫ אלאול | ال\u001b[1m\u001b[31mإ\u001b[0mو\u001b[1m\u001b[31mّ\u001b[0mل | ال\u001b[1m\u001b[31mا\u001b[0mول | 2\n",
            "(99) ‫ לא | لا | لا | 0\n",
            "(100) ‫ ליהבכ | ليهبك | ليهبك | 0\n",
            "(101) ‫ רצ'אה | رضاه | رضاه | 0\n",
            "(102) ‫ ולא | ولا | ولا | 0\n",
            "(103) ‫ ליזיל | ليزيل | ليزيل | 0\n",
            "(104) ‫ ענכ | عنك | عنك | 0\n",
            "(105) ‫ סכ'טה | سخطه | سخطه | 0\n",
            "(106) ‫ בל | بل | بل | 0\n",
            "(107) ‫ ללתשבה | للتشب\u001b[1m\u001b[31mّ\u001b[0mه | للتشبه | 1\n",
            "(108) ‫ ללעקל | للعقل | للعقل | 0\n",
            "(109) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1\n",
            "(110) ‫ פי | في | في | 0\n",
            "(111) ‫ אית'אר | \u001b[1m\u001b[31mإ\u001b[0mيثار | \u001b[1m\u001b[31mا\u001b[0mيثار | 1\n",
            "(112) ‫ אלחק | الحق\u001b[1m\u001b[31mّ\u001b[0m | الحق | 1\n",
            "(113) ‫ ווצפ | ووصف | ووصف | 0\n",
            "(114) ‫ כל | كل\u001b[1m\u001b[31mّ\u001b[0m | كل | 1\n",
            "(115) ‫ שי | شي\u001b[1m\u001b[31mء\u001b[0m | شي | 1\n",
            "(116) ‫ במא | بما | بما | 0\n",
            "(117) ‫ יג'ב | يجب | يجب | 0\n",
            "(118) ‫ לה | له | له | 0\n",
            "(119) ‫ ואעתקאדה | واعتقاده | واعتقاده | 0\n",
            "(120) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(121) ‫ מא | ما | ما | 0\n",
            "(122) ‫ הו | هو | هو | 0\n",
            "(123) ‫ עליה | عليه | عليه | 0\n",
            "(124) ‫ פהד'א | فهذا | فهذا | 0\n",
            "(125) ‫ מנ | من | من | 0\n",
            "(126) ‫ צפאת | صفات | صفات | 0\n",
            "(127) ‫ אלעקל | العقل | العقل | 0\n",
            "(128) ‫ פאד' | ف\u001b[1m\u001b[31mإ\u001b[0mذ | ف\u001b[1m\u001b[31mا\u001b[0mذ | 1\n",
            "(129) ‫ צרת | صرت | صرت | 0\n",
            "(130) ‫ בהד'ה | بهذه | بهذه | 0\n",
            "(131) ‫ אלצפה | الصف\u001b[1m\u001b[31mة\u001b[0m | الصف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(132) ‫ מנ | من | من | 0\n",
            "(133) ‫ אלאעתקאד | الاعتقاد | الاعتقاد | 0\n",
            "(134) ‫ לא | لا | لا | 0\n",
            "(135) ‫ תבאלי | تبالي | تبالي | 0\n",
            "(136) ‫ באי | ب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | ب\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(137) ‫ שרע | شرع | شرع | 0\n",
            "(138) ‫ תשרעת | تشر\u001b[1m\u001b[31mّ\u001b[0mعت | تشرعت | 1\n",
            "(139) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mا\u001b[0mو | 1\n",
            "(140) ‫ תדינת | تدي\u001b[1m\u001b[31mّ\u001b[0mنت | تدينت | 1\n",
            "(141) ‫ ועט'מת | وعظ\u001b[1m\u001b[31mّ\u001b[0mمت | وعظمت | 1\n",
            "(142) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(143) ‫ קול | قول | قول | 0\n",
            "(144) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(145) ‫ לסאנ | لسان | لسان | 0\n",
            "(146) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(147) ‫ אעמאל | \u001b[1m\u001b[31mإ\u001b[0mعمال | \u001b[1m\u001b[31mا\u001b[0mعمال | 1\n",
            "(148) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mا\u001b[0mو | 1\n",
            "(149) ‫ אכ'תרע | اخترع | اخترع | 0\n",
            "(150) ‫ לנפסכ | لنفسك | لنفسك | 0\n",
            "(151) ‫ דינא | دينا\u001b[1m\u001b[31mً\u001b[0m | دينا | 1\n",
            "(152) ‫ למעני | لمعن\u001b[1m\u001b[31mى\u001b[0m | لمعن\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(153) ‫ אלתכ'שע | التخش\u001b[1m\u001b[31mّ\u001b[0mع | التخشع | 1\n",
            "(154) ‫ ואלתעט'ימ | والتعظيم | والتعظيم | 0\n",
            "(155) ‫ ואלתסביח | والتسبيح | والتسبيح | 0\n",
            "(156) ‫ ולתדביר | وتدبير | و\u001b[1m\u001b[31mل\u001b[0mتدبير | 1\n",
            "(157) ‫ אכ'לאקכ | \u001b[1m\u001b[31mإ\u001b[0mخلاقك | \u001b[1m\u001b[31mا\u001b[0mخلاقك | 1\n",
            "(158) ‫ ותדביר | وتدبير | وتدبير | 0\n",
            "(159) ‫ מנזלכ | منزلك | منزلك | 0\n",
            "(160) ‫ ומדינתכ | ومدينتك | ومدينتك | 0\n",
            "(161) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن | \u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(162) ‫ כנת | كنت | كنت | 0\n",
            "(163) ‫ מקבולא | مقبولا\u001b[1m\u001b[31mً\u001b[0m | مقبولا | 1\n",
            "(164) ‫ מנהמ | منهم | منهم | 0\n",
            "(165) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mا\u001b[0mو | 1\n",
            "(166) ‫ תדינ | تدي\u001b[1m\u001b[31mّ\u001b[0mن | تدين | 1\n",
            "(167) ‫ באלנואמיס | بالنواميس | بالنواميس | 0\n",
            "(168) ‫ אלעקליה | العقلي\u001b[1m\u001b[31mة\u001b[0m | العقلي\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(169) ‫ אלמולפה | الم\u001b[1m\u001b[31mؤ\u001b[0mل\u001b[1m\u001b[31mّ\u001b[0mف\u001b[1m\u001b[31mة\u001b[0m | الم\u001b[1m\u001b[31mو\u001b[0mلف\u001b[1m\u001b[31mه\u001b[0m | 3\n",
            "(170) ‫ ללפלאספה | للفلاسف\u001b[1m\u001b[31mة\u001b[0m | للفلاسف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(171) ‫ ואג'על | واجعل | واجعل | 0\n",
            "(172) ‫ קצדכ | قصدك | قصدك | 0\n",
            "(173) ‫ וגרצ'כ | وغرضك | وغرضك | 0\n",
            "(174) ‫ צפא | صفا\u001b[1m\u001b[31mء\u001b[0m | صفا | 1\n",
            "(175) ‫ נפסכ | نفسك | نفسك | 0\n",
            "(176) ‫ ובאלג'מלה | وبالجمل\u001b[1m\u001b[31mة\u001b[0m | وبالجمل\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(177) ‫ פאטלב | فاطلب | فاطلب | 0\n",
            "(178) ‫ צפא | صفا\u001b[1m\u001b[31mء\u001b[0m | صفا | 1\n",
            "(179) ‫ אלקלב | القلب | القلب | 0\n",
            "(180) ‫ באי | ب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | ب\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(181) ‫ וג'ה | وجه | وجه | 0\n",
            "(182) ‫ אמכנכ | \u001b[1m\u001b[31mإ\u001b[0mمكنك | \u001b[1m\u001b[31mا\u001b[0mمكنك | 1\n",
            "(183) ‫ בעד | بعد | بعد | 0\n",
            "(184) ‫ תחציל | تحصيل | تحصيل | 0\n",
            "(185) ‫ כליאת | كل\u001b[1m\u001b[31mّ\u001b[0mيات | كليات | 1\n",
            "(186) ‫ אלעלומ | العلوم | العلوم | 0\n",
            "(187) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(188) ‫ חקאיקהא | حقا\u001b[1m\u001b[31mئ\u001b[0mقها | حقا\u001b[1m\u001b[31mي\u001b[0mقها | 1\n",
            "(189) ‫ פתצאדפ | فتصادف | فتصادف | 0\n",
            "(190) ‫ מטלובכ | مطلبك | مطل\u001b[1m\u001b[31mو\u001b[0mبك | 1\n",
            "(191) ‫ אעני | \u001b[1m\u001b[31mإ\u001b[0mعني | \u001b[1m\u001b[31mا\u001b[0mعني | 1\n",
            "(192) ‫ אלאתצאל | الات\u001b[1m\u001b[31mّ\u001b[0mصال | الاتصال | 1\n",
            "(193) ‫ בד'לכ | بذلك | بذلك | 0\n",
            "(194) ‫ אלרוחאני | الروحاني\u001b[1m\u001b[31mّ\u001b[0m | الروحاني | 1\n",
            "(195) ‫ אעני | \u001b[1m\u001b[31mإ\u001b[0mعني | \u001b[1m\u001b[31mا\u001b[0mعني | 1\n",
            "(196) ‫ אלעקל | العقل | العقل | 0\n",
            "(197) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1\n",
            "(198) ‫ ורבמא | ورب\u001b[1m\u001b[31mّ\u001b[0mما | وربما | 1\n",
            "(199) ‫ אנבאכ | \u001b[1m\u001b[31mإ\u001b[0mنب\u001b[1m\u001b[31mإ\u001b[0mك | \u001b[1m\u001b[31mا\u001b[0mنب\u001b[1m\u001b[31mا\u001b[0mك | 2\n",
            "(200) ‫ ואמרכ | و\u001b[1m\u001b[31mإ\u001b[0mمرك | و\u001b[1m\u001b[31mا\u001b[0mمرك | 1\n",
            "(201) ‫ בעלמ | بعلم | بعلم | 0\n",
            "(202) ‫ גיב | غيب | غيب | 0\n",
            "(203) ‫ מנ | من | من | 0\n",
            "(204) ‫ מנאמאת | منامات | منامات | 0\n",
            "(205) ‫ צאדקה | صادق\u001b[1m\u001b[31mة\u001b[0m | صادق\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(206) ‫ וכ'יאלאת | وخيالات | وخيالات | 0\n",
            "(207) ‫ מציבה | مصيب\u001b[1m\u001b[31mة\u001b[0m | مصيب\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(208) ‫ קאל | قال | قال | 0\n",
            "(209) ‫ לה | له | له | 0\n",
            "(210) ‫ אלכ'זרי | الخزري\u001b[1m\u001b[31mّ\u001b[0m | الخزري | 1\n",
            "(211) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(212) ‫ כלאמכ | كلامك | كلامك | 0\n",
            "(213) ‫ למקנע | لمقنع | لمقنع | 0\n",
            "(214) ‫ לכנה | لكن\u001b[1m\u001b[31mّ\u001b[0mه | لكنه | 1\n",
            "(215) ‫ גיר | غير | غير | 0\n",
            "(216) ‫ מטאבק | مطابق | مطابق | 0\n",
            "(217) ‫ לטלבתי | لطلبتي | لطلبتي | 0\n",
            "(218) ‫ לאני | ل\u001b[1m\u001b[31mإ\u001b[0mني\u001b[1m\u001b[31mّ\u001b[0m | ل\u001b[1m\u001b[31mا\u001b[0mني | 2\n",
            "(219) ‫ אעלמ | \u001b[1m\u001b[31mإ\u001b[0mعلم | \u001b[1m\u001b[31mا\u001b[0mعلم | 1\n",
            "(220) ‫ מנ | من | من | 0\n",
            "(221) ‫ נפסי | نفسي | نفسي | 0\n",
            "(222) ‫ אני | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mي | \u001b[1m\u001b[31mا\u001b[0mني | 2\n",
            "(223) ‫ צאפי | صافي | صافي | 0\n",
            "(224) ‫ אלנפס | النفس | النفس | 0\n",
            "(225) ‫ מסדד | مسد\u001b[1m\u001b[31mّ\u001b[0mد | مسدد | 1\n",
            "(226) ‫ אלאעמאל | ال\u001b[1m\u001b[31mإ\u001b[0mعمال | ال\u001b[1m\u001b[31mا\u001b[0mعمال | 1\n",
            "(227) ‫ נחו | نحو | نحو | 0\n",
            "(228) ‫ רצ'א | رضا | رضا | 0\n",
            "(229) ‫ אלרב | الرب\u001b[1m\u001b[31mّ\u001b[0m | الرب | 1\n",
            "(230) ‫ לכנ | لكن | لكن | 0\n",
            "(231) ‫ כאנ | كان | كان | 0\n",
            "(232) ‫ ג'ואבי | جوابي | جوابي | 0\n",
            "(233) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(234) ‫ הד'א | هذا | هذا | 0\n",
            "(235) ‫ אלעמל | العمل | العمل | 0\n",
            "(236) ‫ ליס | ليس | ليس | 0\n",
            "(237) ‫ במרצ'י | بمرضي\u001b[1m\u001b[31mّ\u001b[0m | بمرضي | 1\n",
            "(238) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن | و\u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(239) ‫ כאנת | كانت | كانت | 0\n",
            "(240) ‫ אלניה | الني\u001b[1m\u001b[31mّة\u001b[0m | الني\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(241) ‫ מרצ'יה | مرضي\u001b[1m\u001b[31mّة\u001b[0m | مرضي\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(242) ‫ פלא | فلا | فلا | 0\n",
            "(243) ‫ שכ | شك\u001b[1m\u001b[31mّ\u001b[0m | شك | 1\n",
            "(244) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(245) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1\n",
            "(246) ‫ עמלא | عملا\u001b[1m\u001b[31mً\u001b[0m | عملا | 1\n",
            "(247) ‫ מא | ما | ما | 0\n",
            "(248) ‫ מרצ'יא | مرضيا\u001b[1m\u001b[31mً\u001b[0m | مرضيا | 1\n",
            "(249) ‫ בד'אתה | بذاته | بذاته | 0\n",
            "(250) ‫ לא | لا | لا | 0\n",
            "(251) ‫ בחסב | بحسب | بحسب | 0\n",
            "(252) ‫ אלט'נונ | الظنون | الظنون | 0\n",
            "(253) ‫ ואלא | و\u001b[1m\u001b[31mإ\u001b[0mلا\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mلا | 2\n",
            "(254) ‫ פאנ | ف\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | ف\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(255) ‫ אלנצראני | النصراني\u001b[1m\u001b[31mّ\u001b[0m | النصراني | 1\n",
            "(256) ‫ ואלמסלמ | والمسلم | والمسلم | 0\n",
            "(257) ‫ אללד'ינ | اللذين | اللذين | 0\n",
            "(258) ‫ אקתסמא | اقتسما | اقتسما | 0\n",
            "(259) ‫ אלמעמורה | المعمور\u001b[1m\u001b[31mة\u001b[0m | المعمور\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(260) ‫ יתקאתלאנ | يتقاتلان | يتقاتلان | 0\n",
            "(261) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1\n",
            "(262) ‫ ואחד | واحد | واحد | 0\n",
            "(263) ‫ מנהמא | منهما | منهما | 0\n",
            "(264) ‫ קד | قد | قد | 0\n",
            "(265) ‫ אצפי | \u001b[1m\u001b[31mإ\u001b[0mصف\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mصف\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(266) ‫ ניתה | ني\u001b[1m\u001b[31mّ\u001b[0mته | نيته | 1\n",
            "(267) ‫ ללה | لله | لله | 0\n",
            "(268) ‫ ותרהב | وتره\u001b[1m\u001b[31mّ\u001b[0mب | وترهب | 1\n",
            "(269) ‫ ותזהד | وتزه\u001b[1m\u001b[31mّ\u001b[0mد | وتزهد | 1\n",
            "(270) ‫ וצאמ | وصام | وصام | 0\n",
            "(271) ‫ וצלי | وصل\u001b[1m\u001b[31mّى\u001b[0m | وصل\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(272) ‫ ומצ'י | ومض\u001b[1m\u001b[31mى\u001b[0m | ومض\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(273) ‫ מצממא | مصم\u001b[1m\u001b[31mّ\u001b[0mما\u001b[1m\u001b[31mً\u001b[0m | مصمما | 2\n",
            "(274) ‫ לקתל | لقتل | لقتل | 0\n",
            "(275) ‫ צאחבה | صاحبه | صاحبه | 0\n",
            "(276) ‫ והו | وهو | وهو | 0\n",
            "(277) ‫ יעתקד | يعتقد | يعتقد | 0\n",
            "(278) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(279) ‫ פי | في | في | 0\n",
            "(280) ‫ קתלה | قتله | قتله | 0\n",
            "(281) ‫ אעט'מ | \u001b[1m\u001b[31mإ\u001b[0mعظم | \u001b[1m\u001b[31mا\u001b[0mعظم | 1\n",
            "(282) ‫ חסנה | حسن\u001b[1m\u001b[31mة\u001b[0m | حسن\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(283) ‫ ותקרב | وتقر\u001b[1m\u001b[31mّ\u001b[0mب | وتقرب | 1\n",
            "(284) ‫ אלי | \u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(285) ‫ אללה | الله | الله | 0\n",
            "(286) ‫ פיקתתלאנ | فيقتتلان | فيقتتلان | 0\n",
            "(287) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1\n",
            "(288) ‫ ואחד | واحد | واحد | 0\n",
            "(289) ‫ מנהמא | منهما | منهما | 0\n",
            "(290) ‫ יעתקד | يعتقد | يعتقد | 0\n",
            "(291) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(292) ‫ מסירה | مسيره | مسيره | 0\n",
            "(293) ‫ אלי | \u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(294) ‫ אלג'נה | الجن\u001b[1m\u001b[31mّة\u001b[0m | الجن\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(295) ‫ ואלפרדוס | والفردوس | والفردوس | 0\n",
            "(296) ‫ ותצידקהמא | وتصد\u001b[1m\u001b[31mي\u001b[0mقهما | وتص\u001b[1m\u001b[31mي\u001b[0mدقهما | 2\n",
            "(297) ‫ מחאל | محال | محال | 0\n",
            "(298) ‫ ענד | عند | عند | 0\n",
            "(299) ‫ אלעקל | العقل | العقل | 0\n",
            "(300) ‫ קאל | قال | قال | 0\n",
            "(301) ‫ אלפילסופ | الفيلسوف | الفيلسوف | 0\n",
            "(302) ‫ ליס | ليس | ليس | 0\n",
            "(303) ‫ פי | في | في | 0\n",
            "(304) ‫ דינ | دين | دين | 0\n",
            "(305) ‫ אלפלאספה | الفلاسف\u001b[1m\u001b[31mة\u001b[0m | الفلاسف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(306) ‫ קתל | قتل | قتل | 0\n",
            "(307) ‫ ואחד | واحد | واحد | 0\n",
            "(308) ‫ מנ | من | من | 0\n",
            "(309) ‫ האולא | ه\u001b[1m\u001b[31mؤ\u001b[0mلا\u001b[1m\u001b[31mء\u001b[0m | ه\u001b[1m\u001b[31mاو\u001b[0mلا | 3\n",
            "(310) ‫ אד' | \u001b[1m\u001b[31mإ\u001b[0mذ | \u001b[1m\u001b[31mا\u001b[0mذ | 1\n",
            "(311) ‫ יומונ | ي\u001b[1m\u001b[31mؤ\u001b[0mم\u001b[1m\u001b[31mّ\u001b[0mون | ي\u001b[1m\u001b[31mو\u001b[0mمون | 2\n",
            "(312) ‫ אלעקל | العقل | العقل | 0\n",
            "(313) ‫ קאל | قال | قال | 0\n",
            "(314) ‫ אלכ'זרי | الخزري\u001b[1m\u001b[31mّ\u001b[0m | الخزري | 1\n",
            "(315) ‫ ואי | و\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mي | 2\n",
            "(316) ‫ חירה | حير\u001b[1m\u001b[31mةٍ\u001b[0m | حير\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(317) ‫ ענד | عند | عند | 0\n",
            "(318) ‫ אלפלאספה | الفلاسف\u001b[1m\u001b[31mة\u001b[0m | الفلاسف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(319) ‫ אעט'מ | \u001b[1m\u001b[31mإ\u001b[0mعظم | \u001b[1m\u001b[31mا\u001b[0mعظم | 1\n",
            "(320) ‫ מנ | من | من | 0\n",
            "(321) ‫ אעתקאדהמ | اعتقادهم | اعتقادهم | 0\n",
            "(322) ‫ אלחד'ת | الح\u001b[1m\u001b[31mدث\u001b[0m | الح\u001b[1m\u001b[31mذت\u001b[0m | 2\n",
            "(323) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(324) ‫ אלעאלמ | العالم | العالم | 0\n",
            "(325) ‫ כ'לק | خلق | خلق | 0\n",
            "(326) ‫ פי | في | في | 0\n",
            "(327) ‫ סתה' | ستة | ستة | 0\n",
            "(328) ‫ איאמ | \u001b[1m\u001b[31mإ\u001b[0mيام | \u001b[1m\u001b[31mا\u001b[0mيام | 1\n",
            "(329) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(330) ‫ אלסבב | السبب | السبب | 0\n",
            "(331) ‫ אלאול | ال\u001b[1m\u001b[31mإ\u001b[0mو\u001b[1m\u001b[31mّ\u001b[0mل | ال\u001b[1m\u001b[31mا\u001b[0mول | 2\n",
            "(332) ‫ יכלמ | يكل\u001b[1m\u001b[31mّ\u001b[0mم | يكلم | 1\n",
            "(333) ‫ שכ'צא | شخصا\u001b[1m\u001b[31mً\u001b[0m | شخصا | 1\n",
            "(334) ‫ מנ | من | من | 0\n",
            "(335) ‫ אלנאס | الناس | الناس | 0\n",
            "(336) ‫ פצ'לא | فضلا\u001b[1m\u001b[31mً\u001b[0m | فضلا | 1\n",
            "(337) ‫ ענ | عن | عن | 0\n",
            "(338) ‫ ד'לכ | ذلك | ذلك | 0\n",
            "(339) ‫ אלתנזיה | التنزيه | التنزيه | 0\n",
            "(340) ‫ אלד'י | الذي | الذي | 0\n",
            "(341) ‫ תנזהה | تنز\u001b[1m\u001b[31mّ\u001b[0mهه | تنزهه | 1\n",
            "(342) ‫ אלפלאספה | الفلاسف\u001b[1m\u001b[31mة\u001b[0m | الفلاسف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(343) ‫ ענ | عن | عن | 0\n",
            "(344) ‫ מערפה | معرف\u001b[1m\u001b[31mة\u001b[0m | معرف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(345) ‫ אלג'זאיאת | الجز\u001b[1m\u001b[31mئ\u001b[0mيات | الجز\u001b[1m\u001b[31mا\u001b[0mيات | 1\n",
            "(346) ‫ ומע | ومع | ومع | 0\n",
            "(347) ‫ הד'א | هذا | هذا | 0\n",
            "(348) ‫ פכאנ | فكان | فكان | 0\n",
            "(349) ‫ ינבגי | ينبغي | ينبغي | 0\n",
            "(350) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(351) ‫ אעמאל | \u001b[1m\u001b[31mإ\u001b[0mعمال | \u001b[1m\u001b[31mا\u001b[0mعمال | 1\n",
            "(352) ‫ אלפלאספה | الفلاسف\u001b[1m\u001b[31mة\u001b[0m | الفلاسف\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(353) ‫ ועלומהמ | وعلومهم | وعلومهم | 0\n",
            "(354) ‫ ותחקיקיהמ | وتحقيقهم | وتحقيق\u001b[1m\u001b[31mي\u001b[0mهم | 1\n",
            "(355) ‫ ואג'תהאדהמ | واجتهادهم | واجتهادهم | 0\n",
            "(356) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن | \u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(357) ‫ תכונ | تكون | تكون | 0\n",
            "(358) ‫ אלנבוה | النبو\u001b[1m\u001b[31mّة\u001b[0m | النبو\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(359) ‫ משהורה | مشهور\u001b[1m\u001b[31mة\u001b[0m | مشهور\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(360) ‫ פיהמ | فيهم | فيهم | 0\n",
            "(361) ‫ שאיעה | شا\u001b[1m\u001b[31mئ\u001b[0mع\u001b[1m\u001b[31mة\u001b[0m | شا\u001b[1m\u001b[31mي\u001b[0mع\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(362) ‫ בינהמ | بينهم | بينهم | 0\n",
            "(363) ‫ לאתצאלהמ | لات\u001b[1m\u001b[31mّ\u001b[0mصالهم | لاتصالهم | 1\n",
            "(364) ‫ באלרוחאניאת | بالروحانيات | بالروحانيات | 0\n",
            "(365) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن | و\u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(366) ‫ יוצפ | يوصف | يوصف | 0\n",
            "(367) ‫ ענהמ | عنهم | عنهم | 0\n",
            "(368) ‫ גראיב | غرا\u001b[1m\u001b[31mئ\u001b[0mب | غرا\u001b[1m\u001b[31mي\u001b[0mب | 1\n",
            "(369) ‫ ומעג'זאת | ومعجزات | ومعجزات | 0\n",
            "(370) ‫ וכראמאת | وكرامات | وكرامات | 0\n",
            "(371) ‫ ולקד | ولقد | ولقد | 0\n",
            "(372) ‫ נרי | نر\u001b[1m\u001b[31mى\u001b[0m | نر\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(373) ‫ אלמנאמאת | المنامات | المنامات | 0\n",
            "(374) ‫ אלצאדקה | الصادق\u001b[1m\u001b[31mة\u001b[0m | الصادق\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(375) ‫ למנ | لمن | لمن | 0\n",
            "(376) ‫ למ | لم | لم | 0\n",
            "(377) ‫ יענ | يعن | يعن | 0\n",
            "(378) ‫ באלעלמ | بالعلم | بالعلم | 0\n",
            "(379) ‫ ולא | ولا | ولا | 0\n",
            "(380) ‫ באצפא | ب\u001b[1m\u001b[31mإ\u001b[0mصفا\u001b[1m\u001b[31mء\u001b[0m | ب\u001b[1m\u001b[31mا\u001b[0mصفا | 2\n",
            "(381) ‫ נפסה | نفسه | نفسه | 0\n",
            "(382) ‫ ונג'ד | ونجد | ونجد | 0\n",
            "(383) ‫ צ'ד | ضد\u001b[1m\u001b[31mّ\u001b[0m | ضد | 1\n",
            "(384) ‫ ד'לכ | ذلك | ذلك | 0\n",
            "(385) ‫ פי | في | في | 0\n",
            "(386) ‫ מנ | من | من | 0\n",
            "(387) ‫ ראמה | رامه | رامه | 0\n",
            "(388) ‫ פדל | فدل\u001b[1m\u001b[31mّ\u001b[0m | فدل | 1\n",
            "(389) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(390) ‫ ללאמר | لل\u001b[1m\u001b[31mإ\u001b[0mمر | لل\u001b[1m\u001b[31mا\u001b[0mمر | 1\n",
            "(391) ‫ אלאלאהי | ال\u001b[1m\u001b[31mإ\u001b[0mلهي\u001b[1m\u001b[31mّ\u001b[0m | ال\u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mا\u001b[0mهي | 3\n",
            "(392) ‫ וללנפוס | وللنفوس | وللنفوس | 0\n",
            "(393) ‫ סרא | سر\u001b[1m\u001b[31mّ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | سرا | 2\n",
            "(394) ‫ סוי | سو\u001b[1m\u001b[31mى\u001b[0m | سو\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(395) ‫ מא | ما | ما | 0\n",
            "(396) ‫ ד'כרתה | ذكرته | ذكرته | 0\n",
            "(397) ‫ יא | يا | يا | 0\n",
            "(398) ‫ פילסופ | فيلسوف | فيلسوف | 0\n",
            "(399) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1\n",
            "(400) ‫ קאל | قال | قال | 0\n",
            "(401) ‫ אלכ'זרי | الخزري\u001b[1m\u001b[31mّ\u001b[0m | الخزري | 1\n",
            "(402) ‫ פי | في | في | 0\n",
            "(403) ‫ נפסה | نفسه | نفسه | 0\n",
            "(404) ‫ אסאל | \u001b[1m\u001b[31mإ\u001b[0mس\u001b[1m\u001b[31mإ\u001b[0mل | \u001b[1m\u001b[31mا\u001b[0mس\u001b[1m\u001b[31mا\u001b[0mل | 2\n",
            "(405) ‫ אלנצארי | النصار\u001b[1m\u001b[31mى\u001b[0m | النصار\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(406) ‫ ואלמסלמינ | والمسلمين | والمسلمين | 0\n",
            "(407) ‫ פאנ | ف\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | ف\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(408) ‫ אחד | \u001b[1m\u001b[31mإ\u001b[0mحد | \u001b[1m\u001b[31mا\u001b[0mحد | 1\n",
            "(409) ‫ אלעמלינ | العملين | العملين | 0\n",
            "(410) ‫ הו | هو | هو | 0\n",
            "(411) ‫ לא | لا | لا | 0\n",
            "(412) ‫ שכ | شك | شك | 0\n",
            "(413) ‫ אלמרצ'י | المرضي\u001b[1m\u001b[31mّ\u001b[0m | المرضي | 1\n",
            "(414) ‫ ואמא | و\u001b[1m\u001b[31mإ\u001b[0mم\u001b[1m\u001b[31mّ\u001b[0mا | و\u001b[1m\u001b[31mا\u001b[0mما | 2\n",
            "(415) ‫ אליהוד | اليهود | اليهود | 0\n",
            "(416) ‫ פכפי | فكف\u001b[1m\u001b[31mى\u001b[0m | فكف\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(417) ‫ מא | ما | ما | 0\n",
            "(418) ‫ ט'הר | ظهر | ظهر | 0\n",
            "(419) ‫ מנ | من | من | 0\n",
            "(420) ‫ ד'לתהמ | ذل\u001b[1m\u001b[31mّ\u001b[0mتهم | ذلتهم | 1\n",
            "(421) ‫ וקלתהמ | وقل\u001b[1m\u001b[31mّ\u001b[0mتهم | وقلتهم | 1\n",
            "(422) ‫ ומקת | ومقت | ومقت | 0\n",
            "(423) ‫ אלג'מיע | الجميع | الجميع | 0\n",
            "(424) ‫ להמ | لهم | لهم | 0\n",
            "(425) ‫ פדעא | فدعا | فدعا | 0\n",
            "(426) ‫ בעאלמ | بعالم | بعالم | 0\n",
            "(427) ‫ מנ | من | من | 0\n",
            "(428) ‫ עלמא | علما\u001b[1m\u001b[31mء\u001b[0m | علما | 1\n",
            "(429) ‫ אלנצארי | النصار\u001b[1m\u001b[31mى\u001b[0m | النصار\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(430) ‫ פסאלה | فس\u001b[1m\u001b[31mإ\u001b[0mله | فس\u001b[1m\u001b[31mا\u001b[0mله | 1\n",
            "(431) ‫ ענ | عن | عن | 0\n",
            "(432) ‫ עלמה | علمه | علمه | 0\n",
            "(433) ‫ ועמלה | وعمله | وعمله | 0\n",
            "(434) ‫ פקאל | فقال | فقال | 0\n",
            "(435) ‫ לה | له | له | 0\n",
            "(436) ‫ אנא | \u001b[1m\u001b[31mإ\u001b[0mنا | \u001b[1m\u001b[31mا\u001b[0mنا | 1\n",
            "(437) ‫ מומנ | م\u001b[1m\u001b[31mؤ\u001b[0mمن | م\u001b[1m\u001b[31mو\u001b[0mمن | 1\n",
            "(438) ‫ באלחדת' | بالحدث | بالحدث | 0\n",
            "(439) ‫ ללמכ'לוקאת | للمخلوقات | للمخلوقات | 0\n",
            "(440) ‫ ובאלקדמ | وبالقدم | وبالقدم | 0\n",
            "(441) ‫ לכ'אלק | \u001b[1m\u001b[31mل\u001b[0mلخالق | لخالق | 1\n",
            "(442) ‫ תע' | تع\u001b[1m\u001b[31mالى\u001b[0m | تع\u001b[1m\u001b[31m'\u001b[0m | 3\n",
            "(443) ‫ ואנה | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mه | و\u001b[1m\u001b[31mا\u001b[0mنه | 2\n",
            "(444) ‫ כ'לק | خلق | خلق | 0\n",
            "(445) ‫ אלעאלמ | العالم | العالم | 0\n",
            "(446) ‫ באסרה | ب\u001b[1m\u001b[31mإ\u001b[0mسره | ب\u001b[1m\u001b[31mا\u001b[0mسره | 1\n",
            "(447) ‫ פי | في | في | 0\n",
            "(448) ‫ סתה' | ستة | ستة | 0\n",
            "(449) ‫ איאמ | \u001b[1m\u001b[31mإ\u001b[0mيام | \u001b[1m\u001b[31mا\u001b[0mيام | 1\n",
            "(450) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(451) ‫ ג'מיע | جميع | جميع | 0\n",
            "(452) ‫ אלנאטקינ | الناطقين | الناطقين | 0\n",
            "(453) ‫ מנ | من | من | 0\n",
            "(454) ‫ ד'ריה | ذري\u001b[1m\u001b[31mة\u001b[0m | ذري\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(455) ‫ אדמ | \u001b[1m\u001b[31mآ\u001b[0mدم | \u001b[1m\u001b[31mا\u001b[0mدم | 1\n",
            "(456) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1\n",
            "(457) ‫ ד'ריה | ذري\u001b[1m\u001b[31mة\u001b[0m | ذري\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(458) ‫ נוח | نوح | نوح | 0\n",
            "(459) ‫ ואליה | و\u001b[1m\u001b[31mإ\u001b[0mليه | و\u001b[1m\u001b[31mا\u001b[0mليه | 1\n",
            "(460) ‫ ינתסבונ | ينتسبون | ينتسبون | 0\n",
            "(461) ‫ כלהמ | كل\u001b[1m\u001b[31mّ\u001b[0mهم | كلهم | 1\n",
            "(462) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mا\u001b[0mن | 2\n",
            "(463) ‫ ללה | لله | لله | 0\n",
            "(464) ‫ ענאיה | عناي\u001b[1m\u001b[31mة\u001b[0m | عناي\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(465) ‫ באלכ'לק | بالخلق | بالخلق | 0\n",
            "(466) ‫ ואתצאלא | وات\u001b[1m\u001b[31mّ\u001b[0mصالا\u001b[1m\u001b[31mً\u001b[0m | واتصالا | 2\n",
            "(467) ‫ באלנאטקינ | بالناطقين | بالناطقين | 0\n",
            "(468) ‫ וסכ'טא | وسخطا\u001b[1m\u001b[31mً\u001b[0m | وسخطا | 1\n",
            "(469) ‫ ורצ'א | ورضا\u001b[1m\u001b[31mً\u001b[0m | ورضا | 1\n",
            "(470) ‫ ורחמה | ورحم\u001b[1m\u001b[31mة\u001b[0m | ورحم\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(471) ‫ וכלאמא | وكلاما\u001b[1m\u001b[31mً\u001b[0m | وكلاما | 1\n",
            "(472) ‫ וט'הורא | وظهورا\u001b[1m\u001b[31mً\u001b[0m | وظهورا | 1\n",
            "(473) ‫ ותג'ליא | وتجل\u001b[1m\u001b[31mّ\u001b[0mيا\u001b[1m\u001b[31mً\u001b[0m | وتجليا | 2\n",
            "(474) ‫ לאנביאה | ل\u001b[1m\u001b[31mإ\u001b[0mنبيا\u001b[1m\u001b[31mئ\u001b[0mه | ل\u001b[1m\u001b[31mا\u001b[0mنبياه | 2\n",
            "(475) ‫ ואוליאה | و\u001b[1m\u001b[31mإ\u001b[0mوليا\u001b[1m\u001b[31mئ\u001b[0mه | و\u001b[1m\u001b[31mا\u001b[0mولياه | 2\n",
            "(476) ‫ וחלולא | وحلولا\u001b[1m\u001b[31mً\u001b[0m | وحلولا | 1\n",
            "(477) ‫ פי | في | في | 0\n",
            "(478) ‫ מא | ما | ما | 0\n",
            "(479) ‫ בינ | بين | بين | 0\n",
            "(480) ‫ מנ | من | من | 0\n",
            "(481) ‫ ירצ'אה | يرضاه | يرضاه | 0\n",
            "(482) ‫ מנ | من | من | 0\n",
            "(483) ‫ אלג'מאהיר | الجماهير | الجماهير | 0\n",
            "(484) ‫ ואלג'מלה | والجمل\u001b[1m\u001b[31mة\u001b[0m | والجمل\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(485) ‫ פכל | فكل\u001b[1m\u001b[31mّ\u001b[0m | فكل | 1\n",
            "(486) ‫ מא | ما | ما | 0\n",
            "(487) ‫ ג'א | جا\u001b[1m\u001b[31mء\u001b[0m | جا | 1\n",
            "(488) ‫ פי | في | في | 0\n",
            "(489) ‫ אלתוראה | التورا\u001b[1m\u001b[31mة\u001b[0m | التورا\u001b[1m\u001b[31mه\u001b[0m | 1\n",
            "(490) ‫ ופי | وفي | وفي | 0\n",
            "(491) ‫ את'אר | \u001b[1m\u001b[31mآ\u001b[0mثار | \u001b[1m\u001b[31mا\u001b[0mثار | 1\n",
            "(492) ‫ בני | بني | بني | 0\n",
            "(493) ‫ אסראיל | \u001b[1m\u001b[31mإ\u001b[0mسرا\u001b[1m\u001b[31mئ\u001b[0mيل | \u001b[1m\u001b[31mا\u001b[0mسرايل | 2\n",
            "(494) ‫ אלתי | التي | التي | 0\n",
            "(495) ‫ לא | لا | لا | 0\n",
            "(496) ‫ מדפע | مدفع | مدفع | 0\n",
            "(497) ‫ פי | في | في | 0\n",
            "(498) ‫ צדקהא | صدقها | صدقها | 0\n",
            "(499) ‫ לשהרתהא | لشهرتها | لشهرتها | 0\n",
            "(500) ‫ ודואמהא | ودوامها | ودوامها | 0\n",
            "accuracy:  0.8769578643578643\n",
            "accuracy1:  0.8774193548387097\n",
            "(1) ‫ דאר | دار | دار | 0\n",
            "(2) ‫ אלגזא | ال\u001b[1m\u001b[31mج\u001b[0mزا\u001b[1m\u001b[31mء\u001b[0m | ال\u001b[1m\u001b[31mغ\u001b[0mزا | 2\n",
            "(3) ‫ וקבל | وقبل | وقبل | 0\n",
            "(4) ‫ ד'לכ | ذلك | ذلك | 0\n",
            "(5) ‫ מא | ما | ما | 0\n",
            "(6) ‫ ראי | ر\u001b[1m\u001b[31mأى\u001b[0m | ر\u001b[1m\u001b[31mاي\u001b[0m | 2\n",
            "(7) ‫ אנ | \u001b[1m\u001b[31mأ\u001b[0mن | \u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(8) ‫ יפרק | يفر\u001b[1m\u001b[31mّ\u001b[0mق | يفرق | 1\n",
            "(9) ‫ בינ | بين | بين | 0\n",
            "(10) ‫ רוחה | روحه | روحه | 0\n",
            "(11) ‫ וגסמה | و\u001b[1m\u001b[31mج\u001b[0mسمه | و\u001b[1m\u001b[31mغ\u001b[0mسمه | 1\n",
            "(12) ‫ אלי | \u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m | 2\n",
            "(13) ‫ וקת | وقت | وقت | 0\n",
            "(14) ‫ אסתכמאל | استكمال | استكمال | 0\n",
            "(15) ‫ אלנפוס | النفوس | النفوس | 0\n",
            "(16) ‫ חתי | حت\u001b[1m\u001b[31mى\u001b[0m | حت\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(17) ‫ יגמעהא | ي\u001b[1m\u001b[31mج\u001b[0mمعها | ي\u001b[1m\u001b[31mغ\u001b[0mمعها | 1\n",
            "(18) ‫ אלגמיע | ال\u001b[1m\u001b[31mج\u001b[0mميع | ال\u001b[1m\u001b[31mغ\u001b[0mميع | 1\n",
            "(19) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(20) ‫ מא | ما | ما | 0\n",
            "(21) ‫ בינת | بي\u001b[1m\u001b[31mّ\u001b[0mنت | بينت | 1\n",
            "(22) ‫ פלא | فلا | فلا | 0\n",
            "(23) ‫ נעלמ | نعلم | نعلم | 0\n",
            "(24) ‫ יהודיא | يهودي\u001b[1m\u001b[31mّ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | يهوديا | 2\n",
            "(25) ‫ יכ'אלפ | يخالف | يخالف | 0\n",
            "(26) ‫ עלי | عل\u001b[1m\u001b[31mى\u001b[0m | عل\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(27) ‫ הד'ה | هذه | هذه | 0\n",
            "(28) ‫ אלאמאנה | ال\u001b[1m\u001b[31mأ\u001b[0mمان\u001b[1m\u001b[31mة\u001b[0m | ال\u001b[1m\u001b[31mا\u001b[0mمان\u001b[1m\u001b[31mه\u001b[0m | 2\n",
            "(29) ‫ ולא | ولا | ولا | 0\n",
            "(30) ‫ יסתצעב | يستصعب | يستصعب | 0\n",
            "(31) ‫ ענד | عند | عند | 0\n",
            "(32) ‫ עקלה | عقله | عقله | 0\n",
            "(33) ‫ כיפ | كيف | كيف | 0\n",
            "(34) ‫ יחיי | يحيي | يحيي | 0\n",
            "(35) ‫ רבה | رب\u001b[1m\u001b[31mّ\u001b[0mه | ربه | 1\n",
            "(36) ‫ אלמותי | الموت\u001b[1m\u001b[31mى\u001b[0m | الموت\u001b[1m\u001b[31mي\u001b[0m | 1\n",
            "(37) ‫ אד' | \u001b[1m\u001b[31mإ\u001b[0mذ | \u001b[1m\u001b[31mا\u001b[0mذ | 1\n",
            "(38) ‫ קד | قد | قد | 0\n",
            "(39) ‫ צח | صح\u001b[1m\u001b[31mّ\u001b[0m | صح | 1\n",
            "(40) ‫ לה | له | له | 0\n",
            "(41) ‫ אנה | \u001b[1m\u001b[31mأ\u001b[0mنه | \u001b[1m\u001b[31mا\u001b[0mنه | 1\n",
            "(42) ‫ כ'לק | خلق | خلق | 0\n",
            "(43) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا | 2\n",
            "(44) ‫ לא | لا | لا | 0\n",
            "(45) ‫ מנ | من | من | 0\n",
            "(46) ‫ שי | شي\u001b[1m\u001b[31mء\u001b[0m | شي | 1\n",
            "(47) ‫ פלא | فلا | فلا | 0\n",
            "(48) ‫ יגוז | ي\u001b[1m\u001b[31mج\u001b[0mوز | ي\u001b[1m\u001b[31mغ\u001b[0mوز | 1\n",
            "(49) ‫ אנ | \u001b[1m\u001b[31mأ\u001b[0mن | \u001b[1m\u001b[31mا\u001b[0mن | 1\n",
            "(50) ‫ יסתעסר | يستعسر | يستعسر | 0\n",
            "accuracy:  0.845952380952381\n",
            "accuracy1:  0.8489583333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0.15404761904761904, 0.15104166666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74",
        "colab_type": "text"
      },
      "source": [
        "##test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF general TEST_LOSS\"\n",
        "\n",
        "\n",
        "def test(this_dataset=test_dataset_double_kuzari,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_example_batch[i])\n",
        "\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=decode_arr(dense[i]).strip() #SHOULD BE STRING(BLANKS)?\n",
        "                real=decode_arr(target_example_batch[i],targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"LER (label error rate): \",total_accuracy)\n",
        "  #print_log(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "\n",
        "#test(limit=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpW54oxAkxCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "5cc3d5e3-1c90-46ce-a797-dcf43290a4a2"
      },
      "source": [
        "test(test_dataset_double_kuzari,limit=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/ctc_ops.py:1399: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/ctc_ops.py:1382: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n",
            "(1) ‫ ואהל אלאדיאנ ת'מ עלי | وأهل الأديان ثمّ على | وأهل الأديان ثمّ على | 0.0000\n",
            "(2) ‫ הכד'א כאנ קומה מעה , | هكذا كان قومه معه , | هكذا كان قومه معه , | 0.0000\n",
            "(3) ‫ תקתצ'י אלמעאני אלתי יריד | تقتضي المعاني التي يريد | تقتضي المعاني التي يريد | 0.0000\n",
            "LER (label error rate):  0.023761761635291978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.5300145, 0.023761761635291978)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f",
        "colab_type": "text"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DGjAhEUfxbRc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0b2a47b0-d6e9-46a7-c51e-d3af058aa506"
      },
      "source": [
        "CELL_NAME=\"GUIDE TEXT\"\n",
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "###TODO : change hebrew insertion to \"H\"\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guide_text='''כנת איהא אלתלמיד' אלעזיז עברית-ר' עברית-יוסף עברית-ש\"צ עברית-ב\"ר \n",
        "עברית-יהודה עברית-נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "guide_text=re.sub(r'עברית-[^\\s]+', 'H',guide_text)\n",
        "guide_lines=guide_text.split('\\n')\n",
        "\n",
        "for l in guide_lines:\n",
        "  print(LTRchar+l)\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר עברית-פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‫כנת איהא אלתלמיד' אלעזיז H H H H \n",
            "‫H H למא מת'לת ענדי וקצדת\n",
            "‫ מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
            "‫ אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
            "‫אלאסכנדריה קבל אן אמתחן\n",
            "‫תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
            "‫קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
            "‫זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
            "‫עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
            "‫פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
            "‫וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
            "‫אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
            "‫פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "28eab711-fc94-483d-bfcc-e6d04bd0fd89"
      },
      "source": [
        "CELL_NAME=\"DEF TEST_GUIDE\"\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return forward_text(guide_lines,num_of_paths,BATCH_SIZE)\n",
        "print_log_screen(test_guide())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "كنت أيها التلميذ العزيز H H H H                                                                  \n",
            "H H لما مثلت عندي وقصدت                                                                          \n",
            "من أقاصى البلاد للقراءة على , عظم شأنك عندي لشدة حرصك على                                        \n",
            "الطلب ولمّا رأيته في أشعارك من شدة الاشتياق للأمور النظرية وكان ذلك منذ وصلتني رسائلك ومقاماتك من\n",
            "الاسكندرية قبل أن امتحن                                                                          \n",
            "تصورك وقلت لعل شوقه أقوى من إدراكه فلما قرأت على ما قد                                           \n",
            "قراءته من علم الهيئة وما تقدّم لك مما لا بد منه توطئة لها من التعاليم                            \n",
            "زدت بك غبطه لجودة ذهنك وسرعة تصورك ورأيت شوقك للتعاليم                                           \n",
            "عظيماً فتركتك للارتياض فيها لعلمي بمالكا                                                         \n",
            "فلما قرأت على ما قد قرأته من صناعة المنطق تعلقت أمالي بك                                         \n",
            "ورأيتك أهلا لتكشف لك أسرار الكتب النبوية حتّى تطلع منها على ما ينبغي                             \n",
            "أن يطلع عليه الكاملون فأخذت أن الوح لك تلويحات وأشير لك بإشارات                                  \n",
            "فرأئتك تطلب مني الازدياد وسمتني أن أبين لك أشياء من الأمور                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ",
        "colab_type": "text"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF pretrain_letters\"\n",
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    if STATEFUL:\n",
        "      hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)       \n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test_single_letters()\n",
        "    print_log_screen(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQQCzYQPWy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BASIC_MODEL=\"/gdrive/My Drive/checkpoints/BASIC_MODEL_PRETRAIN\"\n",
        "\n",
        "\n",
        "# optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "# #model=load_checkpoint(BASIC_MODEL)\n",
        "# pretrain_letters(10)\n",
        "\n",
        "# save_checkpoint(\"\",ckp_path=BASIC_MODEL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TRAIN SINGLE LETTERS AND TEST LETTERS\"\n",
        "# model=rebuild()\n",
        "# test_single_letters()\n",
        "# optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "# pretrain_letters(10,BATCH_SIZE)\n",
        "# test_single_letters()\n",
        "# save_checkpoint(\"testing1\")\n",
        "# model=rebuild()\n",
        "# test_single_letters()\n",
        "# model=load_checkpoint()\n",
        "# test_single_letters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du",
        "colab_type": "text"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF main TRAIN_LOOP\"\n",
        "\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train(cur_dataset=train_dataset_double_kuzari,stop_loop=10000000000):\n",
        "  global GLOBAL_epoch\n",
        "  if STATEFUL:\n",
        "    hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:            \n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?\n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print_log_screen(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA",
        "colab_type": "text"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhSQaqd3ye9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5d08aaec-9f7a-4500-924e-72761b39c653"
      },
      "source": [
        "dataset_double_synt=gen_all_synth(1).concatenate(train_dataset_double_kuzari).shuffle(500)\n",
        "count=0\n",
        "for input,target,input_lens,target_lens in dataset_double_synt:\n",
        "  #print(decode_JA(input[0],input_lens[0]))\n",
        "  count+=1\n",
        "  #print(count)\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‫ מונסי ואנ בעת' אלאשואק  |  مؤنسي  وإن بعث الأشواق\n",
            "‫ אלאכ'לאל בג'מיעהא ולא יג'ב  |  الإخلال بجميعها ولا يجب\n",
            "‫ וכאנ פתחה להא יומ אלת'לאת'א  |  وكان فتحه لها يوم الثلاثاء\n",
            "‫ אלארצ' עדלא בעד אנ מלית  |  الأرض عدلا بعد أن ملئت\n",
            "‫   |      \n",
            "2450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPaH-g2wziSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "71f86c0d-3309-45c1-cfa0-58aa79913e74"
      },
      "source": [
        "test_kfir(kfir_kuzari_test_SWITCH,True,None,False)  \n",
        "test_kfir(kfir_rasag_test_SWITCH,True,None,False)\n",
        "#TESTING ALL EVERY OUTER LOOP \n",
        "all_test_loss,all_accuracy=test()\n",
        "#shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "all_test_loss1,all_accuracy1=test(test_dataset_double_rasag)  #HAEMUNOT VEHADEOT\n",
        "#shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)\n",
        "\n",
        "\n",
        "guide_result=test_guide()\n",
        "\n",
        "#TODO SAVE CHECKPOINT\n",
        "if all_accuracy<BEST_ACCURACY:\n",
        "  save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "  BEST_ACCURACY=all_accuracy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 500\n",
            "#examples: 500 , accuracy: 0.9063816738816739\n",
            "#letters: 2325 , accuracy1: 0.9058064516129032\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 50\n",
            "#examples: 50 , accuracy: 0.9473333333333334\n",
            "#letters: 192 , accuracy1: 0.9375\n",
            "(1) ‫ ואהל אלאדיאנ ת'מ עלי | وأهل الأديان ثم\u001b[1m\u001b[31mّ\u001b[0m على | وأهل الأديان ثم على | 0.0500\n",
            "(2) ‫ הכד'א כאנ קומה מעה , | هكذا كان قومه معه , | هكذا كان قومه معه , | 0.0000\n",
            "(3) ‫ תקתצ'י אלמעאני אלתי יריד | تقتضي المعاني التي يريد | تقتضي المعاني التي يريد | 0.0000\n",
            "(4) ‫ אכ'תיארא , לכנה אצ'טרא[רא] | اختياراً , لكن\u001b[1m\u001b[31mّ\u001b[0mه اضطرار | اختياراً , لكنه اضطرا\u001b[1m\u001b[31m[\u001b[0mر\u001b[1m\u001b[31mا]\u001b[0m | 0.1739\n",
            "(5) ‫ משארק ומגארב מעא , פתציר | مشارق ومغارب معا\u001b[1m\u001b[31mً\u001b[0m , فتصير | مشارق ومغارب معا , فتصير | 0.0400\n",
            "(6) ‫ מנה כ'לט , יחדת' פיה | منه خلط , يحدث فيه | منه خلط , يحدث فيه | 0.0000\n",
            "(7) ‫ וכאנ אהל אלארצ' יקצדונה | وكان أهل الأرض يقصدونه | وكان أهل الأرض يقصدونه | 0.0000\n",
            "(8) ‫ אללסאנ מטאבקא ללפכר אלא | اللسان مطابقا\u001b[1m\u001b[31mً\u001b[0m للفكر \u001b[1m\u001b[31mأ\u001b[0mلا\u001b[1m\u001b[31mّ\u001b[0m | اللسان مطابقا للفكر \u001b[1m\u001b[31mإ\u001b[0mلا | 0.1200\n",
            "(9) ‫ אלתי לא תלזמ , וקד כאנ | التي لا تلزم , وقد كان | التي لا تلزم , وقد كان | 0.0000\n",
            "(10) ‫ אלתורה משכל , פצ'לא ענ | التور\u001b[1m\u001b[31mاة\u001b[0m مشكل , فضلا\u001b[1m\u001b[31mً\u001b[0m عن | التور\u001b[1m\u001b[31mه\u001b[0m مشكل , فضلا عن | 0.1304\n",
            "(11) ‫ H H H H תחרכ אלנאס , | \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H\u001b[0m تحر\u001b[1m\u001b[31mّ\u001b[0mك الناس , | \u001b[1m\u001b[31mٌ\u001b[0m \u001b[1m\u001b[31m:\u001b[0m \u001b[1m\u001b[31mٌ\u001b[0m تحرك الناس , | 0.2857\n",
            "(12) ‫ ישאר אלי H H H H ויסג'ד | يشار إلى \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H H\u001b[0m ويسجد | يشار إلى  \u001b[1m\u001b[31mص\u001b[0m ويسجد | 0.2727\n",
            "(13) ‫ ותציר פיה H H , כמא קיל | وتصير فيه \u001b[1m\u001b[31mH H \u001b[0m, كما قيل | وتصير فيه , كما قيل | 0.1739\n",
            "(14) ‫ אלטביעיה באלמא אחק תשביה | الطبيعي\u001b[1m\u001b[31mة\u001b[0m بالماء أحق\u001b[1m\u001b[31mّ\u001b[0m تشبيه | الطبيعي\u001b[1m\u001b[31mه\u001b[0m بالماء أحق تشبيه | 0.0769\n",
            "(15) ‫ סודא והד'ה ביצ'א , והד'א | سوداء وهذه بيضاء , وهذ\u001b[1m\u001b[31mه\u001b[0m | سوداء وهذه بيضاء , وهذ\u001b[1m\u001b[31mا\u001b[0m | 0.0435\n",
            "(16) ‫ בפסאד אלאלה , לכנ אלנפס | بفساد الآل\u001b[1m\u001b[31mة\u001b[0m , لكن\u001b[1m\u001b[31mّ\u001b[0m النفس | بفساد الآل\u001b[1m\u001b[31mه\u001b[0m , لكن النفس | 0.0833\n",
            "(17) ‫ אלאמור ענ עלמה , לאנ | الأمور عن علمه , لأن\u001b[1m\u001b[31mّ\u001b[0m | الأمور عن علمه , لأن | 0.0476\n",
            "LER (label error rate):  0.08769247580507904\n",
            "(1) ‫ באנ קאל תבארכ אללה אלאה | بأن قال تبارك الله إله | بأن قال تبارك الله إل\u001b[1m\u001b[31mا\u001b[0mه | 0.0455\n",
            "(2) ‫ פהו מא אדרכה אלאנסאנ | فهو ما أدركه الإنسان | فهو ما أدركه الإنسان | 0.0000\n",
            "(3) ‫ כל אלנאס משתרכונ פי אלעלומ | كل\u001b[1m\u001b[31mّ\u001b[0m الن\u001b[1m\u001b[31mّ\u001b[0mاس مشتركون في العلوم | كل الناس مشتركون في العلوم | 0.0714\n",
            "(4) ‫ מא שאכלה . פלמא צח לנא | ما شاكله . فلما صح\u001b[1m\u001b[31mّ\u001b[0m لنا | ما شاكله . فلما صح لنا | 0.0435\n",
            "(5) ‫ . ואנ כאנוא מנ ג'הה' | . وإن كانوا من جهة | . وإن كانوا من جهة | 0.0000\n",
            "(6) ‫ לה אעתקאדה וינתקל מע | له اعتقاده وينتقل مع | له اعتقاده وينتقل مع | 0.0000\n",
            "(7) ‫ רסמת פי כתבה בתג'סימ | رسمت في كتبه بتجسيم | رسمت في كتبه بتجسيم | 0.0000\n",
            "(8) ‫ H H H H H H H H H H H | \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H H H H H H H H\u001b[0m | \u001b[1m\u001b[31m4\u001b[0m \u001b[1m\u001b[31m.\u001b[0m \u001b[1m\u001b[31mق\u001b[0m | 0.9048\n",
            "(9) ‫ אזדאדת כ'ופא ופזעא כמא | ازدادت خوفا\u001b[1m\u001b[31mً\u001b[0m وفزعا\u001b[1m\u001b[31mً\u001b[0m كما | ازدادت خوفا وفزعا كما | 0.0870\n",
            "(10) ‫ , ואלת'לאת' איצ'א למ | , والثلاث أيضاً لم | , والثلاث أيضاً لم | 0.0000\n",
            "(11) ‫ H H H H H H . ואלת'אלת' | \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H H H H\u001b[0m . والثالث\u001b[1m\u001b[31mة\u001b[0m | \u001b[1m\u001b[31mد\u001b[0m \u001b[1m\u001b[31m.\u001b[0m . والثالث | 0.5000\n",
            "(12) ‫ , ולא מנ ליס הו מכ'תארא | , ولا من ليس هو مختارا\u001b[1m\u001b[31mً\u001b[0m | , ولا من ليس هو مختارا | 0.0435\n",
            "(13) ‫ : H H H H H H H . פקלת | : \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H H H H\u001b[0m . فقلت | :  \u001b[1m\u001b[31mص\u001b[0m \u001b[1m\u001b[31m.\u001b[0m . فقلت | 0.5000\n",
            "(14) ‫ H H H פמנ למ יתמ לה ד'לכ | \u001b[1m\u001b[31mH\u001b[0m \u001b[1m\u001b[31mH H\u001b[0m فمن لم يتم له ذلك | \u001b[1m\u001b[31mز\u001b[0m \u001b[1m\u001b[31m:\u001b[0m فمن لم يتم له ذلك | 0.1739\n",
            "(15) ‫ , פנקול לה , ג'סמ אלאנסאנ | , فنقول له , جسم الإنسان | , فنقول له , جسم الإنسان | 0.0000\n",
            "(16) ‫ אנ יכונ מנ נצ פציח ימאנעה | أن يكون من نص\u001b[1m\u001b[31mّ\u001b[0m فصيح يمانعه | أن يكون من نص فصيح يمانعه | 0.0385\n",
            "LER (label error rate):  0.09364553777266582\n",
            "saving checkpoing at /gdrive/My Drive/checkpoints/2020-05-13 14:27:12.030576/ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "935590d6-7136-4b4f-da6c-1257ffc10547"
      },
      "source": [
        "CELL_NAME=\"MAIN\"\n",
        "\n",
        "##############################################\n",
        "####RUN PARAMETERS:\n",
        "mail_subject=\"NOT STATEFULL: pretrain letters. synt DROPOUT 0.9 no KUZARI\"\n",
        "mail_subject=this_time+\":\"+mail_subject\n",
        "\n",
        "pretrain_letter=15\n",
        "synth=True\n",
        "keep_percent=0.70\n",
        "USE_CHECKPOINT=True\n",
        "\n",
        "\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print_log_screen(description)\n",
        "#################################\n",
        "\n",
        "#INIT\n",
        "BEST_ACCURACY=1\n",
        "f= open(\"my_log.txt\",\"w+\") #attach to mail summary of tests\n",
        "init_random()\n",
        "\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "if not USE_CHECKPOINT:\n",
        "  model=rebuild()\n",
        "\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "\n",
        "###################################\n",
        "####PRETRAIN\n",
        "if not USE_CHECKPOINT and pretrain_letter>0:\n",
        "  print_log_screen(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print_log_screen('-'*200)\n",
        "##################################\n",
        "print_log_screen(\"START TRAIN\")\n",
        "\n",
        "for jjj in range(6): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(5): #iter without sendmail and only partial test    \n",
        "    if keep_percent<1:\n",
        "        train_dataset_double_kuzari= produce_dataset(\n",
        "            create_parralel_phrases(kuzari_lines_train,keep_percent),\n",
        "            to_shuffle=TO_SHUFFLE)        \n",
        "    if synth:\n",
        "      dataset_double_synt=gen_all_synth(keep_percent).concatenate(train_dataset_double_kuzari).shuffle(500)\n",
        "      loss=train(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train(train_dataset_double_kuzari)\n",
        "    \n",
        "    #total_test_loss,total_accuracy=test(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    total_test_loss,total_accuracy=test(limit=5)\n",
        "    test(this_dataset=test_dataset_double_rasag,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print_log_screen('-'*200)\n",
        "    test_kfir(kfir_kuzari_test_SWITCH,False)\n",
        "    test_kfir(kfir_rasag_test_SWITCH,False)\n",
        "    print_log_screen('-'*200)\n",
        "    \n",
        "  print_log('FULL STATISTICS')\n",
        "  print_log('='*200)\n",
        "  test_kfir(kfir_kuzari_test_SWITCH,True,None,False)  \n",
        "  test_kfir(kfir_rasag_test_SWITCH,True,None,False)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test(test_dataset_double_rasag)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  if all_accuracy<BEST_ACCURACY:\n",
        "    save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "    BEST_ACCURACY=all_accuracy\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print_log(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print_log(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print_log(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  \n",
        "  f.write(\"EPOCH \"+str(GLOBAL_epoch)+'\\n')\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  \n",
        "  log_flush()\n",
        "  f.flush()\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "\n",
        "\n",
        "  \n",
        "f.close()\n",
        "close_log()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "pretrain: 15\n",
            "with synth data\n",
            "dropout:0.7\n",
            "\n",
            "init random to 1\n",
            "START TRAIN\n",
            "‫ אצ'ל_י קלב_ עליה  |  أضلعي  قلباً عليه\n",
            "‫ א_א__ש ענ א_י אסחאק ע_  |  الأعمش عن أبي إسحاق عن\n",
            "‫ א_צו_ה' , אד_ אל__ר__ לא  |  الصورة , إذ الصورة لا\n",
            "‫ י_ול _סאנ _נ ת'__ת :  |  يقول حسان بن ثابت : \n",
            "‫ __ קולה אנ כ_ _רט _אנ  |  من قوله إن كل شرط كان\n",
            "Epoch 11 Batch 0 Loss 19.3022\n",
            "Epoch 11 Batch 10 Loss 36.2109\n",
            "Epoch 11 Batch 20 Loss 20.2204\n",
            "Epoch 11 Batch 30 Loss 31.1346\n",
            "Epoch 11 Batch 40 Loss 31.6732\n",
            "Epoch 11 Batch 50 Loss 25.3414\n",
            "Epoch 11 Batch 60 Loss 47.5873\n",
            "Epoch 11 Batch 70 Loss 31.5253\n",
            "Epoch 11 Batch 80 Loss 42.4523\n",
            "Epoch 11 Batch 90 Loss 19.5084\n",
            "Epoch 11 Batch 100 Loss 21.0379\n",
            "Epoch 11 Batch 110 Loss 25.3551\n",
            "Epoch 11 Batch 120 Loss 31.6969\n",
            "Epoch 11 Batch 130 Loss 29.9605\n",
            "Epoch 11 Batch 140 Loss 15.3811\n",
            "Epoch 11 Batch 150 Loss 31.1189\n",
            "Epoch 11 Batch 160 Loss 25.1968\n",
            "Epoch 11 Batch 170 Loss 20.1112\n",
            "Epoch 11 Batch 180 Loss 24.5876\n",
            "Epoch 11 Batch 190 Loss 31.2301\n",
            "Epoch 11 Batch 200 Loss 25.1095\n",
            "Epoch 11 Batch 210 Loss 14.7806\n",
            "Epoch 11 Batch 220 Loss 25.6551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-033a12618786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mdataset_double_synt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_all_synth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_double_kuzari\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_double_synt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_double_kuzari\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-1201caf1d6b9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cur_dataset, stop_loop)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_n\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             skip_on_eager=False), ops.colocate_with(var):\n\u001b[0;32m--> 616\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \"Cannot use a constraint function on a sparse variable.\")\n\u001b[1;32m    169\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m--> 170\u001b[0;31m           g.values, self._v, g.indices)\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_resource_apply_sparse_duplicate_indices\u001b[0;34m(self, grad, handle, indices)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \"\"\"\n\u001b[1;32m    980\u001b[0m     summed_grad, unique_indices = _deduplicate_indexed_slices(\n\u001b[0;32m--> 981\u001b[0;31m         values=grad, indices=indices)\n\u001b[0m\u001b[1;32m    982\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummed_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_deduplicate_indexed_slices\u001b[0;34m(values, indices)\u001b[0m\n\u001b[1;32m     77\u001b[0m   summed_values = math_ops.unsorted_segment_sum(\n\u001b[1;32m     78\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       array_ops.shape(unique_indices)[0])\n\u001b[0m\u001b[1;32m     80\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msummed_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36munsorted_segment_sum\u001b[0;34m(data, segment_ids, num_segments, name)\u001b[0m\n\u001b[1;32m  10950\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m  10951\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"UnsortedSegmentSum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10952\u001b[0;31m         tld.op_callbacks, data, segment_ids, num_segments)\n\u001b[0m\u001b[1;32m  10953\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10954\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwpwDinoeIu5",
        "colab_type": "text"
      },
      "source": [
        "#MAIN OUTPUT (ABOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test()\n",
        "#test(test_dataset_double_rasag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   train()\n",
        "#   test(single_words_test_dataset,limit=5)\n",
        "#   test(limit=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # test(only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(this_dataset=test_dataset_double_kuzari,only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(this_dataset=test_dataset_double_rasag,only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe",
        "colab_type": "text"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zcZ9IdHq23e",
        "colab_type": "text"
      },
      "source": [
        "#OLD STAFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"SMALL TRY\"\n",
        "\n",
        "# this_string='وأهل الأديان ثمّ على'\n",
        "# this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "# this_string='ثمّ'\n",
        "\n",
        "# #this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string=normalize_unicode(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"TRY SYNTH\"\n",
        "\n",
        "# #ACTIVATE\n",
        "# ibnsina_text=load_lines_synth()\n",
        "\n",
        "# #STATSTICS OF SYNTH TEXT\n",
        "# sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "# print_log(\"NOT IN LETTER LIST:\")\n",
        "# for c in sina_vocab:\n",
        "#    if c not in targ_lang.char2idx:\n",
        "#       print_log(\"(\",c,\")\")\n",
        "\n",
        "# print_log(\"\\nLETTER COUNTS\")\n",
        "# for i in range(len(sina_vocab)):\n",
        "#   print_log(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   \n",
        "\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "# #sina_words=ibnsina_text.split(\" \")\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "# sina_words=ibnsina_text.split() #for removing also newlines\n",
        "# print_log(ibnsina_text[:100])\n",
        "# sina_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA",
        "colab_type": "text"
      },
      "source": [
        "##Shuffled test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"SHUFFLED TEST\"\n",
        "\n",
        "# def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "#   val_inputs_list=[]\n",
        "#   val_outputs_list=[]\n",
        "\n",
        "#   for i,j,l1,l2 in test_dataset:\n",
        "#     for tt in range(BATCH_SIZE):\n",
        "#      # print_log(i[tt],j[tt])\n",
        "#       i_prediction=decode_JA(tf.constant(i[tt]),l1[tt])\n",
        "#       j_prediction=decode_arr(tf.constant(j[tt]),l2[tt])      \n",
        "#       if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "#         val_inputs_list+=i_prediction.split()\n",
        "#         val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "#   print_log(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "#   word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "#   print_log(\"BEFORE SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   random.shuffle(word_pairs)\n",
        "#   print_log(\"AFTER SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   return word_pairs\n",
        "\n",
        "# #TESTING\n",
        "# #word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag.take(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF GET SHUFFLE DATA\"\n",
        "# def get_shuffled_data(word_pairs):\n",
        "\n",
        "#   accum=0\n",
        "#   heb_acum=\"\"\n",
        "#   arab_acum=\"\"\n",
        "#   results_line=[]\n",
        "#   for i,j in word_pairs:\n",
        "#     if accum>19:\n",
        "#       results_line.append(undouble_hebrew(heb_acum)+'\\t'+arab_acum)\n",
        "#       assert(len(i)%2==0)\n",
        "#       accum=len(i)/2\n",
        "#       heb_acum=i\n",
        "#       arab_acum=j\n",
        "#     else:\n",
        "#       heb_acum+=\" \"+i\n",
        "#       arab_acum+=\" \"+j \n",
        "#       assert(len(i)%2==0)\n",
        "#       accum += len(i)/2 + 1;\n",
        "#   results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "#   print_log(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "#   input_tensor_shuffle, target_tensor_shuffle \\\n",
        "#   ,input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "#   print_log(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "#   print_log(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "#   BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "#   shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "#                                                             target_tensor_shuffle,\n",
        "#                                                             input_lenghts_shuffle,\n",
        "#                                                             target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#   shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#   return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GEN SHUFFLED DATA\"\n",
        "\n",
        "# word_pairs=get_shuffled_word_pairs(test_dataset_double_kuzari)\n",
        "# shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "# view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "# word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag)\n",
        "# shuffle_test_dataset_double_rasag=get_shuffled_data(word_pairs1)\n",
        "# view_data(shuffle_test_dataset_double_rasag)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9",
        "colab_type": "text"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF TEST_SHUFFLE\"\n",
        "# def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "#   return test(this_dataset=data,limit=limit)\n",
        "# #shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Single words test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"GEN SINGEL WORDS\"\n",
        "\n",
        "# # results_line=[]\n",
        "# # for i,j in word_pairs:\n",
        "# #     results_line.append(undouble_hebrew(i)+'\\t'+j)\n",
        "\n",
        "# # print_log(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "# # input_tensor_shuffle, target_tensor_shuffle \\\n",
        "# # , input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "# # print_log(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "# # BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "# # single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "# #                                                                 target_tensor_shuffle,\n",
        "# #                                                                 input_lenghts_shuffle,\n",
        "# #                                                                 target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# # single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# # view_data(single_words_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}