{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ori1234/JA-RNN/blob/master/Thesis_8_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY",
        "colab_type": "text"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i) :\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
        "\n",
        "THIS console.log(\"Working\");document.querySelector(\"colab-connect-button\")\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window. It would keep on clicking the page and prevent it from disconnecting.\n",
        "\n",
        "\n",
        "\n",
        "Note: Although I did the same thing, I forgot abt it for 12 hours and got my GPU privileges suspended temporarily. Make sure you dont run anything for more than 12 hrs on Colab!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE",
        "colab_type": "text"
      },
      "source": [
        "**SUMMERY**\n",
        "\n",
        "\n",
        "\n",
        "this is based on https://colab.research.google.com/drive/1m6VuABiVrkKmhUGitHFDl99r6-qe_ZoJ#scrollTo=OHn4Dct23jEm at tirza's account titled:\n",
        "FINAL 4CURRENT__CLEAN_Copy_of_CTC_UnEquel_Input_Output_heb_to_ar.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldvhaPtVF0Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######!rm log*.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxiE-PU7A-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bcecf20-c5d2-4863-b48e-c3a9b2086e50"
      },
      "source": [
        "from datetime import datetime\n",
        "this_time=str(datetime.now())\n",
        "log_file=\"log___\"+this_time+\".txt\"\n",
        "f_logg= open(log_file,\"w\")\n",
        "\n",
        "CELL_NAME=\"START LOG\"\n",
        "\n",
        "def _print_log(also_print,*txts):  \n",
        "  txt=\"\"\n",
        "  for t in txts:\n",
        "    txt+=\" \"+str(t)\n",
        "  f_logg.write(CELL_NAME+\" \"+str(datetime.now())+\": \"+txt+'\\n') #TODO ADD TIME!!!!\n",
        "  if also_print:\n",
        "    print(*txts)\n",
        "\n",
        "def print_log(*txts):\n",
        "  #print(*txts)\n",
        "  _print_log(False,*txts)\n",
        "\n",
        "\n",
        "def print_log_screen(*txts):\n",
        "  _print_log(True,*txts)\n",
        "\n",
        "\n",
        "def log_flush():\n",
        "  f_logg.flush()\n",
        "\n",
        "def close_log():\n",
        "  f_logg.close()\n",
        "\n",
        "print_log_screen(\"logging to file (will be added to mail)\",log_file)\n",
        "\n",
        "\n",
        "#DON'T FORGET TO CLOSE THE FILE AT THE END\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logging to file (will be added to mail) log___2020-05-04 12:32:58.607342.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cBiO4tA-TFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"second\"\n",
        "\n",
        "# print_log_screen(\"hello\",\"world\")\n",
        "# print_log(\"hello\",\"world\",'no screen')\n",
        "\n",
        "# log_flush()\n",
        "# with open(log_file, 'rb') as f:\n",
        "#     text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "\n",
        "# print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp",
        "colab_type": "text"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab_type": "code",
        "outputId": "b85208b9-8e85-4cba-a098-0a295480bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "CELL_NAME=\"MOUNT DRIVE\"\n",
        "print_log_screen(\"mounting to drive at /gdrive\")\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mounting to drive at /gdrive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY",
        "colab_type": "text"
      },
      "source": [
        "Global Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3a4a209-b096-4279-f8db-af482c8471e2"
      },
      "source": [
        "CELL_NAME=\"BATCH_SIZE\"\n",
        "\n",
        "BATCH_SIZE = 64*2 #original 64*2 but don't have enough data right now\n",
        "print_log_screen(\"set batch size to \"+str(BATCH_SIZE))\n",
        "this_notebook=\"/gdrive/My Drive/RMSPROP Back to Thesis.ipynb\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set batch size to 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH_iDyGn0mWh",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"IMPORTS\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "#The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw",
        "colab_type": "text"
      },
      "source": [
        "Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "#tf.random.set_seed(1)\n",
        "tf.compat.v1.set_random_seed(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g",
        "colab_type": "text"
      },
      "source": [
        "Arabic Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK",
        "colab_type": "text"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l17b5XJR_5Mj",
        "colab_type": "text"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"HELPERS\"\n",
        "\n",
        "\n",
        "LTRchar=u'\\u202B'\n",
        "#https://stackoverflow.com/questions/50975763/how-right-to-left-rtl-google-colaboratory\n",
        "#https://stackoverflow.com/questions/51576756/display-render-an-html-file-inside-jupyter-notebook-on-google-colab-platform\n",
        "#https://stackoverflow.com/questions/42556063/right-to-left-and-left-to-right-printed-nicely\n",
        "\n",
        "\n",
        "##HELPERS\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print_log(len(idx))\n",
        "      if leng==-1:\n",
        "       # print_log(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "   #   print_log(result,\"#\"+str(leng))\n",
        "      return result\n",
        "\n",
        "def view_data(data):\n",
        "  for i,j,l1,l2 in data.take(3):\n",
        "    print_log_screen(LTRchar,print_by_idx_CTC(i[0],inp_lang.idx2char,l1[0]),\" | \",print_by_idx_CTC(j[0],targ_lang.idx2char,l2[0]))\n",
        "\n",
        "\n",
        "#s is a sentence\n",
        "#dict as a dictionary that maps chars to ints\n",
        "# def vectorize(s,dict):\n",
        "#   return [dict[c] for c in s]\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print_log(LTRchar,s,\":\", c,\"not in dict (skipping this char)\")\n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def un_double_letters(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "def init_log():\n",
        "  global fLog\n",
        "  fLog= open(\"NEW_all_log.txt\",\"w+\")\n",
        "\n",
        "\n",
        "def print_to_log(s):  \n",
        "  print_log(s)\n",
        "  fLog.write(s+\"\\n\")\n",
        "  fL.flush\n",
        "\n",
        "BLANK=\"_\"\n",
        "def clear_blank(s):\n",
        "  return s.replace(\"_\",\"\")\n",
        "\n",
        "def clear_arab_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF",
        "colab_type": "text"
      },
      "source": [
        "send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"SEND MAIL\"\n",
        "\n",
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "  #msg = \"COLAB WORK FINISH ALERT!\"\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  #files=[\"/content/sample_data/README.md\",\"/content/train.png\"]  #list of graphs to send or logs....\n",
        "  files=[log_file,\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  #files.append(this_notebook)\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  #server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg)\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()\n",
        "send_results(\"test\",\"test body\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP",
        "colab_type": "text"
      },
      "source": [
        "plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PLOT\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "#my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LOAD_LINES\"\n",
        "\n",
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "kfir_kuzari_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test.txt\"\n",
        "kfir_rasag_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test.txt\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    text=text.replace('ֿ',\"'\")   ##I ADDED THIS. than did the replacement in the file uploaded to drive\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n') \n",
        "  print_log(lines)\n",
        "  print_log(len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "###letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LETTER MAPPING\"\n",
        "\n",
        "#all letters\n",
        "\n",
        "\n",
        "#arab_letters=\"آأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي\" \n",
        "#heb_by_order=\"אאואיאבה'תת'ג'חכ'דד'רזסשצצ'טט'עגפקכלמנהויי\"\n",
        "#TODO EDIT: simple map will take first char in maped list for each hebrew letter\n",
        "\n",
        "\n",
        "tag=\"'\"\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"\n",
        "\n",
        "\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآأ\", # mising alif wasla  \n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "\n",
        "for i in additional_letters:\n",
        "  letter_dict[i]=i\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print_log(len(arab_heb_maping))\n",
        "print_log(arab_heb_maping)\n",
        "print_log(len(heb_arab_maping))\n",
        "print_log(heb_arab_maping)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adVEVZ_vYdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"SIMPLE LETTER MAP\"\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31",
        "colab_type": "text"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PREPROCESS SENTENCES\"\n",
        "\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    s = s.rstrip().strip()#.translate(str.maketrans('', '', \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"))\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "arr=\"بأفراد كانوا لباباً \"\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "\n",
        "def preprocess_hebrew(w):\n",
        "    w = unicode_to_ascii(w.strip())    \n",
        "    w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    w = w.replace('ֿ',\"'\")\n",
        "    return w\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "#    Takes a file of <heb, arab> phrases separated by tab\n",
        "#    Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(lines,num_examples=200000000):\n",
        "    word_pairs=[]\n",
        "    for l in lines[:num_examples]:\n",
        "      splited=l.split('\\t')\n",
        "      heb=splited[0]\n",
        "      heb=unicode_to_ascii(heb)\n",
        "      heb=preprocess_hebrew(heb)\n",
        "      heb=double_hebrew(heb)\n",
        "      heb=clear_blank(clear_arab_punctuation(heb))\n",
        "      arr=splited[1]\n",
        "      arr=remove_arab_nikud(arr)\n",
        "      arr=standard_nunization(arr)\n",
        "      arr=clear_blank(clear_arab_punctuation(arr))\n",
        "      arr=unicode_to_ascii(arr) \n",
        "      word_pairs.append([heb,arr])        \n",
        "    return word_pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWRQ3jGAesuv",
        "colab_type": "text"
      },
      "source": [
        "small example for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM",
        "colab_type": "code",
        "outputId": "b9bf27e6-92ac-4b1f-e164-a3825fda0f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "CELL_NAME=\"SMALL TRY\"\n",
        "\n",
        "this_string='وأهل الأديان ثمّ على'\n",
        "this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "this_string='ثمّ'\n",
        "\n",
        "#this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "print_log_screen(len(this_string))\n",
        "this_string=unicode_to_ascii(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "print_log_screen(len(this_string))\n",
        "this_string"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ثمّ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq",
        "colab_type": "text"
      },
      "source": [
        "languageIndex class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print_log(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print_log(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print_log(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print_log(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT",
        "colab_type": "text"
      },
      "source": [
        "load_dataset method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF LOAD_DATASET\"\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#inp_lang = LanguageIndex(heb_by_order)\n",
        "inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys()))\n",
        "#targ_lang = LanguageIndex(arab_letters+u\"\\u0651\"+\"\".join(tanween)+ u\"\\u0621\")# shadda and hamza on line  (arab_nikud)\n",
        "targ_lang = LanguageIndex(\"\".join(arab_heb_maping.keys())+\"\".join(tanween)+ shada + hamza_on_line)\n",
        "\n",
        "\n",
        "def load_dataset(pairs):\n",
        "    # creating cleaned input, output pairs    \n",
        "   # pairs = create_dataset(lines,num_examples)  \n",
        "  \n",
        "    input_tensor = [vectorize(heb,inp_lang.char2idx) for heb, arr in pairs]\n",
        "    input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "    # English sentences\n",
        "    target_tensor = [vectorize(arr,targ_lang.char2idx) for heb, arr in pairs]\n",
        "    target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "    print_log()\n",
        "    print_log(LTRchar,pairs[0])\n",
        "    print_log(input_lenghts[0])\n",
        "    print_log(target_lengths[0])\n",
        "    print_log(input_tensor[0])\n",
        "    print_log(target_tensor[0])\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                  value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    \n",
        "    print_log()\n",
        "    print_log(LTRchar,pairs[0])\n",
        "    print_log(input_lenghts[0])\n",
        "    print_log(target_lengths[0])\n",
        "    print_log(input_tensor[0])\n",
        "    print_log(len(input_tensor[0]))\n",
        "    print_log(target_tensor[0])\n",
        "    print_log(len(target_tensor[0]))\n",
        "\n",
        "    return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlQJhoggxw3",
        "colab_type": "text"
      },
      "source": [
        "call load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LOAD DATASET\"\n",
        "\n",
        "lines=load_lines()\n",
        "#num_examples = 2000000000  #don't limit\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor, target_tensor ,input_lenghts,target_lengths = load_dataset(pairs)\n",
        "lines=load_lines(haemunot)\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = load_dataset(pairs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN",
        "colab_type": "text"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF GEN DATA\"\n",
        "\n",
        "def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths,_test_size=0.2):  \n",
        "  input_tensor_train, input_tensor_val, \\\n",
        "  target_tensor_train, target_tensor_val, \\\n",
        "  input_lengths_train, input_lengths_val, \\\n",
        "  target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "                                                              target_tensor,\n",
        "                                                              input_lenghts,\n",
        "                                                              target_lengths, test_size=_test_size)\n",
        "  print_log(len(input_tensor_train), \n",
        "        len(target_tensor_train), \n",
        "        len(input_tensor_val), \n",
        "        len(target_tensor_val))\n",
        "  \n",
        "  BUFFER_SIZE = len(input_tensor_train)\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "                                                target_tensor_train,\n",
        "                                                input_lengths_train,\n",
        "                                                target_lengths_train)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "                                                    target_tensor_val,\n",
        "                                                    input_lengths_val,\n",
        "                                                    target_lengths_val)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)                                                  \n",
        "  \n",
        "  dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return dataset_double,test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0elYUMb8Ls",
        "colab_type": "text"
      },
      "source": [
        "##activate gen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DklcYhg_b6Lr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "932ad604-6851-413a-d4d7-f6abe368f003"
      },
      "source": [
        "CELL_NAME=\"USE GEN DATA\"\n",
        "\n",
        "#ACTIVATE\n",
        "dataset_double,test_dataset_double=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "print(\"test_dataset_double\")\n",
        "view_data(test_dataset_double)\n",
        "dataset_double1,test_dataset_double1=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "print(\"test_dataset_double1\")\n",
        "view_data(test_dataset_double1)\n",
        "print(\"test_dataset_double1\")\n",
        "view_data(test_dataset_double1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_dataset_double\n",
            "‫ ההדד''אא .. קקאאלל אאללככ''זזרריי ::  |  هذا . قال الخزريّ :\n",
            "‫ ,, ווללאא HH HH ללממאא פפיי דד''ללככ  |  , ولا H H لما في ذلك\n",
            "‫ וואאססתתחחאאללתתההממאא וודדככ''ווללההממאא  |  واستحالتهما ودخولهما\n",
            "test_dataset_double1\n",
            "‫ ההאאההננאא ממצצננוועעאא ככדד''אאככ קקווללננאא  |  هاهنا مصنوعاً كذاك قولنا\n",
            "‫ אאללייהה .. ווננתתבבעע דד''ללככ בבששררחח  |  إليه . ونتبع ذلك بشرح\n",
            "‫ .. ווקקללתת אאייצצ''אא ללעעללההממ יידדעעווננ  |  . وقلت أيضاً لعلّهم يدّعون\n",
            "test_dataset_double1\n",
            "‫ ההאאההננאא ממצצננוועעאא ככדד''אאככ קקווללננאא  |  هاهنا مصنوعاً كذاك قولنا\n",
            "‫ אאללייהה .. ווננתתבבעע דד''ללככ בבששררחח  |  إليه . ونتبع ذلك بشرح\n",
            "‫ .. ווקקללתת אאייצצ''אא ללעעללההממ יידדעעווננ  |  . وقلت أيضاً لعلّهم يدّعون\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA",
        "colab_type": "text"
      },
      "source": [
        "##Shuffled test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"SHUFFLED TEST\"\n",
        "\n",
        "def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "  val_inputs_list=[]\n",
        "  val_outputs_list=[]\n",
        "\n",
        "  for i,j,l1,l2 in test_dataset:\n",
        "    for tt in range(BATCH_SIZE):\n",
        "     # print_log(i[tt],j[tt])\n",
        "      i_prediction=print_by_idx_CTC(tf.constant(i[tt]),inp_lang.idx2char,l1[tt])\n",
        "      j_prediction=print_by_idx_CTC(tf.constant(j[tt]),targ_lang.idx2char,l2[tt])      \n",
        "      if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "        val_inputs_list+=i_prediction.split()\n",
        "        val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "  print_log(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "  word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "  print_log(\"BEFORE SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print_log(i)\n",
        "  random.shuffle(word_pairs)\n",
        "  print_log(\"AFTER SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print_log(i)\n",
        "  return word_pairs\n",
        "\n",
        "#TESTING\n",
        "#word_pairs1=get_shuffled_word_pairs(test_dataset_double1.take(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF GET SHUFFLE DATA\"\n",
        "def get_shuffled_data(word_pairs):\n",
        "\n",
        "  accum=0\n",
        "  heb_acum=\"\"\n",
        "  arab_acum=\"\"\n",
        "  results_line=[]\n",
        "  for i,j in word_pairs:\n",
        "    if accum>19:\n",
        "      results_line.append(un_double_letters(heb_acum)+'\\t'+arab_acum)\n",
        "      assert(len(i)%2==0)\n",
        "      accum=len(i)/2\n",
        "      heb_acum=i\n",
        "      arab_acum=j\n",
        "    else:\n",
        "      heb_acum+=\" \"+i\n",
        "      arab_acum+=\" \"+j \n",
        "      assert(len(i)%2==0)\n",
        "      accum += len(i)/2 + 1;\n",
        "  results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "  print_log(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "  input_tensor_shuffle, target_tensor_shuffle \\\n",
        "  ,input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "  print_log(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "  print_log(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "  BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "  shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "                                                            target_tensor_shuffle,\n",
        "                                                            input_lenghts_shuffle,\n",
        "                                                            target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "  shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GEN SHUFFLED DATA\"\n",
        "\n",
        "# word_pairs=get_shuffled_word_pairs(test_dataset_double)\n",
        "# shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "# view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "# word_pairs1=get_shuffled_word_pairs(test_dataset_double1)\n",
        "# shuffle_test_dataset_double1=get_shuffled_data(word_pairs1)\n",
        "# view_data(shuffle_test_dataset_double1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Single words test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GEN SINGEL WORDS\"\n",
        "\n",
        "# results_line=[]\n",
        "# for i,j in word_pairs:\n",
        "#     results_line.append(un_double_letters(i)+'\\t'+j)\n",
        "\n",
        "# print_log(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "# input_tensor_shuffle, target_tensor_shuffle \\\n",
        "# , input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "# print_log(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "# BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "# single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "#                                                                 target_tensor_shuffle,\n",
        "#                                                                 input_lenghts_shuffle,\n",
        "#                                                                 target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# view_data(single_words_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8",
        "colab_type": "text"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GEN SYNTH\"\n",
        "\n",
        "#TODO edit when time permits\n",
        "\n",
        "def load_text_for_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(ibnsina, 'rb') as f:\n",
        "      ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  ibnsina_text=clear_arab_punctuation(ibnsina_text)\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  import re\n",
        "  ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "  ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "  return ibnsina_text\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TRY SYNTH\"\n",
        "\n",
        "# #ACTIVATE\n",
        "# ibnsina_text=load_text_for_synth()\n",
        "\n",
        "# #STATSTICS OF SYNTH TEXT\n",
        "# sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "# print_log(\"NOT IN LETTER LIST:\")\n",
        "# for c in sina_vocab:\n",
        "#    if c not in targ_lang.char2idx:\n",
        "#       print_log(\"(\",c,\")\")\n",
        "\n",
        "# print_log(\"\\nLETTER COUNTS\")\n",
        "# for i in range(len(sina_vocab)):\n",
        "#   print_log(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   \n",
        "\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "# #sina_words=ibnsina_text.split(\" \")\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "# sina_words=ibnsina_text.split() #for removing also newlines\n",
        "# print_log(ibnsina_text[:100])\n",
        "# sina_words[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF gen_synth_sentences\"\n",
        "def gen_synth_sentences(sina_words):\n",
        "\n",
        "  #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in sina_words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      sentences.append(unicode_to_ascii(remove_arab_nikud(\" \".join(res))))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "    #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "  return sentences\n",
        "\n",
        "# #ACTIVATE\n",
        "# sentences=gen_synth_sentences(sina_words)\n",
        "# len(sentences)\n",
        "# sentences[:5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF gen_dropout\"\n",
        "\n",
        "def gen_dropout(sentences,keep_prob=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)        \n",
        "    heb=[]\n",
        "    for c in arr:\n",
        "      if c in arab_heb_maping:\n",
        "        if (np.random.binomial(1,keep_prob)) or c==\" \":\n",
        "          heb.append(arab_heb_maping[c])\n",
        "        else:\n",
        "          heb.append(BLANK)\n",
        "\n",
        "    heb_sentences.append(double_hebrew(preprocess_hebrew(\"\".join(heb))))\n",
        "\n",
        "  print_log(heb_sentences[:5]) \n",
        "  print_log(arab_setences[:5])\n",
        "\n",
        "  print_log(len(arab_setences))\n",
        "  input_tensor_synth, target_tensor_synth, \\\n",
        "  input_lenghts_synth,target_lengths_synth = load_dataset(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # Show length\n",
        "  print_log(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  print_log(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "                                                      target_tensor_synth,\n",
        "                                                      input_lenghts_synth,\n",
        "                                                      target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF gen_dropout_all\"\n",
        "\n",
        "def gen_dropout_all(keep=1):\n",
        "  path=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"\n",
        "  synth_text=load_text_for_synth(path)\n",
        "  sentences=gen_synth_sentences(synth_text.split())\n",
        "  dataset_double_synt=gen_dropout(sentences,keep)\n",
        "\n",
        "  path1=\"/gdrive/My Drive/JUDEO-ARAB/daruri-IR.txt\"\n",
        "  synth_text1=load_text_for_synth(path1)\n",
        "  sentences1=gen_synth_sentences(synth_text1.split())\n",
        "  dataset_double_synt1=gen_dropout(sentences1,keep).concatenate(dataset_double_synt)\n",
        "\n",
        "  path2=\"/gdrive/My Drive/JUDEO-ARAB/farabi-tahsil.txt\"\n",
        "  synth_text2=load_text_for_synth(path2)\n",
        "  sentences2=gen_synth_sentences(synth_text2.split())\n",
        "  dataset_double_synt2=gen_dropout(sentences2,keep).concatenate(dataset_double_synt1)\n",
        "\n",
        "\n",
        "  path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "  synth_text3=load_text_for_synth(path3)\n",
        "  sentences3=gen_synth_sentences(synth_text3.split())\n",
        "  dataset_double_synt3=gen_dropout(sentences2,keep).concatenate(dataset_double_synt2)\n",
        "\n",
        "  BUFFER_SIZE=1000 #TODO get size\n",
        "  return dataset_double_synt3.concatenate(dataset_double).shuffle(BUFFER_SIZE)  ##TODO: this is without dropout. \n",
        "#view_data(gen_dropout_all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4",
        "colab_type": "text"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "76b033fd-3eee-4b50-ab37-d4b39b6ae10a"
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL\"\n",
        "\n",
        "#NETWORK PARAMS\n",
        "# The embedding dimension\n",
        "embedding_dim = 8 #256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 #1024\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  #rnn = tf.keras.layers.CuDNNGRU\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "  #rnn = tf.keras.layers.LSTM #see https://stackoverflow.com/questions/55761337/module-tensorflow-python-keras-api-v2-keras-layers-has-no-attribute-cudnnlst\n",
        "\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "#FUNCTION TO BUILD MODEL\n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    \n",
        "  \n",
        "\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-50c8d5f237f1>:11: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "494a47c8-3356-481f-cc30-86e3aedb1e16"
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL1\"\n",
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "model=rebuild()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 8)            384       \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (128, None, 2048)         6352896   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 65)           133185    \n",
            "=================================================================\n",
            "Total params: 63,146,433\n",
            "Trainable params: 63,146,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG",
        "colab_type": "text"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF show diff\"\n",
        "from termcolor import colored\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "# #USEAGE:\n",
        "# s1=\"لامة النصارى واستحقو\" \n",
        "# s2=\"لأمّة النصارى واستحقوّا\"\n",
        "# a,b=show_diff(s1,s2,'blue')\n",
        "# #print_log(\"\".join(b))\n",
        "# print_log(a,\"|\",b)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo",
        "colab_type": "text"
      },
      "source": [
        "## forward run each letter seperatly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST single letters\"\n",
        "def test__CTC_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print_log(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print_log(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "       print(\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print_log(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print_log(\"####max is the blank symbole\")\n",
        "      print_log('-'*10)\n",
        "  \n",
        "#test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF",
        "colab_type": "text"
      },
      "source": [
        "##forward run a sentence (with top n beam search results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM2_GVYmOwdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF test__CTC_word_multiline\"\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    l = double_hebrew(l)\n",
        "    v=vectorize(l,inp_lang.char2idx)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  #print_log(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "  #inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  # if print_deteils:\n",
        "  #   print_log('#####deteils')\n",
        "  # for jj in range(num_of_letters):\n",
        "  #     if print_deteils:\n",
        "  #       print_log(word_str[jj])\n",
        "      \n",
        "  #     pred_distr=predict_ltrs[0][jj]\n",
        "  #     pred_distr=tf.nn.softmax(pred_distr)\n",
        "  #     if print_deteils:\n",
        "  #       for ii in range(num_of_arab_letters):\n",
        "  #         print_log(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "  #       print_log(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "  #       print_log()\n",
        "  #     maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "  #     if print_deteils:\n",
        "  #       print_log(maximum)\n",
        "  #     if (maximum<42):\n",
        "  #       if print_deteils:\n",
        "  #         print_log(targ_lang.vocab[maximum])\n",
        "  #       res.append(targ_lang.vocab[maximum])\n",
        "  #     else:\n",
        "  #       if print_deteils:\n",
        "  #         print_log(\"####max is the blank symbole\")\n",
        "  #       res.append(\"-\")\n",
        "  #     if print_deteils:\n",
        "  #       print_log('-'*10)\n",
        "  # print_log(word_str)\n",
        "  #print_log(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print_log(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=print_by_idx_CTC(dense[t],targ_lang.idx2char)\n",
        "      print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "# lines='''כמאלא\n",
        "# כמאלא'''\n",
        "\n",
        "# print_log_screen(test__CTC_word_multiline(lines,3,BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K6i5QWGO4DS",
        "colab_type": "text"
      },
      "source": [
        "##TEST KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ec7daf71-7c01-4d68-ae81-101b1dcc9344"
      },
      "source": [
        "CELL_NAME=\"DEF TEST KFIR\"\n",
        "import editdistance\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline_kfir(word_str,targ_str,num_of_paths=1,_BATCH_SIZE=BATCH_SIZE,SHOW_PRINT=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  targ_lines=targ_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  num_of_letters= len(targ_str) - targ_str.count('\\n')\n",
        "  assert(num_of_lines==len(targ_lines))\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    #l = double_hebrew(l)\n",
        "    v=vectorize(l,inp_lang.char2idx)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  \n",
        "  total_res=[]\n",
        "  total_edit_dist=0\n",
        "  total_normalized_edit_dist=0\n",
        "  line_counter=1\n",
        "  for t in range(num_of_lines):\n",
        "    #print_log(lines[t])\n",
        "    # print_log(\"real:\",targ_lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=print_by_idx_CTC(dense[t],targ_lang.idx2char).strip() \n",
        "      # print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "      total_res.append(prediction)\n",
        "    # print_log(len(prediction))\n",
        "    #print_log(len(targ_lines[t]))\n",
        "    ed_dist=editdistance.eval(targ_lines[t], prediction)\n",
        "    # print_log(\"edit_dist:\",ed_dist)\n",
        "    normalized_ed_dist=ed_dist/len(targ_lines[t])\n",
        "    real,prediction=show_diff(targ_lines[t],prediction,'red')\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"({0})\".format(line_counter),LTRchar,un_double_letters(lines[t]),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(ed_dist))\n",
        "    line_counter+=1\n",
        "\n",
        "\n",
        "    total_normalized_edit_dist+=normalized_ed_dist\n",
        "    total_edit_dist+=ed_dist\n",
        " # print(total_edit_dist,num_of_letters)\n",
        "  return total_res,total_normalized_edit_dist,num_of_lines,total_edit_dist,num_of_letters\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''יכ'אלף\n",
        "עלי\n",
        "הד'ה\n",
        "אלאמאנה\n",
        "ולא'''\n",
        "\n",
        "lines='''ייככ''אאללףף\n",
        "עעלליי\n",
        "ההדד''הה\n",
        "אאללאאממאאננהה\n",
        "ווללאא'''\n",
        "\n",
        "lines='''ייככ''אאללףף\n",
        "אאללאאממאאננהה'''\n",
        "\n",
        "targ_lines='''يخالف\n",
        "على\n",
        "هذه\n",
        "الأمانة\n",
        "ولا'''\n",
        "\n",
        "targ_lines='''يخالف\n",
        "الأمانة'''\n",
        "\n",
        "test__CTC_word_multiline_kfir(lines,targ_lines,1,BATCH_SIZE,True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1) ‫ יכ'אלף | \u001b[1m\u001b[31mي\u001b[0mخ\u001b[1m\u001b[31mالف\u001b[0m | \u001b[1m\u001b[31mّكأ\u001b[0mخ\u001b[1m\u001b[31mH\u001b[0m | 5.0000\n",
            "(2) ‫ אלאמאנה | \u001b[1m\u001b[31mالأمانة\u001b[0m | \u001b[1m\u001b[31mخوخؤح\u001b[0m | 7.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ّكأخH', 'خوخؤح'], 2.0, 2, 12, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5e5-59jCxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "a59010ed-7740-4f7e-c911-96f8e10c8cfc"
      },
      "source": [
        "CELL_NAME=\"DEF test_kfir1\"\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "\n",
        "def test_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False):\n",
        "\n",
        "  lines=load_lines(data_path)\n",
        "  pairs = create_dataset(lines)  \n",
        "\n",
        "  if replace_GAIN:\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs]\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "\n",
        "  num_of_lines=len(pairs)\n",
        "  index=0\n",
        "  total_num_of_examples=0\n",
        "  total_num_of_letters=0\n",
        "  total_sum_e_d_normalized=0\n",
        "  total_sum_e_d=0\n",
        "  while index<=num_of_lines:\n",
        "    batch_hebrew=hebrew_lines[index:index+BATCH_SIZE]\n",
        "    batch_arab=arab_lines[index:index+BATCH_SIZE]\n",
        "    _,sum_of_e_d_normalized,num_of_examples,sum_of_e_d,num_of_letters=test__CTC_word_multiline_kfir('\\n'.join(batch_hebrew),'\\n'.join(batch_arab),1,BATCH_SIZE,SHOW_PRINT)\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"BATCH (sum_of_e_d_normalized,num_of_examples): \",sum_of_e_d_normalized,num_of_examples)\n",
        "      print_log_screen(\"BATCH (sum_of_e_d,num_of_letters): \",sum_of_e_d,num_of_letters)\n",
        "    total_num_of_examples+=num_of_examples\n",
        "    total_num_of_letters+=num_of_letters\n",
        "    total_sum_e_d+=sum_of_e_d\n",
        "    total_sum_e_d_normalized+=sum_of_e_d_normalized\n",
        "    index+=BATCH_SIZE\n",
        "\n",
        "  print_log_screen(\"#examples:\",total_num_of_examples,\", accuracy:\",1-total_sum_e_d_normalized/total_num_of_examples)\n",
        "  print_log_screen(\"#letters:\",total_num_of_letters,\", accuracy1:\",1-total_sum_e_d/total_num_of_letters)\n",
        "  return total_sum_e_d_normalized/total_num_of_examples,total_sum_e_d/total_num_of_letters\n",
        "\n",
        "#test_kfir(kfir_kuzari_test,True)\n",
        "# #test_kfir(kfir_kuzari_test)\n",
        "# test_kfir(kfir_rasag_test,True)\n",
        "#test_kfir(kfir_rasag_test,True,True)\n",
        "\n",
        "# #test_kfir(kfir_rasag_test)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#examples: 500 , accuracy: 0.041299206349206585\n",
            "#letters: 2325 , accuracy1: 0.04946236559139783\n",
            "(1) ‫ דאר | \u001b[1m\u001b[31mدار\u001b[0m | \u001b[1m\u001b[31mبلب\u001b[0m | 3.0000\n",
            "(2) ‫ אלגזא | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mجزاء\u001b[0m | ل\u001b[1m\u001b[31mوبآ\u001b[0m | 5.0000\n",
            "(3) ‫ וקבל | \u001b[1m\u001b[31mوقبل\u001b[0m | \u001b[1m\u001b[31m,7,\u001b[0m | 4.0000\n",
            "(4) ‫ ד'לכ | \u001b[1m\u001b[31mذ\u001b[0mل\u001b[1m\u001b[31mك\u001b[0m | ل\u001b[1m\u001b[31m6ر\u001b[0m | 3.0000\n",
            "(5) ‫ מא | \u001b[1m\u001b[31mما\u001b[0m | \u001b[1m\u001b[31mطب\u001b[0m | 2.0000\n",
            "(6) ‫ ראי | \u001b[1m\u001b[31mرأى\u001b[0m | \u001b[1m\u001b[31mل)ب\u001b[0m | 3.0000\n",
            "(7) ‫ אנ | \u001b[1m\u001b[31mأن\u001b[0m | \u001b[1m\u001b[31mط7\u001b[0m | 2.0000\n",
            "(8) ‫ יפרק | \u001b[1m\u001b[31mيفرّق\u001b[0m | \u001b[1m\u001b[31m,ئ\"\u001b[0m | 5.0000\n",
            "(9) ‫ בינ | \u001b[1m\u001b[31mبين\u001b[0m | \u001b[1m\u001b[31m7,\"\u001b[0m | 3.0000\n",
            "(10) ‫ רוחה | ر\u001b[1m\u001b[31mوحه\u001b[0m | \u001b[1m\u001b[31mل\u001b[0mر\u001b[1m\u001b[31m;\u001b[0m | 4.0000\n",
            "(11) ‫ וגסמה | \u001b[1m\u001b[31mوجسمه\u001b[0m | \u001b[1m\u001b[31m7ط:آ\u001b[0m | 5.0000\n",
            "(12) ‫ אלי | \u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | ل\u001b[1m\u001b[31m)ب\u001b[0m | 3.0000\n",
            "(13) ‫ וקת | \u001b[1m\u001b[31mوقت\u001b[0m | \u001b[1m\u001b[31m,7ب\u001b[0m | 3.0000\n",
            "(14) ‫ אסתכמאל | \u001b[1m\u001b[31mاستكمال\u001b[0m | \u001b[1m\u001b[31mطبوطأخ\u001b[0m | 7.0000\n",
            "(15) ‫ אלנפוס | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mنفوس\u001b[0m | ل\u001b[1m\u001b[31mٌ,ض:\u001b[0m | 5.0000\n",
            "(16) ‫ חתי | \u001b[1m\u001b[31mحتى\u001b[0m | \u001b[1m\u001b[31mطبئ\u001b[0m | 3.0000\n",
            "(17) ‫ יגמעהא | \u001b[1m\u001b[31mيجمعها\u001b[0m | \u001b[1m\u001b[31mلط;ضآ\u001b[0m | 6.0000\n",
            "(18) ‫ אלגמיע | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mجميع\u001b[0m | ل\u001b[1m\u001b[31mو-;ئ\u001b[0m | 5.0000\n",
            "(19) ‫ עלי | \u001b[1m\u001b[31mع\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | ل\u001b[1m\u001b[31mر0\u001b[0m | 3.0000\n",
            "(20) ‫ מא | \u001b[1m\u001b[31mما\u001b[0m | \u001b[1m\u001b[31mطب\u001b[0m | 2.0000\n",
            "(21) ‫ בינת | ب\u001b[1m\u001b[31mيّنت\u001b[0m | \u001b[1m\u001b[31m7,\u001b[0mب | 5.0000\n",
            "(22) ‫ פלא | \u001b[1m\u001b[31mفلا\u001b[0m | \u001b[1m\u001b[31mضب\u001b[0m | 3.0000\n",
            "(23) ‫ נעלמ | \u001b[1m\u001b[31mنع\u001b[0mل\u001b[1m\u001b[31mم\u001b[0m | ل\u001b[1m\u001b[31m,0\u001b[0m | 4.0000\n",
            "(24) ‫ יהודיא | \u001b[1m\u001b[31mيهوديّاً\u001b[0m | \u001b[1m\u001b[31mلبئ;آ\u001b[0m | 8.0000\n",
            "(25) ‫ יכ'אלפ | \u001b[1m\u001b[31mي\u001b[0mخ\u001b[1m\u001b[31mالف\u001b[0m | \u001b[1m\u001b[31mمطأ\u001b[0mخ\u001b[1m\u001b[31mر\u001b[0m | 5.0000\n",
            "(26) ‫ עלי | \u001b[1m\u001b[31mع\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mط\u001b[0mل\u001b[1m\u001b[31m0\u001b[0m | 2.0000\n",
            "(27) ‫ הד'ה | \u001b[1m\u001b[31mه\u001b[0mذ\u001b[1m\u001b[31mه\u001b[0m | \u001b[1m\u001b[31mل\u001b[0mذ\u001b[1m\u001b[31mل\u001b[0m | 2.0000\n",
            "(28) ‫ אלאמאנה | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mأمانة\u001b[0m | ل\u001b[1m\u001b[31mوخؤئ,\u001b[0m | 6.0000\n",
            "(29) ‫ ולא | \u001b[1m\u001b[31mو\u001b[0mل\u001b[1m\u001b[31mا\u001b[0m | \u001b[1m\u001b[31mب\u001b[0mل\u001b[1m\u001b[31mب\u001b[0m | 2.0000\n",
            "(30) ‫ יסתצעב | \u001b[1m\u001b[31mيستصع\u001b[0mب | \u001b[1m\u001b[31mل\u001b[0mب\u001b[1m\u001b[31mخرH\u001b[0m | 6.0000\n",
            "(31) ‫ ענד | \u001b[1m\u001b[31mعند\u001b[0m | \u001b[1m\u001b[31mذ,ئ\u001b[0m | 3.0000\n",
            "(32) ‫ עקלה | \u001b[1m\u001b[31mعق\u001b[0mل\u001b[1m\u001b[31mه\u001b[0m | ل\u001b[1m\u001b[31m,ب\u001b[0m | 4.0000\n",
            "(33) ‫ כיפ | \u001b[1m\u001b[31mكيف\u001b[0m | \u001b[1m\u001b[31m7,0\u001b[0m | 3.0000\n",
            "(34) ‫ יחיי | ي\u001b[1m\u001b[31mحيي\u001b[0m | \u001b[1m\u001b[31m-\u001b[0mي\u001b[1m\u001b[31m!\u001b[0m | 3.0000\n",
            "(35) ‫ רבה | \u001b[1m\u001b[31mر\u001b[0mب\u001b[1m\u001b[31mّه\u001b[0m | \u001b[1m\u001b[31mل\u001b[0mب\u001b[1m\u001b[31m7\u001b[0m | 3.0000\n",
            "(36) ‫ אלמותי | \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mموتى\u001b[0m | \u001b[1m\u001b[31mطو\u001b[0mل\u001b[1m\u001b[31m;ئ\u001b[0m | 6.0000\n",
            "(37) ‫ אד' | \u001b[1m\u001b[31mإذ\u001b[0m | \u001b[1m\u001b[31mلب0\u001b[0m | 3.0000\n",
            "(38) ‫ קד | \u001b[1m\u001b[31mقد\u001b[0m | \u001b[1m\u001b[31m7,\u001b[0m | 2.0000\n",
            "(39) ‫ צח | \u001b[1m\u001b[31mصحّ\u001b[0m | \u001b[1m\u001b[31mرغ\u001b[0m | 3.0000\n",
            "(40) ‫ לה | ل\u001b[1m\u001b[31mه\u001b[0m | ل\u001b[1m\u001b[31mب\u001b[0m | 1.0000\n",
            "(41) ‫ אנה | \u001b[1m\u001b[31mأنه\u001b[0m | \u001b[1m\u001b[31mطلب\u001b[0m | 3.0000\n",
            "(42) ‫ כ'לק | \u001b[1m\u001b[31mخلق\u001b[0m | \u001b[1m\u001b[31mطر,\u001b[0m | 3.0000\n",
            "(43) ‫ שיא | \u001b[1m\u001b[31mشيئاً\u001b[0m | \u001b[1m\u001b[31m7لب\u001b[0m | 5.0000\n",
            "(44) ‫ לא | \u001b[1m\u001b[31mلا\u001b[0m | \u001b[1m\u001b[31m7ب\u001b[0m | 2.0000\n",
            "(45) ‫ מנ | \u001b[1m\u001b[31mمن\u001b[0m | \u001b[1m\u001b[31mط,\u001b[0m | 2.0000\n",
            "(46) ‫ שי | \u001b[1m\u001b[31mشيء\u001b[0m | \u001b[1m\u001b[31m7,\u001b[0m | 3.0000\n",
            "(47) ‫ פלא | \u001b[1m\u001b[31mف\u001b[0mل\u001b[1m\u001b[31mا\u001b[0m | ل\u001b[1m\u001b[31mرب\u001b[0m | 3.0000\n",
            "(48) ‫ יגוז | \u001b[1m\u001b[31mيجوز\u001b[0m | \u001b[1m\u001b[31mلبئ\u001b[0m | 4.0000\n",
            "(49) ‫ אנ | \u001b[1m\u001b[31mأن\u001b[0m | \u001b[1m\u001b[31mلب\u001b[0m | 2.0000\n",
            "(50) ‫ יסתעסר | \u001b[1m\u001b[31mيستعسر\u001b[0m | \u001b[1m\u001b[31mلبخبئ\u001b[0m | 6.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  47.85714285714286 50\n",
            "BATCH (sum_of_e_d,num_of_letters):  183 192\n",
            "#examples: 50 , accuracy: 0.042857142857142816\n",
            "#letters: 192 , accuracy1: 0.046875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9571428571428572, 0.953125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L",
        "colab_type": "text"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "373dc10c-e141-43fc-dfb7-bba5fea32233"
      },
      "source": [
        "CELL_NAME=\"TEST_BASELINE\"\n",
        "import editdistance\n",
        "def test_loss_baseline(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"baseline accuracy: \",1-total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 1-total_accuracy\n",
        "# #test_loss_baseline(limit=3)\n",
        "# print_log_screen(\"KUZARI TEST\")\n",
        "# test_loss_baseline(test_dataset_double)\n",
        "# print_log_screen(\"RASAG TEST\")\n",
        "# test_loss_baseline(test_dataset_double1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KUZARI TEST\n",
            "(1) ‫ הד'א . קאל אלכ'זרי : | هذا . قال الخزري\u001b[1m\u001b[31mّ\u001b[0m : | هذا . قال الخزري : | 0.0526\n",
            "(2) ‫ , ולא H H למא פי ד'לכ | , ولا H H لما في ذلك | , ولا H H لما في ذلك | 0.0000\n",
            "(3) ‫ ואסתחאלתהמא ודכ'ולהמא | واستحالتهما ودخولهما | واستحالتهما ودخولهما | 0.0000\n",
            "(4) ‫ ט'הרת פי אברהימ . קאל | ظهرت في \u001b[1m\u001b[31mإ\u001b[0mبر\u001b[1m\u001b[31mا\u001b[0mهيم . قال | ظهرت في \u001b[1m\u001b[31mا\u001b[0mبرهيم . قال | 0.0952\n",
            "(5) ‫ ואלוחי ותסמי כ'אציתהא | والوحي وتسم\u001b[1m\u001b[31mّى\u001b[0m خاص\u001b[1m\u001b[31mّ\u001b[0mيتها | والوحي وتسم\u001b[1m\u001b[31mي\u001b[0m خاصيتها | 0.1364\n",
            "(6) ‫ מנ וקת יכ'תצ בה , ימכנ | من وقت يختص\u001b[1m\u001b[31mّ\u001b[0m به , يمكن | من وقت يختص به , يمكن | 0.0455\n",
            "(7) ‫ : אנה לא שכ יתמני אנ | : \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mه لا شك\u001b[1m\u001b[31mّ\u001b[0m يتمن\u001b[1m\u001b[31mّى\u001b[0m \u001b[1m\u001b[31mأ\u001b[0mن | : \u001b[1m\u001b[31mا\u001b[0mنه لا شك يتمن\u001b[1m\u001b[31mي\u001b[0m \u001b[1m\u001b[31mا\u001b[0mن | 0.2609\n",
            "(8) ‫ ללמעמורה כלהא אלא באנ | للمعمور\u001b[1m\u001b[31mة\u001b[0m كل\u001b[1m\u001b[31mّ\u001b[0mها \u001b[1m\u001b[31mإ\u001b[0mلا\u001b[1m\u001b[31mّ\u001b[0m ب\u001b[1m\u001b[31mأ\u001b[0mن | للمعمور\u001b[1m\u001b[31mه\u001b[0m كلها \u001b[1m\u001b[31mا\u001b[0mلا ب\u001b[1m\u001b[31mا\u001b[0mن | 0.2174\n",
            "(9) ‫ מנ כ'יאל ופכר וגיר ד'לכ | من خيال وفكر وغير ذلك | من خيال وفكر وغير ذلك | 0.0000\n",
            "(10) ‫ מלכה דונ תעלמ , ינסג' | ملك\u001b[1m\u001b[31mة\u001b[0m دون تعل\u001b[1m\u001b[31mّ\u001b[0mم , ينسج | ملك\u001b[1m\u001b[31mه\u001b[0m دون تعلم , ينسج | 0.0952\n",
            "(11) ‫ , וליס ללאנסאנ פי תלכ | , وليس لل\u001b[1m\u001b[31mإ\u001b[0mنسان في تلك | , وليس لل\u001b[1m\u001b[31mا\u001b[0mنسان في تلك | 0.0476\n",
            "(12) ‫ מטלובכ , אעני אלאתצאל | مطلبك , \u001b[1m\u001b[31mأ\u001b[0mعني الات\u001b[1m\u001b[31mّ\u001b[0mصال | مطل\u001b[1m\u001b[31mو\u001b[0mبك , \u001b[1m\u001b[31mا\u001b[0mعني الاتصال | 0.1429\n",
            "(13) ‫ , באי שי ידרכ ופי אי | , ب\u001b[1m\u001b[31mأ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m شي\u001b[1m\u001b[31mء\u001b[0m يدرك وفي \u001b[1m\u001b[31mأ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | , ب\u001b[1m\u001b[31mا\u001b[0mي شي يدرك وفي \u001b[1m\u001b[31mا\u001b[0mي | 0.2174\n",
            "(14) ‫ ונשכר עליהא פי H H H | ونشكر عليها في H H H | ونشكر عليها في H H H | 0.0000\n",
            "(15) ‫ ואלאתפאקיה כל אלמבאלאה | والات\u001b[1m\u001b[31mّ\u001b[0mفاقي\u001b[1m\u001b[31mة\u001b[0m كل\u001b[1m\u001b[31mّ\u001b[0m المبالا\u001b[1m\u001b[31mة\u001b[0m | والاتفاقي\u001b[1m\u001b[31mه\u001b[0m كل المبالا\u001b[1m\u001b[31mه\u001b[0m | 0.1667\n",
            "(16) ‫ מנ חות אלבחר קותא מנ | من حوت البحر قوتا\u001b[1m\u001b[31mً\u001b[0m من | من حوت البحر قوتا من | 0.0476\n",
            "(17) ‫ אתאהמא אלנבוה אבני ת'מאנינ | \u001b[1m\u001b[31mأ\u001b[0mتاهما النبو\u001b[1m\u001b[31mّة\u001b[0m ابني ثمانين | \u001b[1m\u001b[31mا\u001b[0mتاهما النبو\u001b[1m\u001b[31mه\u001b[0m ابني ثمانين | 0.1154\n",
            "baseline accuracy:  0.9051579503509072\n",
            "RASAG TEST\n",
            "(1) ‫ האהנא מצנועא כד'אכ קולנא | هاهنا مصنوعا\u001b[1m\u001b[31mً\u001b[0m كذاك قولنا | هاهنا مصنوعا كذاك قولنا | 0.0417\n",
            "(2) ‫ אליה . ונתבע ד'לכ בשרח | \u001b[1m\u001b[31mإ\u001b[0mليه . ونتبع ذلك بشرح | \u001b[1m\u001b[31mا\u001b[0mليه . ونتبع ذلك بشرح | 0.0476\n",
            "(3) ‫ . וקלת איצ'א לעלהמ ידעונ | . وقلت \u001b[1m\u001b[31mأ\u001b[0mيضا\u001b[1m\u001b[31mً\u001b[0m لعل\u001b[1m\u001b[31mّ\u001b[0mهم يد\u001b[1m\u001b[31mّ\u001b[0mعون | . وقلت \u001b[1m\u001b[31mا\u001b[0mيضا لعلهم يدعون | 0.1538\n",
            "(4) ‫ ואלסמעיה פינבגי אנ אבינ | والسمعي\u001b[1m\u001b[31mة\u001b[0m فينبغي \u001b[1m\u001b[31mأ\u001b[0mن \u001b[1m\u001b[31mأ\u001b[0mبي\u001b[1m\u001b[31mّ\u001b[0mن | والسمعي\u001b[1m\u001b[31mه\u001b[0m فينبغي \u001b[1m\u001b[31mا\u001b[0mن \u001b[1m\u001b[31mا\u001b[0mبين | 0.1667\n",
            "(5) ‫ ראי אלנאס ד'כרה פי פט'נוא | ر\u001b[1m\u001b[31mأى\u001b[0m الناس ذكره في فظن\u001b[1m\u001b[31mّ\u001b[0mوا | ر\u001b[1m\u001b[31mاي\u001b[0m الناس ذكره في فظنوا | 0.1250\n",
            "(6) ‫ לה מראי פקאל אנה לא סבב | له مر\u001b[1m\u001b[31mئ\u001b[0mي فقال \u001b[1m\u001b[31mإ\u001b[0mنه لا سبب | له مر\u001b[1m\u001b[31mا\u001b[0mي فقال \u001b[1m\u001b[31mا\u001b[0mنه لا سبب | 0.0870\n",
            "(7) ‫ דקה' נט'רה פיצל אלי אנ | دق\u001b[1m\u001b[31mّ\u001b[0mة نظره فيصل \u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m \u001b[1m\u001b[31mأ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | دقة نظره فيصل \u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m \u001b[1m\u001b[31mا\u001b[0mن | 0.2273\n",
            "(8) ‫ ואנמא רדדת אלכלאמ פי | و\u001b[1m\u001b[31mإ\u001b[0mنما رددت الكلام في | و\u001b[1m\u001b[31mا\u001b[0mنما رددت الكلام في | 0.0500\n",
            "(9) ‫ H H H H H . וינבגי אנ | H H H H H . وينبغي \u001b[1m\u001b[31mأ\u001b[0mن | H H H H H . وينبغي \u001b[1m\u001b[31mا\u001b[0mن | 0.0476\n",
            "(10) ‫ חכמה ג'הלא מעא , חכמה | حكم\u001b[1m\u001b[31mة\u001b[0m جهلا\u001b[1m\u001b[31mً\u001b[0m معا\u001b[1m\u001b[31mً\u001b[0m , حكم\u001b[1m\u001b[31mة\u001b[0m | حكم\u001b[1m\u001b[31mه\u001b[0m جهلا معا , حكم\u001b[1m\u001b[31mه\u001b[0m | 0.1818\n",
            "(11) ‫ אלעלמ פקט כמא שרחנא פי | العلم فقط كما شرحنا في | العلم فقط كما شرحنا في | 0.0000\n",
            "(12) ‫ . פאקול אולא , אנהמ רפצ'וא | . ف\u001b[1m\u001b[31mأ\u001b[0mقول \u001b[1m\u001b[31mأ\u001b[0mولا\u001b[1m\u001b[31mً\u001b[0m , \u001b[1m\u001b[31mإ\u001b[0mنهم رفضوا | . ف\u001b[1m\u001b[31mا\u001b[0mقول \u001b[1m\u001b[31mا\u001b[0mولا , \u001b[1m\u001b[31mا\u001b[0mنهم رفضوا | 0.1538\n",
            "(13) ‫ הד'ה ג'מל קריבה ינתפע | هذه جمل قريب\u001b[1m\u001b[31mة\u001b[0m ينتفع | هذه جمل قريب\u001b[1m\u001b[31mه\u001b[0m ينتفع | 0.0526\n",
            "(14) ‫ מעטיה אלאת אלחס פכיפ | معطي\u001b[1m\u001b[31mة\u001b[0m \u001b[1m\u001b[31mآ\u001b[0mلات الحس\u001b[1m\u001b[31mّ\u001b[0m فكيف | معطي\u001b[1m\u001b[31mه\u001b[0m \u001b[1m\u001b[31mا\u001b[0mلات الحس فكيف | 0.1429\n",
            "(15) ‫ נאזלא מנ אלהוא הל הו | نازلا\u001b[1m\u001b[31mً\u001b[0m من الهوا\u001b[1m\u001b[31mء\u001b[0m هل هو | نازلا من الهوا هل هو | 0.0909\n",
            "(16) ‫ אלכביר , לאנה אנמא אחיי | الكبير , ل\u001b[1m\u001b[31mأ\u001b[0mنه \u001b[1m\u001b[31mإ\u001b[0mنما \u001b[1m\u001b[31mأ\u001b[0mحي\u001b[1m\u001b[31mى\u001b[0m | الكبير , ل\u001b[1m\u001b[31mا\u001b[0mنه \u001b[1m\u001b[31mا\u001b[0mنما \u001b[1m\u001b[31mا\u001b[0mحي\u001b[1m\u001b[31mي\u001b[0m | 0.1739\n",
            "baseline accuracy:  0.9086851441991064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9086851441991064"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWMVLTywwIW",
        "colab_type": "text"
      },
      "source": [
        "##TEST BASELINE KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrcOFoswu-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "462fec5a-5947-4600-ffbf-0f47f6ca718d"
      },
      "source": [
        "CELL_NAME=\"DEF TEST_BASELINE_KFIR\"\n",
        "\n",
        "import editdistance\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "def test_loss_baseline_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False):\n",
        "  lines=load_lines(data_path) #THIS IS DOUBLED ALLREADY\n",
        "  pairs = create_dataset(lines)  \n",
        "  \n",
        "  if replace_GAIN:\n",
        "    print_log_screen(\"replaceing gimel with jain to match my train convention\")\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs] #ייננבבגג''יי\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "  total_examples=len(pairs)\n",
        "  total_loss=0\n",
        "  sum_of_e_dist=0\n",
        "  num_of_letters=0\n",
        "  for l in arab_lines:\n",
        "    num_of_letters+=len(l)\n",
        "  total_accuracy=0\n",
        "  line_counter=1\n",
        "  for heb_input,real in zip(hebrew_lines,arab_lines):\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)                \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                normalized_accuracy=accuracy/len(real)\n",
        "                total_accuracy+=normalized_accuracy\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                sum_of_e_dist+=accuracy\n",
        "                if SHOW_PRINT:\n",
        "                  print_log(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",str(accuracy))\n",
        "                line_counter+=1\n",
        "\t\t\t\t\t\t\t   \n",
        "  total_accuracy/=total_examples\n",
        "  sum_of_e_dist/num_of_letters\n",
        "  print_log_screen(data_path ,\"accuracy: \",1-total_accuracy)\n",
        "  print_log_screen(data_path ,\"accuracy1: \",1-sum_of_e_dist/num_of_letters)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy,sum_of_e_dist/num_of_letters\n",
        "#test_loss_baseline(limit=3)\n",
        "#test_loss_baseline_kfir(kfir_kuzari_test)\n",
        "test_loss_baseline_kfir(kfir_kuzari_test,True,True)\n",
        "#test_loss_baseline_kfir(kfir_rasag_test)\n",
        "test_loss_baseline_kfir(kfir_rasag_test,True,True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replaceing gimel with jain to match my train convention\n",
            "/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test.txt accuracy:  0.8769578643578643\n",
            "/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test.txt accuracy1:  0.8774193548387097\n",
            "replaceing gimel with jain to match my train convention\n",
            "/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test.txt accuracy:  0.845952380952381\n",
            "/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test.txt accuracy1:  0.8489583333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0.15404761904761904, 0.15104166666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74",
        "colab_type": "text"
      },
      "source": [
        "##test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF general TEST_LOSS\"\n",
        "\n",
        "import editdistance\n",
        "def test_loss(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=print_by_idx_CTC(dense[i],targ_lang.idx2char).strip()\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"LER (label error rate): \",total_accuracy)\n",
        "#  print_log(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "#test_loss(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f",
        "colab_type": "text"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DGjAhEUfxbRc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "da0ffa0c-1d13-4b38-f61b-04fd15599b91"
      },
      "source": [
        "CELL_NAME=\"GUIDE TEXT\"\n",
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "###TODO : change hebrew insertion to \"H\"\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guid_text='''כנת איהא אלתלמיד' אלעזיז עברית-ר' עברית-יוסף עברית-ש\"צ עברית-ב\"ר \n",
        "עברית-יהודה עברית-נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "import re\n",
        "guid_text=re.sub(r'עברית-[^\\s]+', 'H',guid_text)\n",
        "print(guid_text)\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר עברית-פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "כנת איהא אלתלמיד' אלעזיז H H H H \n",
            "H H למא מת'לת ענדי וקצדת\n",
            " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
            " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
            "אלאסכנדריה קבל אן אמתחן\n",
            "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
            "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
            "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
            "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
            "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
            "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
            "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
            "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF TEST_GUIDE\"\n",
        "# guid_lines=guid_text.split('\\n')\n",
        "\n",
        "# def test_guide(limit=1000000,num_of_paths=1):\n",
        "#   line_res=[]\n",
        "#   count=0\n",
        "#   for l in guid_lines:    \n",
        "#     if count>limit:\n",
        "#       break\n",
        "#     l = l.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ').replace('\"',\"\").replace('ֿ',\"'\")\n",
        "#     l_res=test__CTC_word(l,num_of_paths=num_of_paths)\n",
        "#     line_res.append(l_res)\n",
        "#     count+=1\n",
        "#   print_log('\\n'.join(line_res))\n",
        "#   return '\\n'.join(line_res)\n",
        "\n",
        "# print_log(test_guide(limit=1))\n",
        "# #print_log(test_guide())\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return test__CTC_word_multiline(guid_text,num_of_paths,BATCH_SIZE)\n",
        "print_log(test_guide())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9",
        "colab_type": "text"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF TEST_SHUFFLE\"\n",
        "# def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "#   return test_loss(this_dataset=data,limit=limit)\n",
        "# #shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE",
        "colab_type": "text"
      },
      "source": [
        "##CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEFINE CHECKPOINT\"\n",
        "GLOBAL_epoch=-1 \n",
        "\n",
        "checkpoint_path='/gdrive/My Drive/checkpoints/'+this_time+\"/ckpt\"\n",
        "def save_checkpoint(massage):\n",
        "  print_log_screen(\"saving checkpoing at \"+checkpoint_path)\n",
        "  model.save_weights(checkpoint_path)\n",
        "  f_check= open(checkpoint_path+\".txt\",\"a+\")  \n",
        "  f_check.write(\"saving chekcpoing at epoch\"+ str(GLOBAL_epoch) +'\\n')\n",
        "  f_check.write(massage+'\\n')\n",
        "  f_check.close()\n",
        "  \n",
        "def load_checkpoint():\n",
        "  print_log_screen(\"loading checkpoing from \"+checkpoint_path)\n",
        "  model1=rebuild()\n",
        "  model1.load_weights(checkpoint_path)\n",
        "  return model1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ",
        "colab_type": "text"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF pretrain_letters\"\n",
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)       \n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test__CTC_letters()\n",
        "    print_log_screen(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TRAIN SINGLE LETTERS AND TEST LETTERS\"\n",
        "# model=rebuild()\n",
        "# test__CTC_letters()\n",
        "# optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "# pretrain_letters(10,BATCH_SIZE)\n",
        "# test__CTC_letters()\n",
        "# save_checkpoint(\"testing1\")\n",
        "# model=rebuild()\n",
        "# test__CTC_letters()\n",
        "# model=load_checkpoint()\n",
        "# test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du",
        "colab_type": "text"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF main TRAIN_LOOP\"\n",
        "GLOBAL_epoch=0\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train_loop(cur_dataset=dataset_double,stop_loop=10000000000):\n",
        "  # Training step  \n",
        "  global GLOBAL_epoch\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initially hidden is None\n",
        "  hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?  \n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print_log_screen(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA",
        "colab_type": "text"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c3b9b33-63bc-4df5-e98c-75c395eec593"
      },
      "source": [
        "CELL_NAME=\"MAIN\"\n",
        "\n",
        "GLOBAL_epoch=0\n",
        "\n",
        "BEST_ACCURACY=1\n",
        "f= open(\"my_log.txt\",\"w+\")\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "#tf.random.set_seed(1)\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "mail_subject=\"pretrain letters. synt DROPOUT 0.85. STANDARD TANWIN\"\n",
        "mail_subject=this_time+\":\"+mail_subject\n",
        "\n",
        "pretrain_letter=15\n",
        "synth=True\n",
        "keep_percent=0.85\n",
        "\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print_log_screen(description)\n",
        "\n",
        "#LOG Stats\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "\n",
        "model=rebuild()\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "if pretrain_letter>0:\n",
        "  #optimizer = tf.train.AdamOptimizer() #adam is a bit better for this #didn't work\n",
        "  print_log_screen(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print_log_screen('-'*200)\n",
        "\n",
        "print_log_screen(\"START TRAIN\")\n",
        "#optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "\n",
        "#dataset_double_synt=gen_dropout(keep_percent)\n",
        "for jjj in range(10): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(1): #iter without sendmail and only partial test\n",
        "    log_flush()\n",
        "    if synth:\n",
        "      dataset_double_synt=gen_dropout_all(keep_percent)\n",
        "      loss=train_loop(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train_loop(dataset_double)\n",
        "    #total_test_loss,total_accuracy=test_loss(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    #total_test_loss,total_accuracy=test_loss(limit=5)\n",
        "    #test_loss(this_dataset=test_dataset_double1,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "\n",
        "\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print_log_screen('-'*200)\n",
        "    test_kfir(kfir_kuzari_test,True)\n",
        "    #test_kfir(kfir_kuzari_test)\n",
        "    test_kfir(kfir_rasag_test,True)\n",
        "    #test_kfir(kfir_rasag_test)\n",
        "    print_log_screen('-'*200)\n",
        "  print_log('FULL STATISTICS')\n",
        "  print_log('='*200)\n",
        "  test_kfir(kfir_kuzari_test,True,True)\n",
        "  #test_kfir(kfir_kuzari_test)\n",
        "  test_kfir(kfir_rasag_test,True,True)\n",
        "  #test_kfir(kfir_rasag_test)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test_loss()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test_loss(test_dataset_double1)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  if all_accuracy<BEST_ACCURACY:\n",
        "    save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "    BEST_ACCURACY=all_accuracy\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print_log(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print_log(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print_log(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  f.write(\"EPOCH \"+str(global_EPOCH)+'\\n')\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  f.flush()\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "f.close()\n",
        "close_log()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "pretrain: 15\n",
            "with synth data\n",
            "dropout:0.85\n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (128, None, 8)            384       \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (128, None, 2048)         6352896   \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_14 (Bidirectio (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "bidirectional_15 (Bidirectio (128, None, 2048)         18886656  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (128, None, 65)           133185    \n",
            "=================================================================\n",
            "Total params: 63,146,433\n",
            "Trainable params: 63,146,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "PRETRAIN\n",
            "Epoch 1 Loss 125.2195\n",
            "Epoch 2 Loss 125.1581\n",
            "Epoch 3 Loss 124.8970\n",
            "Epoch 4 Loss 123.9645\n",
            "Epoch 5 Loss 122.8982\n",
            "Epoch 6 Loss 122.7627\n",
            "Epoch 7 Loss 111.5898\n",
            "Epoch 8 Loss 45.2505\n",
            "Epoch 9 Loss 15.5479\n",
            "Epoch 10 Loss 19.4222\n",
            "Epoch 11 Loss 12.5106\n",
            "Epoch 12 Loss 16.0244\n",
            "Epoch 13 Loss 11.5549\n",
            "Epoch 14 Loss 11.4707\n",
            "Epoch 15 Loss 15.1505\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "START TRAIN\n",
            "Epoch 1 Batch 0 Loss 55.0014\n",
            "Epoch 1 Batch 10 Loss 24.2125\n",
            "Epoch 1 Batch 20 Loss 16.9317\n",
            "Epoch 1 Batch 30 Loss 13.3107\n",
            "Epoch 1 Batch 40 Loss 13.0362\n",
            "Epoch 1 Batch 50 Loss 13.8050\n",
            "Epoch 1 Batch 60 Loss 11.8025\n",
            "Epoch 1 Batch 70 Loss 9.1218\n",
            "Epoch 1 Batch 80 Loss 13.7799\n",
            "Epoch 1 Batch 90 Loss 9.5091\n",
            "Epoch 1 Batch 100 Loss 18.0498\n",
            "Epoch 1 Batch 110 Loss 6.7434\n",
            "Epoch 1 Batch 120 Loss 9.6008\n",
            "Epoch 1 Batch 130 Loss 8.7733\n",
            "Epoch 1 Batch 140 Loss 7.0855\n",
            "Epoch 1 Batch 150 Loss 9.6430\n",
            "Epoch 1 Batch 160 Loss 7.2793\n",
            "Epoch 1 Batch 170 Loss 7.9986\n",
            "Epoch 1 Batch 180 Loss 7.2570\n",
            "Epoch 1 Batch 190 Loss 8.3470\n",
            "Epoch 1 Batch 200 Loss 7.9123\n",
            "Epoch 1 Batch 210 Loss 5.7017\n",
            "Epoch 1 Batch 220 Loss 7.8605\n",
            "Epoch 1 Batch 230 Loss 4.9654\n",
            "Epoch 1 Batch 240 Loss 5.5751\n",
            "Epoch 1 Batch 250 Loss 7.8376\n",
            "Epoch 1 Batch 260 Loss 5.6827\n",
            "Epoch 1 Batch 270 Loss 5.9227\n",
            "Epoch 1 Batch 280 Loss 7.0804\n",
            "Epoch 1 Batch 290 Loss 5.8747\n",
            "Epoch 1 Batch 300 Loss 6.2242\n",
            "Epoch 1 Batch 310 Loss 5.0365\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "#examples: 500 , accuracy: 0.8992110389610389\n",
            "#letters: 2325 , accuracy1: 0.9006451612903226\n",
            "#examples: 50 , accuracy: 0.9261428571428572\n",
            "#letters: 192 , accuracy1: 0.9114583333333334\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "(1) ‫ לא | لا | لا | 0.0000\n",
            "(2) ‫ תכ'אפ | تخاف | تخاف | 0.0000\n",
            "(3) ‫ אלפנא | الفناء | الفناء | 0.0000\n",
            "(4) ‫ אבדא | \u001b[1m\u001b[31mإ\u001b[0mبدا\u001b[1m\u001b[31mً\u001b[0m | \u001b[1m\u001b[31mا\u001b[0mبدا | 2.0000\n",
            "(5) ‫ פתציר | فتصير | فتصير | 0.0000\n",
            "(6) ‫ נפס | نفس | نفس | 0.0000\n",
            "(7) ‫ אלאנסאנ | الإنسان | الإنسان | 0.0000\n",
            "(8) ‫ אלכאמל | الكامل | الكامل | 0.0000\n",
            "(9) ‫ וד'לכ | وذلك | وذلك | 0.0000\n",
            "(10) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(11) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا\u001b[1m\u001b[31mء\u001b[0m | 2.0000\n",
            "(12) ‫ ואחדא | واحداً | واحداً | 0.0000\n",
            "(13) ‫ פלא | فلا | فلا | 0.0000\n",
            "(14) ‫ יבאלי | ي\u001b[1m\u001b[31mخ\u001b[0mل\u001b[1m\u001b[31mى\u001b[0m | ي\u001b[1m\u001b[31mبإ\u001b[0mل\u001b[1m\u001b[31mي\u001b[0m | 3.0000\n",
            "(15) ‫ בפנא | بفنا\u001b[1m\u001b[31mء\u001b[0m | بفنا | 1.0000\n",
            "(16) ‫ ג'סדה | جسد\u001b[1m\u001b[31mه\u001b[0m | جسد\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(17) ‫ ואלאתה | وآلاته | وآلاته | 0.0000\n",
            "(18) ‫ אד' | \u001b[1m\u001b[31mإ\u001b[0mذ | \u001b[1m\u001b[31mا\u001b[0mذ | 1.0000\n",
            "(19) ‫ קד | قد | قد | 0.0000\n",
            "(20) ‫ צאר | صار | صار | 0.0000\n",
            "(21) ‫ וד'לכ | وذلك | وذلك | 0.0000\n",
            "(22) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا\u001b[1m\u001b[31mء\u001b[0m | 2.0000\n",
            "(23) ‫ ואחדא | واحداً | واحداً | 0.0000\n",
            "(24) ‫ וטאבת | وطابت | وطابت | 0.0000\n",
            "(25) ‫ נפסה | نفس\u001b[1m\u001b[31mه\u001b[0m | نفس\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(26) ‫ פי | في | في | 0.0000\n",
            "(27) ‫ אלחיאה | الحياة | الحياة | 0.0000\n",
            "(28) ‫ אד | \u001b[1m\u001b[31mإذ\u001b[0m | \u001b[1m\u001b[31mاد\u001b[0m | 2.0000\n",
            "(29) ‫ צאר | صار | صار | 0.0000\n",
            "(30) ‫ פי | في | في | 0.0000\n",
            "(31) ‫ זמרה | زمرة | زمرة | 0.0000\n",
            "(32) ‫ הרמס | هرمس | هرمس | 0.0000\n",
            "(33) ‫ ואסקלאביוס | و\u001b[1m\u001b[31mإ\u001b[0mسقلابيوس | و\u001b[1m\u001b[31mا\u001b[0mسقلابيوس | 1.0000\n",
            "(34) ‫ וסקראט | وسقراط | وسقراط | 0.0000\n",
            "(35) ‫ ואפלאטונ | و\u001b[1m\u001b[31mإ\u001b[0mفلاطون | و\u001b[1m\u001b[31mأ\u001b[0mفلاطون | 1.0000\n",
            "(36) ‫ וארסטוטאליס | و\u001b[1m\u001b[31mإ\u001b[0mرسطوطاليس | و\u001b[1m\u001b[31mا\u001b[0mرسطوطاليس | 1.0000\n",
            "(37) ‫ בל | بل | بل | 0.0000\n",
            "(38) ‫ הו | هو | هو | 0.0000\n",
            "(39) ‫ והמ | وهم | وهم | 0.0000\n",
            "(40) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1.0000\n",
            "(41) ‫ מנ | من | من | 0.0000\n",
            "(42) ‫ כאנ | كان | كان | 0.0000\n",
            "(43) ‫ פי | في | في | 0.0000\n",
            "(44) ‫ דרג'תהמ | درجتهم | درجتهم | 0.0000\n",
            "(45) ‫ ואלעקל | والعقل | والعقل | 0.0000\n",
            "(46) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1.0000\n",
            "(47) ‫ שי | شيء | شيء | 0.0000\n",
            "(48) ‫ ואחד | واحد | واحد | 0.0000\n",
            "(49) ‫ פהד'א | فهذا | فهذا | 0.0000\n",
            "(50) ‫ אלד'י | الذي | الذي | 0.0000\n",
            "(51) ‫ יכני | يكن\u001b[1m\u001b[31mّى\u001b[0m | يكن\u001b[1m\u001b[31mي\u001b[0m | 2.0000\n",
            "(52) ‫ ענה | عنه | عنه | 0.0000\n",
            "(53) ‫ ברצ'א | برضا | برضا\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(54) ‫ אללה | الله | الله | 0.0000\n",
            "(55) ‫ עלי | على | على | 0.0000\n",
            "(56) ‫ סביל | سبيل | سبيل | 0.0000\n",
            "(57) ‫ אללגז | اللغز | اللغز | 0.0000\n",
            "(58) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mأ\u001b[0mو | 1.0000\n",
            "(59) ‫ אלתקריב | التقريب | التقريب | 0.0000\n",
            "(60) ‫ פאתבעה | فات\u001b[1m\u001b[31mّ\u001b[0mبع\u001b[1m\u001b[31mه\u001b[0m | فاتبع\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(61) ‫ ואתבע | وات\u001b[1m\u001b[31mّ\u001b[0mبع | واتبع | 1.0000\n",
            "(62) ‫ אלעלמ | العلم | العلم | 0.0000\n",
            "(63) ‫ בחקאיק | بحقا\u001b[1m\u001b[31mئ\u001b[0mق | بحقا\u001b[1m\u001b[31mي\u001b[0mق | 1.0000\n",
            "(64) ‫ אלאמור | ال\u001b[1m\u001b[31mإ\u001b[0mمور | ال\u001b[1m\u001b[31mأ\u001b[0mمور | 1.0000\n",
            "(65) ‫ ליציר | ليصير | ليصير | 0.0000\n",
            "(66) ‫ עקלכ | عقلك | عقلك | 0.0000\n",
            "(67) ‫ פעלא | ف\u001b[1m\u001b[31mا\u001b[0mعلاً | فعلاً | 1.0000\n",
            "(68) ‫ לא | لا | لا | 0.0000\n",
            "(69) ‫ מנפעלא | منفعلاً | منفعلاً | 0.0000\n",
            "(70) ‫ ואלזמ | والزم | والزم | 0.0000\n",
            "(71) ‫ אעדל | \u001b[1m\u001b[31mإ\u001b[0mعدل | \u001b[1m\u001b[31mأ\u001b[0mعدل | 1.0000\n",
            "(72) ‫ אלטרק | الطرق | الطرق | 0.0000\n",
            "(73) ‫ פי | في | في | 0.0000\n",
            "(74) ‫ אלאכ'לאק | ال\u001b[1m\u001b[31mإ\u001b[0mخلاق | ال\u001b[1m\u001b[31mا\u001b[0mخلاق | 1.0000\n",
            "(75) ‫ ואלאעמאל | وال\u001b[1m\u001b[31mإ\u001b[0mعمال | وال\u001b[1m\u001b[31mأ\u001b[0mعمال | 1.0000\n",
            "(76) ‫ לאנה | ل\u001b[1m\u001b[31mإ\u001b[0mنّه | ل\u001b[1m\u001b[31mأ\u001b[0mنّه | 1.0000\n",
            "(77) ‫ מעונה | معونة | معونة | 0.0000\n",
            "(78) ‫ פי | في | في | 0.0000\n",
            "(79) ‫ תצור | تصو\u001b[1m\u001b[31mّ\u001b[0mر | تصور | 1.0000\n",
            "(80) ‫ אלחק | الحق\u001b[1m\u001b[31mّ\u001b[0m | الحق | 1.0000\n",
            "(81) ‫ ולזומ | ولزوم | ولزوم | 0.0000\n",
            "(82) ‫ אלתעלמ | التعل\u001b[1m\u001b[31mّ\u001b[0mم | التعلم | 1.0000\n",
            "(83) ‫ ואלתשבה | والتشب\u001b[1m\u001b[31mّه\u001b[0m | والتشب\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(84) ‫ בד'לכ | بذلك | بذلك | 0.0000\n",
            "(85) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(86) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1.0000\n",
            "(87) ‫ ויתבע | ويتبع | ويتبع | 0.0000\n",
            "(88) ‫ הד'א | هذا | هذا | 0.0000\n",
            "(89) ‫ אלקנוע | القنوع | القنوع | 0.0000\n",
            "(90) ‫ ואלכ'צ'וע | والخضوع | والخضوع | 0.0000\n",
            "(91) ‫ ואלכ'שוע | والخشوع | والخشوع | 0.0000\n",
            "(92) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1.0000\n",
            "(93) ‫ כ'לק | خلق | خلق | 0.0000\n",
            "(94) ‫ פאצ'ל | فاضل | فاضل | 0.0000\n",
            "(95) ‫ מע | مع | مع | 0.0000\n",
            "(96) ‫ אלתעט'ימ | التعظيم | التعظيم | 0.0000\n",
            "(97) ‫ ללסבב | للسبب | للسبب | 0.0000\n",
            "(98) ‫ אלאול | ال\u001b[1m\u001b[31mإ\u001b[0mو\u001b[1m\u001b[31mّ\u001b[0mل | ال\u001b[1m\u001b[31mأ\u001b[0mول | 2.0000\n",
            "(99) ‫ לא | لا | لا | 0.0000\n",
            "(100) ‫ ליהבכ | ليهبك | ليهبك | 0.0000\n",
            "(101) ‫ רצ'אה | رضا\u001b[1m\u001b[31mه\u001b[0m | رضا\u001b[1m\u001b[31mًة\u001b[0m | 2.0000\n",
            "(102) ‫ ולא | ولا | ولا | 0.0000\n",
            "(103) ‫ ליזיל | ليزيل | ليزيل | 0.0000\n",
            "(104) ‫ ענכ | عنك | عنك | 0.0000\n",
            "(105) ‫ סכ'טה | سخط\u001b[1m\u001b[31mه\u001b[0m | سخط\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(106) ‫ בל | بل | بل | 0.0000\n",
            "(107) ‫ ללתשבה | للتشبّ\u001b[1m\u001b[31mه\u001b[0m | للتشبّ\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(108) ‫ ללעקל | للعقل | للعقل | 0.0000\n",
            "(109) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1.0000\n",
            "(110) ‫ פי | في | في | 0.0000\n",
            "(111) ‫ אית'אר | إيثار | إيثار | 0.0000\n",
            "(112) ‫ אלחק | الحق\u001b[1m\u001b[31mّ\u001b[0m | الحق | 1.0000\n",
            "(113) ‫ ווצפ | ووصف | ووصف | 0.0000\n",
            "(114) ‫ כל | كل\u001b[1m\u001b[31mّ\u001b[0m | كل | 1.0000\n",
            "(115) ‫ שי | شيء | شيء | 0.0000\n",
            "(116) ‫ במא | بما | بما | 0.0000\n",
            "(117) ‫ יג'ב | يجب | يجب | 0.0000\n",
            "(118) ‫ לה | له | له | 0.0000\n",
            "(119) ‫ ואעתקאדה | واعتقاد\u001b[1m\u001b[31mه\u001b[0m | واعتقاد\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(120) ‫ עלי | على | على | 0.0000\n",
            "(121) ‫ מא | ما | ما | 0.0000\n",
            "(122) ‫ הו | هو | هو | 0.0000\n",
            "(123) ‫ עליה | عليه | عليه | 0.0000\n",
            "(124) ‫ פהד'א | فهذا | فهذا | 0.0000\n",
            "(125) ‫ מנ | من | من | 0.0000\n",
            "(126) ‫ צפאת | صفات | صفات | 0.0000\n",
            "(127) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(128) ‫ פאד' | فإذ | فإذ | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  10.892099567099567 128\n",
            "BATCH (sum_of_e_d,num_of_letters):  51 574\n",
            "(1) ‫ צרת | صرت | صرت | 0.0000\n",
            "(2) ‫ בהד'ה | بهذه | بهذه | 0.0000\n",
            "(3) ‫ אלצפה | الصفة | الصفة | 0.0000\n",
            "(4) ‫ מנ | من | من | 0.0000\n",
            "(5) ‫ אלאעתקאד | الاعتقاد | الاعتقاد | 0.0000\n",
            "(6) ‫ לא | لا | لا | 0.0000\n",
            "(7) ‫ תבאלי | تبالي | تبالي | 0.0000\n",
            "(8) ‫ באי | ب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | ب\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(9) ‫ שרע | شرع | شرع | 0.0000\n",
            "(10) ‫ תשרעת | تشر\u001b[1m\u001b[31mّ\u001b[0mعت | تشرعت | 1.0000\n",
            "(11) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mأ\u001b[0mو | 1.0000\n",
            "(12) ‫ תדינת | تدي\u001b[1m\u001b[31mّ\u001b[0mنت | تدينت | 1.0000\n",
            "(13) ‫ ועט'מת | وعظ\u001b[1m\u001b[31mّ\u001b[0mمت | وعظمت | 1.0000\n",
            "(14) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(15) ‫ קול | قول | قول | 0.0000\n",
            "(16) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(17) ‫ לסאנ | لسان | لسان | 0.0000\n",
            "(18) ‫ ובאי | وب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | وب\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(19) ‫ אעמאל | \u001b[1m\u001b[31mإ\u001b[0mعمال | \u001b[1m\u001b[31mأ\u001b[0mعمال | 1.0000\n",
            "(20) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mأ\u001b[0mو | 1.0000\n",
            "(21) ‫ אכ'תרע | اخترع | اخترع | 0.0000\n",
            "(22) ‫ לנפסכ | لنفسك | لنفسك | 0.0000\n",
            "(23) ‫ דינא | دينا\u001b[1m\u001b[31mً\u001b[0m | دينا | 1.0000\n",
            "(24) ‫ למעני | لمعنى | لمعنى | 0.0000\n",
            "(25) ‫ אלתכ'שע | التخش\u001b[1m\u001b[31mّ\u001b[0mع | التخشع | 1.0000\n",
            "(26) ‫ ואלתעט'ימ | والتعظيم | والتعظيم | 0.0000\n",
            "(27) ‫ ואלתסביח | والتسبيح | والتسبيح | 0.0000\n",
            "(28) ‫ ולתדביר | وتدبير | و\u001b[1m\u001b[31mل\u001b[0mتدبير | 1.0000\n",
            "(29) ‫ אכ'לאקכ | \u001b[1m\u001b[31mإ\u001b[0mخلاقك | \u001b[1m\u001b[31mأ\u001b[0mخلاقك | 1.0000\n",
            "(30) ‫ ותדביר | وتدبير | وتدبير | 0.0000\n",
            "(31) ‫ מנזלכ | منزلك | منزلك | 0.0000\n",
            "(32) ‫ ומדינתכ | ومدينتك | ومدينتك | 0.0000\n",
            "(33) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن | \u001b[1m\u001b[31mا\u001b[0mن | 1.0000\n",
            "(34) ‫ כנת | كنت | كنت | 0.0000\n",
            "(35) ‫ מקבולא | مقبولا\u001b[1m\u001b[31mً\u001b[0m | مقبولا | 1.0000\n",
            "(36) ‫ מנהמ | منهم | منهم | 0.0000\n",
            "(37) ‫ או | \u001b[1m\u001b[31mإ\u001b[0mو | \u001b[1m\u001b[31mأ\u001b[0mو | 1.0000\n",
            "(38) ‫ תדינ | تدي\u001b[1m\u001b[31mّ\u001b[0mن | تدين | 1.0000\n",
            "(39) ‫ באלנואמיס | بالنواميس | بالنواميس | 0.0000\n",
            "(40) ‫ אלעקליה | العقلية | العقلية | 0.0000\n",
            "(41) ‫ אלמולפה | المؤل\u001b[1m\u001b[31mّ\u001b[0mفة | المؤلفة | 1.0000\n",
            "(42) ‫ ללפלאספה | للفلاسفة | للفلاسفة | 0.0000\n",
            "(43) ‫ ואג'על | و\u001b[1m\u001b[31mا\u001b[0mجعل | و\u001b[1m\u001b[31mأ\u001b[0mجعل | 1.0000\n",
            "(44) ‫ קצדכ | قصدك | قصدك | 0.0000\n",
            "(45) ‫ וגרצ'כ | وغرضك | وغرضك | 0.0000\n",
            "(46) ‫ צפא | صفا\u001b[1m\u001b[31mء\u001b[0m | صفا\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(47) ‫ נפסכ | نفسك | نفسك | 0.0000\n",
            "(48) ‫ ובאלג'מלה | وبالجملة | وبالجملة | 0.0000\n",
            "(49) ‫ פאטלב | فاطلب | فاطلب | 0.0000\n",
            "(50) ‫ צפא | صفاء | صفاء | 0.0000\n",
            "(51) ‫ אלקלב | القلب | القلب | 0.0000\n",
            "(52) ‫ באי | ب\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | ب\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(53) ‫ וג'ה | وجه | وجه | 0.0000\n",
            "(54) ‫ אמכנכ | \u001b[1m\u001b[31mإ\u001b[0mمكنك | \u001b[1m\u001b[31mأ\u001b[0mمكنك | 1.0000\n",
            "(55) ‫ בעד | بعد | بعد | 0.0000\n",
            "(56) ‫ תחציל | تحصيل | تحصيل | 0.0000\n",
            "(57) ‫ כליאת | كل\u001b[1m\u001b[31mّ\u001b[0mيات | كليات | 1.0000\n",
            "(58) ‫ אלעלומ | العلوم | العلوم | 0.0000\n",
            "(59) ‫ עלי | على | على | 0.0000\n",
            "(60) ‫ חקאיקהא | حقا\u001b[1m\u001b[31mئ\u001b[0mق\u001b[1m\u001b[31mه\u001b[0mا | حقا\u001b[1m\u001b[31mي\u001b[0mق\u001b[1m\u001b[31mة\u001b[0mا | 2.0000\n",
            "(61) ‫ פתצאדפ | فتصادف | فتصادف | 0.0000\n",
            "(62) ‫ מטלובכ | مطلبك | مطل\u001b[1m\u001b[31mو\u001b[0mبك | 1.0000\n",
            "(63) ‫ אעני | \u001b[1m\u001b[31mإ\u001b[0mعني | \u001b[1m\u001b[31mأ\u001b[0mعني | 1.0000\n",
            "(64) ‫ אלאתצאל | ال\u001b[1m\u001b[31mا\u001b[0mتّصال | ال\u001b[1m\u001b[31mإ\u001b[0mتّصال | 1.0000\n",
            "(65) ‫ בד'לכ | بذلك | بذلك | 0.0000\n",
            "(66) ‫ אלרוחאני | الروحاني\u001b[1m\u001b[31mّ\u001b[0m | الروحاني | 1.0000\n",
            "(67) ‫ אעני | \u001b[1m\u001b[31mإ\u001b[0mعني | \u001b[1m\u001b[31mأ\u001b[0mعني | 1.0000\n",
            "(68) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(69) ‫ אלפעאל | الفع\u001b[1m\u001b[31mّ\u001b[0mال | الفعال | 1.0000\n",
            "(70) ‫ ורבמא | ورب\u001b[1m\u001b[31mّ\u001b[0mما | وربما | 1.0000\n",
            "(71) ‫ אנבאכ | \u001b[1m\u001b[31mإ\u001b[0mنب\u001b[1m\u001b[31mإ\u001b[0mك | \u001b[1m\u001b[31mأ\u001b[0mنب\u001b[1m\u001b[31mا\u001b[0mك | 2.0000\n",
            "(72) ‫ ואמרכ | و\u001b[1m\u001b[31mإ\u001b[0mمرك | و\u001b[1m\u001b[31mأ\u001b[0mمرك | 1.0000\n",
            "(73) ‫ בעלמ | بعلم | بعلم | 0.0000\n",
            "(74) ‫ גיב | غيب | غيب | 0.0000\n",
            "(75) ‫ מנ | من | من | 0.0000\n",
            "(76) ‫ מנאמאת | منامات | منامات | 0.0000\n",
            "(77) ‫ צאדקה | صادقة | صادقة | 0.0000\n",
            "(78) ‫ וכ'יאלאת | وخيالات | وخيالات | 0.0000\n",
            "(79) ‫ מציבה | مصيبة | مصيبة | 0.0000\n",
            "(80) ‫ קאל | قال | قال | 0.0000\n",
            "(81) ‫ לה | له | له | 0.0000\n",
            "(82) ‫ אלכ'זרי | الخزريّ | الخزريّ | 0.0000\n",
            "(83) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(84) ‫ כלאמכ | كلامك | كلامك | 0.0000\n",
            "(85) ‫ למקנע | لمقنع | لمقنع | 0.0000\n",
            "(86) ‫ לכנה | لكن\u001b[1m\u001b[31mّه\u001b[0m | لكن\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(87) ‫ גיר | غير | غير | 0.0000\n",
            "(88) ‫ מטאבק | مطابق | مطابق | 0.0000\n",
            "(89) ‫ לטלבתי | لطلبتي | لطلبتي | 0.0000\n",
            "(90) ‫ לאני | ل\u001b[1m\u001b[31mإ\u001b[0mني\u001b[1m\u001b[31mّ\u001b[0m | ل\u001b[1m\u001b[31mأ\u001b[0mني | 2.0000\n",
            "(91) ‫ אעלמ | \u001b[1m\u001b[31mإ\u001b[0mعلم | \u001b[1m\u001b[31mأ\u001b[0mعلم | 1.0000\n",
            "(92) ‫ מנ | من | من | 0.0000\n",
            "(93) ‫ נפסי | نفسي | نفسي | 0.0000\n",
            "(94) ‫ אני | \u001b[1m\u001b[31mإ\u001b[0mنّي | \u001b[1m\u001b[31mأ\u001b[0mنّي | 1.0000\n",
            "(95) ‫ צאפי | صافي | صافي | 0.0000\n",
            "(96) ‫ אלנפס | النفس | النفس | 0.0000\n",
            "(97) ‫ מסדד | مسد\u001b[1m\u001b[31mّ\u001b[0mد | مسدد | 1.0000\n",
            "(98) ‫ אלאעמאל | ال\u001b[1m\u001b[31mإ\u001b[0mعمال | ال\u001b[1m\u001b[31mأ\u001b[0mعمال | 1.0000\n",
            "(99) ‫ נחו | نحو | نحو | 0.0000\n",
            "(100) ‫ רצ'א | رضا | رضا\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(101) ‫ אלרב | الرب\u001b[1m\u001b[31mّ\u001b[0m | الرب | 1.0000\n",
            "(102) ‫ לכנ | لكن | لكن | 0.0000\n",
            "(103) ‫ כאנ | كان | كان | 0.0000\n",
            "(104) ‫ ג'ואבי | جوابي | جوابي | 0.0000\n",
            "(105) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(106) ‫ הד'א | هذا | هذا | 0.0000\n",
            "(107) ‫ אלעמל | العمل | العمل | 0.0000\n",
            "(108) ‫ ליס | ليس | ليس | 0.0000\n",
            "(109) ‫ במרצ'י | بمرضي\u001b[1m\u001b[31mّ\u001b[0m | بمرضي | 1.0000\n",
            "(110) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن | و\u001b[1m\u001b[31mأ\u001b[0mن | 1.0000\n",
            "(111) ‫ כאנת | كانت | كانت | 0.0000\n",
            "(112) ‫ אלניה | الني\u001b[1m\u001b[31mّ\u001b[0mة | النية | 1.0000\n",
            "(113) ‫ מרצ'יה | مرضي\u001b[1m\u001b[31mّ\u001b[0mة | مرضية | 1.0000\n",
            "(114) ‫ פלא | فلا | فلا | 0.0000\n",
            "(115) ‫ שכ | شك\u001b[1m\u001b[31mّ\u001b[0m | شك | 1.0000\n",
            "(116) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(117) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1.0000\n",
            "(118) ‫ עמלא | عملاً | عملاً | 0.0000\n",
            "(119) ‫ מא | ما | ما | 0.0000\n",
            "(120) ‫ מרצ'יא | مرضياً | مرضياً | 0.0000\n",
            "(121) ‫ בד'אתה | بذاته | بذاته | 0.0000\n",
            "(122) ‫ לא | لا | لا | 0.0000\n",
            "(123) ‫ בחסב | بحسب | بحسب | 0.0000\n",
            "(124) ‫ אלט'נונ | الظنون | الظنون | 0.0000\n",
            "(125) ‫ ואלא | وإلا\u001b[1m\u001b[31mّ\u001b[0m | وإلا | 1.0000\n",
            "(126) ‫ פאנ | فإن\u001b[1m\u001b[31mّ\u001b[0m | فإن | 1.0000\n",
            "(127) ‫ אלנצראני | النصراني\u001b[1m\u001b[31mّ\u001b[0m | النصراني | 1.0000\n",
            "(128) ‫ ואלמסלמ | والمسلم | والمسلم | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  15.229365079365074 128\n",
            "BATCH (sum_of_e_d,num_of_letters):  66 611\n",
            "(1) ‫ אללד'ינ | اللذين | اللذين | 0.0000\n",
            "(2) ‫ אקתסמא | اقتسما | اقتسما\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(3) ‫ אלמעמורה | المعمورة | المعمورة | 0.0000\n",
            "(4) ‫ יתקאתלאנ | يتقاتلان | يتقاتلان | 0.0000\n",
            "(5) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1.0000\n",
            "(6) ‫ ואחד | واحد | واحد | 0.0000\n",
            "(7) ‫ מנהמא | منهما | منهما | 0.0000\n",
            "(8) ‫ קד | قد | قد | 0.0000\n",
            "(9) ‫ אצפי | \u001b[1m\u001b[31mإ\u001b[0mصف\u001b[1m\u001b[31mى\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mصف\u001b[1m\u001b[31mي\u001b[0m | 2.0000\n",
            "(10) ‫ ניתה | ني\u001b[1m\u001b[31mّ\u001b[0mته | نيته | 1.0000\n",
            "(11) ‫ ללה | لله | لله | 0.0000\n",
            "(12) ‫ ותרהב | وتره\u001b[1m\u001b[31mّ\u001b[0mب | وترهب | 1.0000\n",
            "(13) ‫ ותזהד | وتزه\u001b[1m\u001b[31mّ\u001b[0mد | وتزهد | 1.0000\n",
            "(14) ‫ וצאמ | وصام | وصام | 0.0000\n",
            "(15) ‫ וצלי | وصل\u001b[1m\u001b[31mّى\u001b[0m | وصل\u001b[1m\u001b[31mي\u001b[0m | 2.0000\n",
            "(16) ‫ ומצ'י | ومض\u001b[1m\u001b[31mى\u001b[0m | ومض\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(17) ‫ מצממא | مصم\u001b[1m\u001b[31mّ\u001b[0mما\u001b[1m\u001b[31mً\u001b[0m | مصمما | 2.0000\n",
            "(18) ‫ לקתל | لقتل | لقتل | 0.0000\n",
            "(19) ‫ צאחבה | صاحب\u001b[1m\u001b[31mه\u001b[0m | صاحب\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(20) ‫ והו | وهو | وهو | 0.0000\n",
            "(21) ‫ יעתקד | يعتقد | يعتقد | 0.0000\n",
            "(22) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(23) ‫ פי | في | في | 0.0000\n",
            "(24) ‫ קתלה | قتل\u001b[1m\u001b[31mه\u001b[0m | قتل\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(25) ‫ אעט'מ | \u001b[1m\u001b[31mإ\u001b[0mعظم | \u001b[1m\u001b[31mأ\u001b[0mعظم | 1.0000\n",
            "(26) ‫ חסנה | حسنة | حسنة | 0.0000\n",
            "(27) ‫ ותקרב | وتقرّب | وتقرّب | 0.0000\n",
            "(28) ‫ אלי | إلى | إلى | 0.0000\n",
            "(29) ‫ אללה | الله | الله | 0.0000\n",
            "(30) ‫ פיקתתלאנ | فيقتتلان | فيقتتلان | 0.0000\n",
            "(31) ‫ וכל | وكل\u001b[1m\u001b[31mّ\u001b[0m | وكل | 1.0000\n",
            "(32) ‫ ואחד | واحد | واحد | 0.0000\n",
            "(33) ‫ מנהמא | منهما | منهما | 0.0000\n",
            "(34) ‫ יעתקד | يعتقد | يعتقد | 0.0000\n",
            "(35) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(36) ‫ מסירה | مسير\u001b[1m\u001b[31mه\u001b[0m | مسير\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(37) ‫ אלי | إلى | إلى | 0.0000\n",
            "(38) ‫ אלג'נה | الجن\u001b[1m\u001b[31mّ\u001b[0mة | الجنة | 1.0000\n",
            "(39) ‫ ואלפרדוס | والفردوس | والفردوس | 0.0000\n",
            "(40) ‫ ותצידקהמא | وتصد\u001b[1m\u001b[31mي\u001b[0mقهما | وتص\u001b[1m\u001b[31mي\u001b[0mدقهما | 2.0000\n",
            "(41) ‫ מחאל | محال | محال | 0.0000\n",
            "(42) ‫ ענד | عند | عند | 0.0000\n",
            "(43) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(44) ‫ קאל | قال | قال | 0.0000\n",
            "(45) ‫ אלפילסופ | الفيلسوف | الفيلسوف | 0.0000\n",
            "(46) ‫ ליס | ليس | ليس | 0.0000\n",
            "(47) ‫ פי | في | في | 0.0000\n",
            "(48) ‫ דינ | دين | دين | 0.0000\n",
            "(49) ‫ אלפלאספה | الفلاسفة | الفلاسفة | 0.0000\n",
            "(50) ‫ קתל | قتل | قتل | 0.0000\n",
            "(51) ‫ ואחד | واحد | واحد | 0.0000\n",
            "(52) ‫ מנ | من | من | 0.0000\n",
            "(53) ‫ האולא | ه\u001b[1m\u001b[31mؤ\u001b[0mلاء | ه\u001b[1m\u001b[31mأو\u001b[0mلاء | 2.0000\n",
            "(54) ‫ אד' | إذ | إذ | 0.0000\n",
            "(55) ‫ יומונ | ي\u001b[1m\u001b[31mؤ\u001b[0mم\u001b[1m\u001b[31mّ\u001b[0mون | ي\u001b[1m\u001b[31mو\u001b[0mمون | 2.0000\n",
            "(56) ‫ אלעקל | العقل | العقل | 0.0000\n",
            "(57) ‫ קאל | قال | قال | 0.0000\n",
            "(58) ‫ אלכ'זרי | الخزريّ | الخزريّ | 0.0000\n",
            "(59) ‫ ואי | و\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mي | 2.0000\n",
            "(60) ‫ חירה | حيرة\u001b[1m\u001b[31mٍ\u001b[0m | حيرة | 1.0000\n",
            "(61) ‫ ענד | عند | عند | 0.0000\n",
            "(62) ‫ אלפלאספה | الفلاسفة | الفلاسفة | 0.0000\n",
            "(63) ‫ אעט'מ | \u001b[1m\u001b[31mإ\u001b[0mعظم | \u001b[1m\u001b[31mأ\u001b[0mعظم | 1.0000\n",
            "(64) ‫ מנ | من | من | 0.0000\n",
            "(65) ‫ אעתקאדהמ | اعتقادهم | اعتقادهم | 0.0000\n",
            "(66) ‫ אלחד'ת | الح\u001b[1m\u001b[31mدث\u001b[0m | الح\u001b[1m\u001b[31mذت\u001b[0m | 2.0000\n",
            "(67) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(68) ‫ אלעאלמ | العالم | العالم | 0.0000\n",
            "(69) ‫ כ'לק | خلق | خلق | 0.0000\n",
            "(70) ‫ פי | في | في | 0.0000\n",
            "(71) ‫ סתה' | ستة | ستة | 0.0000\n",
            "(72) ‫ איאמ | \u001b[1m\u001b[31mإ\u001b[0mيام | \u001b[1m\u001b[31mأ\u001b[0mيام | 1.0000\n",
            "(73) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(74) ‫ אלסבב | السبب | السبب | 0.0000\n",
            "(75) ‫ אלאול | ال\u001b[1m\u001b[31mإ\u001b[0mو\u001b[1m\u001b[31mّ\u001b[0mل | ال\u001b[1m\u001b[31mأ\u001b[0mول | 2.0000\n",
            "(76) ‫ יכלמ | يكل\u001b[1m\u001b[31mّ\u001b[0mم | يكلم | 1.0000\n",
            "(77) ‫ שכ'צא | شخصاً | شخصاً | 0.0000\n",
            "(78) ‫ מנ | من | من | 0.0000\n",
            "(79) ‫ אלנאס | الناس | الناس | 0.0000\n",
            "(80) ‫ פצ'לא | فضلاً | فضلاً | 0.0000\n",
            "(81) ‫ ענ | عن | عن | 0.0000\n",
            "(82) ‫ ד'לכ | ذلك | ذلك | 0.0000\n",
            "(83) ‫ אלתנזיה | التنزي\u001b[1m\u001b[31mه\u001b[0m | التنزي\u001b[1m\u001b[31mّة\u001b[0m | 2.0000\n",
            "(84) ‫ אלד'י | الذي | الذي | 0.0000\n",
            "(85) ‫ תנזהה | تنز\u001b[1m\u001b[31mّهه\u001b[0m | تنز\u001b[1m\u001b[31mةة\u001b[0m | 3.0000\n",
            "(86) ‫ אלפלאספה | الفلاسفة | الفلاسفة | 0.0000\n",
            "(87) ‫ ענ | عن | عن | 0.0000\n",
            "(88) ‫ מערפה | معرفة | معرفة | 0.0000\n",
            "(89) ‫ אלג'זאיאת | الجزئيات | الجزئيات | 0.0000\n",
            "(90) ‫ ומע | ومع | ومع | 0.0000\n",
            "(91) ‫ הד'א | هذا | هذا | 0.0000\n",
            "(92) ‫ פכאנ | فكان | فكان | 0.0000\n",
            "(93) ‫ ינבגי | ينبغي | ينبغي | 0.0000\n",
            "(94) ‫ עלי | على | على | 0.0000\n",
            "(95) ‫ אעמאל | \u001b[1m\u001b[31mإ\u001b[0mعمال | \u001b[1m\u001b[31mأ\u001b[0mعمال | 1.0000\n",
            "(96) ‫ אלפלאספה | الفلاسفة | الفلاسفة | 0.0000\n",
            "(97) ‫ ועלומהמ | وعلومهم | وعلومهم | 0.0000\n",
            "(98) ‫ ותחקיקיהמ | وتحقيق\u001b[1m\u001b[31mه\u001b[0mم | وتحقيق\u001b[1m\u001b[31mية\u001b[0mم | 2.0000\n",
            "(99) ‫ ואג'תהאדהמ | واجتهاد\u001b[1m\u001b[31mه\u001b[0mم | واجتهاد\u001b[1m\u001b[31mة\u001b[0mم | 1.0000\n",
            "(100) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن | \u001b[1m\u001b[31mأ\u001b[0mن | 1.0000\n",
            "(101) ‫ תכונ | تكون | تكون | 0.0000\n",
            "(102) ‫ אלנבוה | النبوّة | النبوّة | 0.0000\n",
            "(103) ‫ משהורה | مشهورة | مشهورة | 0.0000\n",
            "(104) ‫ פיהמ | فيهم | فيهم | 0.0000\n",
            "(105) ‫ שאיעה | شائعة | شائعة | 0.0000\n",
            "(106) ‫ בינהמ | بينهم | بينهم | 0.0000\n",
            "(107) ‫ לאתצאלהמ | ل\u001b[1m\u001b[31mا\u001b[0mتّصال\u001b[1m\u001b[31mه\u001b[0mم | ل\u001b[1m\u001b[31mإ\u001b[0mتّصال\u001b[1m\u001b[31mة\u001b[0mم | 2.0000\n",
            "(108) ‫ באלרוחאניאת | بالروحانيات | بالروحانيات | 0.0000\n",
            "(109) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن | و\u001b[1m\u001b[31mأ\u001b[0mن | 1.0000\n",
            "(110) ‫ יוצפ | يوصف | يوصف | 0.0000\n",
            "(111) ‫ ענהמ | عنهم | عنهم | 0.0000\n",
            "(112) ‫ גראיב | غرا\u001b[1m\u001b[31mئ\u001b[0mب | غرا\u001b[1m\u001b[31mي\u001b[0mب | 1.0000\n",
            "(113) ‫ ומעג'זאת | ومعجزات | ومعجزات | 0.0000\n",
            "(114) ‫ וכראמאת | وكرامات | وكرامات | 0.0000\n",
            "(115) ‫ ולקד | ولقد | ولقد | 0.0000\n",
            "(116) ‫ נרי | نر\u001b[1m\u001b[31mى\u001b[0m | نر\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(117) ‫ אלמנאמאת | المنامات | المنامات | 0.0000\n",
            "(118) ‫ אלצאדקה | الصادقة | الصادقة | 0.0000\n",
            "(119) ‫ למנ | لمن | لمن | 0.0000\n",
            "(120) ‫ למ | لم | لم | 0.0000\n",
            "(121) ‫ יענ | يعن | يعن | 0.0000\n",
            "(122) ‫ באלעלמ | بالعلم | بالعلم | 0.0000\n",
            "(123) ‫ ולא | ولا | ولا | 0.0000\n",
            "(124) ‫ באצפא | ب\u001b[1m\u001b[31mإ\u001b[0mصفا\u001b[1m\u001b[31mء\u001b[0m | ب\u001b[1m\u001b[31mأ\u001b[0mصفا\u001b[1m\u001b[31mً\u001b[0m | 2.0000\n",
            "(125) ‫ נפסה | نفس\u001b[1m\u001b[31mه\u001b[0m | نفس\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(126) ‫ ונג'ד | ونجد | ونجد | 0.0000\n",
            "(127) ‫ צ'ד | ضد\u001b[1m\u001b[31mّ\u001b[0m | ضد | 1.0000\n",
            "(128) ‫ ד'לכ | ذلك | ذلك | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  12.976984126984128 128\n",
            "BATCH (sum_of_e_d,num_of_letters):  61 610\n",
            "(1) ‫ פי | في | في | 0.0000\n",
            "(2) ‫ מנ | من | من | 0.0000\n",
            "(3) ‫ ראמה | رام\u001b[1m\u001b[31mه\u001b[0m | رام\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(4) ‫ פדל | فدل\u001b[1m\u001b[31mّ\u001b[0m | فدل | 1.0000\n",
            "(5) ‫ אנ | \u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | \u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(6) ‫ ללאמר | لل\u001b[1m\u001b[31mإ\u001b[0mمر | لل\u001b[1m\u001b[31mأ\u001b[0mمر | 1.0000\n",
            "(7) ‫ אלאלאהי | الإلهيّ | الإلهيّ | 0.0000\n",
            "(8) ‫ וללנפוס | وللنفوس | وللنفوس | 0.0000\n",
            "(9) ‫ סרא | سر\u001b[1m\u001b[31mّ\u001b[0mاً | سراً | 1.0000\n",
            "(10) ‫ סוי | سوى | سوى | 0.0000\n",
            "(11) ‫ מא | ما | ما | 0.0000\n",
            "(12) ‫ ד'כרתה | ذكرت\u001b[1m\u001b[31mه\u001b[0m | ذكرت\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(13) ‫ יא | يا | يا\u001b[1m\u001b[31mء\u001b[0m | 1.0000\n",
            "(14) ‫ פילסופ | فيلسوف | فيلسوف | 0.0000\n",
            "(15) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1.0000\n",
            "(16) ‫ קאל | قال | قال | 0.0000\n",
            "(17) ‫ אלכ'זרי | الخزريّ | الخزريّ | 0.0000\n",
            "(18) ‫ פי | في | في | 0.0000\n",
            "(19) ‫ נפסה | نفس\u001b[1m\u001b[31mه\u001b[0m | نفس\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(20) ‫ אסאל | \u001b[1m\u001b[31mإ\u001b[0mس\u001b[1m\u001b[31mإ\u001b[0mل | \u001b[1m\u001b[31mأ\u001b[0mس\u001b[1m\u001b[31mا\u001b[0mل | 2.0000\n",
            "(21) ‫ אלנצארי | النصار\u001b[1m\u001b[31mى\u001b[0m | النصار\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(22) ‫ ואלמסלמינ | والمسلمين | والمسلمين | 0.0000\n",
            "(23) ‫ פאנ | فإن\u001b[1m\u001b[31mّ\u001b[0m | فإن | 1.0000\n",
            "(24) ‫ אחד | \u001b[1m\u001b[31mإ\u001b[0mحد | \u001b[1m\u001b[31mأ\u001b[0mحد | 1.0000\n",
            "(25) ‫ אלעמלינ | العملين | العملين | 0.0000\n",
            "(26) ‫ הו | هو | هو | 0.0000\n",
            "(27) ‫ לא | لا | لا | 0.0000\n",
            "(28) ‫ שכ | شك | شك | 0.0000\n",
            "(29) ‫ אלמרצ'י | المرضي\u001b[1m\u001b[31mّ\u001b[0m | المرضي | 1.0000\n",
            "(30) ‫ ואמא | وإم\u001b[1m\u001b[31mّ\u001b[0mا | وإما | 1.0000\n",
            "(31) ‫ אליהוד | اليهود | اليهود | 0.0000\n",
            "(32) ‫ פכפי | فكف\u001b[1m\u001b[31mى\u001b[0m | فكف\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(33) ‫ מא | ما | ما | 0.0000\n",
            "(34) ‫ ט'הר | ظهر | ظهر | 0.0000\n",
            "(35) ‫ מנ | من | من | 0.0000\n",
            "(36) ‫ ד'לתהמ | ذل\u001b[1m\u001b[31mّ\u001b[0mتهم | ذلتهم | 1.0000\n",
            "(37) ‫ וקלתהמ | وقل\u001b[1m\u001b[31mّ\u001b[0mتهم | وقلتهم | 1.0000\n",
            "(38) ‫ ומקת | ومقت | ومقت | 0.0000\n",
            "(39) ‫ אלג'מיע | الجميع | الجميع | 0.0000\n",
            "(40) ‫ להמ | لهم | لهم | 0.0000\n",
            "(41) ‫ פדעא | فدعا | فدعا\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(42) ‫ בעאלמ | بعالم | بعالم | 0.0000\n",
            "(43) ‫ מנ | من | من | 0.0000\n",
            "(44) ‫ עלמא | علما\u001b[1m\u001b[31mء\u001b[0m | علما\u001b[1m\u001b[31mً\u001b[0m | 1.0000\n",
            "(45) ‫ אלנצארי | النصار\u001b[1m\u001b[31mى\u001b[0m | النصار\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(46) ‫ פסאלה | فس\u001b[1m\u001b[31mإ\u001b[0mل\u001b[1m\u001b[31mه\u001b[0m | فس\u001b[1m\u001b[31mا\u001b[0mل\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(47) ‫ ענ | عن | عن | 0.0000\n",
            "(48) ‫ עלמה | علم\u001b[1m\u001b[31mه\u001b[0m | علم\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(49) ‫ ועמלה | وعمل\u001b[1m\u001b[31mه\u001b[0m | وعمل\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(50) ‫ פקאל | فقال | فقال | 0.0000\n",
            "(51) ‫ לה | له | له | 0.0000\n",
            "(52) ‫ אנא | \u001b[1m\u001b[31mإ\u001b[0mنا | \u001b[1m\u001b[31mأ\u001b[0mنا | 1.0000\n",
            "(53) ‫ מומנ | م\u001b[1m\u001b[31mؤ\u001b[0mمن | م\u001b[1m\u001b[31mو\u001b[0mمن | 1.0000\n",
            "(54) ‫ באלחדת' | بالحدث | بالحدث | 0.0000\n",
            "(55) ‫ ללמכ'לוקאת | للمخلوقات | للمخلوقات | 0.0000\n",
            "(56) ‫ ובאלקדמ | وبالقدم | وبالقدم | 0.0000\n",
            "(57) ‫ לכ'אלק | \u001b[1m\u001b[31mل\u001b[0mلخالق | لخالق | 1.0000\n",
            "(58) ‫ תע' | تع\u001b[1m\u001b[31mالى\u001b[0m | تع | 3.0000\n",
            "(59) ‫ ואנה | و\u001b[1m\u001b[31mإ\u001b[0mنّه | و\u001b[1m\u001b[31mأ\u001b[0mنّه | 1.0000\n",
            "(60) ‫ כ'לק | خلق | خلق | 0.0000\n",
            "(61) ‫ אלעאלמ | العالم | العالم | 0.0000\n",
            "(62) ‫ באסרה | ب\u001b[1m\u001b[31mإ\u001b[0mسر\u001b[1m\u001b[31mه\u001b[0m | ب\u001b[1m\u001b[31mا\u001b[0mسر\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(63) ‫ פי | في | في | 0.0000\n",
            "(64) ‫ סתה' | ستة | ستة | 0.0000\n",
            "(65) ‫ איאמ | \u001b[1m\u001b[31mإ\u001b[0mيام | \u001b[1m\u001b[31mأ\u001b[0mيام | 1.0000\n",
            "(66) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(67) ‫ ג'מיע | جميع | جميع | 0.0000\n",
            "(68) ‫ אלנאטקינ | الناطقين | الناطقين | 0.0000\n",
            "(69) ‫ מנ | من | من | 0.0000\n",
            "(70) ‫ ד'ריה | ذرية | ذرية | 0.0000\n",
            "(71) ‫ אדמ | \u001b[1m\u001b[31mآ\u001b[0mدم | \u001b[1m\u001b[31mأ\u001b[0mدم | 1.0000\n",
            "(72) ‫ ת'מ | ثم\u001b[1m\u001b[31mّ\u001b[0m | ثم | 1.0000\n",
            "(73) ‫ ד'ריה | ذرية | ذرية | 0.0000\n",
            "(74) ‫ נוח | نوح | نوح | 0.0000\n",
            "(75) ‫ ואליה | وإليه | وإليه | 0.0000\n",
            "(76) ‫ ינתסבונ | ينتسبون | ينتسبون | 0.0000\n",
            "(77) ‫ כלהמ | كلّهم | كلّهم | 0.0000\n",
            "(78) ‫ ואנ | و\u001b[1m\u001b[31mإ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mن | 2.0000\n",
            "(79) ‫ ללה | لله | لله | 0.0000\n",
            "(80) ‫ ענאיה | عناية | عناية | 0.0000\n",
            "(81) ‫ באלכ'לק | بالخلق | بالخلق | 0.0000\n",
            "(82) ‫ ואתצאלא | وات\u001b[1m\u001b[31mّ\u001b[0mصالا\u001b[1m\u001b[31mً\u001b[0m | واتصالا | 2.0000\n",
            "(83) ‫ באלנאטקינ | بالناطقين | بالناطقين | 0.0000\n",
            "(84) ‫ וסכ'טא | وسخطاً | وسخطاً | 0.0000\n",
            "(85) ‫ ורצ'א | ورضاً | ورضاً | 0.0000\n",
            "(86) ‫ ורחמה | ورحمة | ورحمة | 0.0000\n",
            "(87) ‫ וכלאמא | وكلاما\u001b[1m\u001b[31mً\u001b[0m | وكلاما | 1.0000\n",
            "(88) ‫ וט'הורא | وظهوراً | وظهوراً | 0.0000\n",
            "(89) ‫ ותג'ליא | وتجل\u001b[1m\u001b[31mّ\u001b[0mياً | وتجلياً | 1.0000\n",
            "(90) ‫ לאנביאה | ل\u001b[1m\u001b[31mإ\u001b[0mنبيا\u001b[1m\u001b[31mئه\u001b[0m | ل\u001b[1m\u001b[31mأ\u001b[0mنبيا\u001b[1m\u001b[31mءة\u001b[0m | 3.0000\n",
            "(91) ‫ ואוליאה | و\u001b[1m\u001b[31mإ\u001b[0mوليا\u001b[1m\u001b[31mئه\u001b[0m | و\u001b[1m\u001b[31mأ\u001b[0mوليا\u001b[1m\u001b[31mة\u001b[0m | 3.0000\n",
            "(92) ‫ וחלולא | وحلولا\u001b[1m\u001b[31mً\u001b[0m | وحلولا | 1.0000\n",
            "(93) ‫ פי | في | في | 0.0000\n",
            "(94) ‫ מא | ما | ما | 0.0000\n",
            "(95) ‫ בינ | بين | بين | 0.0000\n",
            "(96) ‫ מנ | من | من | 0.0000\n",
            "(97) ‫ ירצ'אה | يرضا\u001b[1m\u001b[31mه\u001b[0m | يرضا\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(98) ‫ מנ | من | من | 0.0000\n",
            "(99) ‫ אלג'מאהיר | الجماهير | الجماهير | 0.0000\n",
            "(100) ‫ ואלג'מלה | والجملة | والجملة | 0.0000\n",
            "(101) ‫ פכל | فكل\u001b[1m\u001b[31mّ\u001b[0m | فكل | 1.0000\n",
            "(102) ‫ מא | ما | ما | 0.0000\n",
            "(103) ‫ ג'א | جاء | جاء | 0.0000\n",
            "(104) ‫ פי | في | في | 0.0000\n",
            "(105) ‫ אלתוראה | التوراة | التوراة | 0.0000\n",
            "(106) ‫ ופי | وفي | وفي | 0.0000\n",
            "(107) ‫ את'אר | \u001b[1m\u001b[31mآ\u001b[0mثار | \u001b[1m\u001b[31mأ\u001b[0mثار | 1.0000\n",
            "(108) ‫ בני | بني | بني | 0.0000\n",
            "(109) ‫ אסראיל | إسرائيل | إسرائيل | 0.0000\n",
            "(110) ‫ אלתי | التي | التي | 0.0000\n",
            "(111) ‫ לא | لا | لا | 0.0000\n",
            "(112) ‫ מדפע | مدفع | مدفع | 0.0000\n",
            "(113) ‫ פי | في | في | 0.0000\n",
            "(114) ‫ צדקהא | صدقها | صدقها | 0.0000\n",
            "(115) ‫ לשהרתהא | لشهرتها | لشهرتها | 0.0000\n",
            "(116) ‫ ודואמהא | ودوامها | ودوامها | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  12.371031746031747 116\n",
            "BATCH (sum_of_e_d,num_of_letters):  57 530\n",
            "#examples: 500 , accuracy: 0.897061038961039\n",
            "#letters: 2325 , accuracy1: 0.8989247311827957\n",
            "(1) ‫ דאר | دار | دار | 0.0000\n",
            "(2) ‫ אלגזא | الجزاء | الجزاء | 0.0000\n",
            "(3) ‫ וקבל | وقبل | وقبل | 0.0000\n",
            "(4) ‫ ד'לכ | ذلك | ذلك | 0.0000\n",
            "(5) ‫ מא | ما | ما | 0.0000\n",
            "(6) ‫ ראי | رأ\u001b[1m\u001b[31mى\u001b[0m | رأ\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(7) ‫ אנ | أن | أن | 0.0000\n",
            "(8) ‫ יפרק | يفر\u001b[1m\u001b[31mّ\u001b[0mق | يفرق | 1.0000\n",
            "(9) ‫ בינ | بين | بين | 0.0000\n",
            "(10) ‫ רוחה | روح\u001b[1m\u001b[31mه\u001b[0m | روح\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(11) ‫ וגסמה | وجسمه | وجسمه | 0.0000\n",
            "(12) ‫ אלי | إلى | إلى | 0.0000\n",
            "(13) ‫ וקת | وقت | وقت | 0.0000\n",
            "(14) ‫ אסתכמאל | استكمال | استكمال | 0.0000\n",
            "(15) ‫ אלנפוס | النفوس | النفوس | 0.0000\n",
            "(16) ‫ חתי | حتى | حتى | 0.0000\n",
            "(17) ‫ יגמעהא | ي\u001b[1m\u001b[31mج\u001b[0mمعها | ي\u001b[1m\u001b[31mغ\u001b[0mمعها | 1.0000\n",
            "(18) ‫ אלגמיע | ال\u001b[1m\u001b[31mج\u001b[0mميع | ال\u001b[1m\u001b[31mغ\u001b[0mميع | 1.0000\n",
            "(19) ‫ עלי | على | على | 0.0000\n",
            "(20) ‫ מא | ما | ما | 0.0000\n",
            "(21) ‫ בינת | بي\u001b[1m\u001b[31mّ\u001b[0mنت | بينت | 1.0000\n",
            "(22) ‫ פלא | فلا | فلا | 0.0000\n",
            "(23) ‫ נעלמ | نعلم | نعلم | 0.0000\n",
            "(24) ‫ יהודיא | يهودي\u001b[1m\u001b[31mّ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | يهوديا | 2.0000\n",
            "(25) ‫ יכ'אלפ | يخالف | يخالف | 0.0000\n",
            "(26) ‫ עלי | على | على | 0.0000\n",
            "(27) ‫ הד'ה | هذه | هذه | 0.0000\n",
            "(28) ‫ אלאמאנה | ال\u001b[1m\u001b[31mأ\u001b[0mمانة | ال\u001b[1m\u001b[31mإ\u001b[0mمانة | 1.0000\n",
            "(29) ‫ ולא | ولا | ولا | 0.0000\n",
            "(30) ‫ יסתצעב | يستصعب | يستصعب | 0.0000\n",
            "(31) ‫ ענד | عند | عند | 0.0000\n",
            "(32) ‫ עקלה | عقل\u001b[1m\u001b[31mه\u001b[0m | عقل\u001b[1m\u001b[31mة\u001b[0m | 1.0000\n",
            "(33) ‫ כיפ | كيف | كيف | 0.0000\n",
            "(34) ‫ יחיי | يحيي | يحيي | 0.0000\n",
            "(35) ‫ רבה | رب\u001b[1m\u001b[31mّه\u001b[0m | رب\u001b[1m\u001b[31mة\u001b[0m | 2.0000\n",
            "(36) ‫ אלמותי | الموت\u001b[1m\u001b[31mى\u001b[0m | الموت\u001b[1m\u001b[31mي\u001b[0m | 1.0000\n",
            "(37) ‫ אד' | إذ | إذ | 0.0000\n",
            "(38) ‫ קד | قد | قد | 0.0000\n",
            "(39) ‫ צח | صح\u001b[1m\u001b[31mّ\u001b[0m | صح | 1.0000\n",
            "(40) ‫ לה | له | له | 0.0000\n",
            "(41) ‫ אנה | أنه | أن\u001b[1m\u001b[31mّ\u001b[0mه | 1.0000\n",
            "(42) ‫ כ'לק | خلق | خلق | 0.0000\n",
            "(43) ‫ שיא | شي\u001b[1m\u001b[31mئ\u001b[0mا\u001b[1m\u001b[31mً\u001b[0m | شيا\u001b[1m\u001b[31mء\u001b[0m | 2.0000\n",
            "(44) ‫ לא | لا | لا | 0.0000\n",
            "(45) ‫ מנ | من | من | 0.0000\n",
            "(46) ‫ שי | شيء | شيء | 0.0000\n",
            "(47) ‫ פלא | فلا | فلا | 0.0000\n",
            "(48) ‫ יגוז | يجوز | يجوز | 0.0000\n",
            "(49) ‫ אנ | أن | أن | 0.0000\n",
            "(50) ‫ יסתעסר | يستعسر | يستعسر | 0.0000\n",
            "BATCH (sum_of_e_d_normalized,num_of_examples):  3.692857142857143 50\n",
            "BATCH (sum_of_e_d,num_of_letters):  17 192\n",
            "#examples: 50 , accuracy: 0.9261428571428572\n",
            "#letters: 192 , accuracy1: 0.9114583333333334\n",
            "(1) ‫ הד'א . קאל אלכ'זרי : | هذا . قال الخزريّ : | هذا . قال الخزريّ : | 0.0000\n",
            "(2) ‫ , ולא H H למא פי ד'לכ | , ولا H H لما في ذلك | , ولا H H لما في ذلك | 0.0000\n",
            "(3) ‫ ואסתחאלתהמא ודכ'ולהמא | واستحالتهما ودخولهما | واستحالتهما ودخولهما | 0.0000\n",
            "(4) ‫ ט'הרת פי אברהימ . קאל | ظهرت في إبر\u001b[1m\u001b[31mا\u001b[0mهيم . قال | ظهرت في إبرهيم . قال | 0.0476\n",
            "(5) ‫ ואלוחי ותסמי כ'אציתהא | والوحي وتسمّى خاص\u001b[1m\u001b[31mّ\u001b[0mيتها | والوحي وتسمّى خاصيتها | 0.0455\n",
            "(6) ‫ מנ וקת יכ'תצ בה , ימכנ | من وقت يختص\u001b[1m\u001b[31mّ\u001b[0m به , يمكن | من وقت يختص به , يمكن | 0.0455\n",
            "(7) ‫ : אנה לא שכ יתמני אנ | : \u001b[1m\u001b[31mإ\u001b[0mنّه لا شك\u001b[1m\u001b[31mّ\u001b[0m يتمن\u001b[1m\u001b[31mّ\u001b[0mى أن | : \u001b[1m\u001b[31mأ\u001b[0mنّه لا شك يتمنى أن | 0.1304\n",
            "(8) ‫ ללמעמורה כלהא אלא באנ | للمعمورة كلّها إلاّ بأن | للمعمورة كلّها إلاّ بأن | 0.0000\n",
            "(9) ‫ מנ כ'יאל ופכר וגיר ד'לכ | من خيال وفكر وغير ذلك | من خيال وفكر وغير ذلك | 0.0000\n",
            "(10) ‫ מלכה דונ תעלמ , ינסג' | ملكة دون تعل\u001b[1m\u001b[31mّ\u001b[0mم , ينسج | ملكة دون تعلم , ينسج | 0.0476\n",
            "(11) ‫ , וליס ללאנסאנ פי תלכ | , وليس للإنسان في تلك | , وليس للإنسان في تلك | 0.0000\n",
            "(12) ‫ מטלובכ , אעני אלאתצאל | مطلبك , أعني ال\u001b[1m\u001b[31mا\u001b[0mتّصال | مطل\u001b[1m\u001b[31mو\u001b[0mبك , أعني ال\u001b[1m\u001b[31mإ\u001b[0mتّصال | 0.0952\n",
            "(13) ‫ , באי שי ידרכ ופי אי | , بأي\u001b[1m\u001b[31mّ\u001b[0m شيء يدرك وفي أي\u001b[1m\u001b[31mّ\u001b[0m | , بأي شيء يدرك وفي أي | 0.0870\n",
            "(14) ‫ ונשכר עליהא פי H H H | ونشكر عليها في H H H | ونشك\u001b[1m\u001b[31mّ\u001b[0mر عليها في H H H | 0.0500\n",
            "(15) ‫ ואלאתפאקיה כל אלמבאלאה | والاتّفاقية كل\u001b[1m\u001b[31mّ\u001b[0m المبالاة | والاتّفاقية كل المبالاة | 0.0417\n",
            "(16) ‫ מנ חות אלבחר קותא מנ | من حوت البحر قوتاً من | من حوت البحر قوتاً من | 0.0000\n",
            "(17) ‫ אתאהמא אלנבוה אבני ת'מאנינ | أت\u001b[1m\u001b[31mا\u001b[0mهما النبوّة \u001b[1m\u001b[31mا\u001b[0mبني ثمانين | أت\u001b[1m\u001b[31mأ\u001b[0mهما النبوّة \u001b[1m\u001b[31mأ\u001b[0mبني ثمانين | 0.0769\n",
            "LER (label error rate):  0.04047115183893288\n",
            "(1) ‫ האהנא מצנועא כד'אכ קולנא | هاهنا مصنوعاً كذاك قولنا | هاهنا مصنوعاً كذاك قولنا | 0.0000\n",
            "(2) ‫ אליה . ונתבע ד'לכ בשרח | إليه . ونتبع ذلك بشرح | إليه . ونتبع ذلك بشرح | 0.0000\n",
            "(3) ‫ . וקלת איצ'א לעלהמ ידעונ | . وقلت أيضاً لعل\u001b[1m\u001b[31mّ\u001b[0mهم يد\u001b[1m\u001b[31mّ\u001b[0mعون | . وقلت أيضاً لعلهم يدعون | 0.0769\n",
            "(4) ‫ ואלסמעיה פינבגי אנ אבינ | والسمعية فينبغي أن أبي\u001b[1m\u001b[31mّ\u001b[0mن | والسمعية فينبغي أن\u001b[1m\u001b[31mّ\u001b[0m أبين | 0.0833\n",
            "(5) ‫ ראי אלנאס ד'כרה פי פט'נוא | رأ\u001b[1m\u001b[31mى\u001b[0m الناس ذكر\u001b[1m\u001b[31mه\u001b[0m في فظن\u001b[1m\u001b[31mّ\u001b[0mوا | رأ\u001b[1m\u001b[31mي\u001b[0m الناس ذكر\u001b[1m\u001b[31mة\u001b[0m في فظنوا | 0.1250\n",
            "(6) ‫ לה מראי פקאל אנה לא סבב | له مر\u001b[1m\u001b[31mئ\u001b[0mي فقال \u001b[1m\u001b[31mإ\u001b[0mنه لا سبب | له مر\u001b[1m\u001b[31mأ\u001b[0mي فقال \u001b[1m\u001b[31mأ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mه لا سبب | 0.1304\n",
            "(7) ‫ דקה' נט'רה פיצל אלי אנ | دق\u001b[1m\u001b[31mّ\u001b[0mة نظر\u001b[1m\u001b[31mه\u001b[0m فيصل إلى أن\u001b[1m\u001b[31mّ\u001b[0m | دقة نظر\u001b[1m\u001b[31mة\u001b[0m فيصل إلى أن | 0.1364\n",
            "(8) ‫ ואנמא רדדת אלכלאמ פי | وإنما رددت الكلام في | وإن\u001b[1m\u001b[31mّ\u001b[0mما رددت الكلام في | 0.0500\n",
            "(9) ‫ H H H H H . וינבגי אנ | H H H H H . وينبغي أن | H H H H H . وينبغي أن | 0.0000\n",
            "(10) ‫ חכמה ג'הלא מעא , חכמה | حكمة جهلاً معاً , حكمة | حكمة جهلاً معاً , حكمة | 0.0000\n",
            "(11) ‫ אלעלמ פקט כמא שרחנא פי | العلم فقط كما شرحنا في | العلم فقط كما شرحنا في | 0.0000\n",
            "(12) ‫ . פאקול אולא , אנהמ רפצ'וא | . فأقول أولاً , \u001b[1m\u001b[31mإ\u001b[0mنهم رفضوا | . فأقول أولاً , \u001b[1m\u001b[31mأ\u001b[0mن\u001b[1m\u001b[31mّ\u001b[0mهم رفضوا | 0.0769\n",
            "(13) ‫ הד'ה ג'מל קריבה ינתפע | هذه جمل قريبة ينتفع | هذه جمل قريبة ينتفع | 0.0000\n",
            "(14) ‫ מעטיה אלאת אלחס פכיפ | معطية آلات الحس\u001b[1m\u001b[31mّ\u001b[0m فكيف | معطية آلات الحس فكيف | 0.0476\n",
            "(15) ‫ נאזלא מנ אלהוא הל הו | نازلاً من الهواء هل هو | نازلاً من الهواء هل هو | 0.0000\n",
            "(16) ‫ אלכביר , לאנה אנמא אחיי | الكبير , لأنه إنما أحي\u001b[1m\u001b[31mى\u001b[0m | الكبير , لأن\u001b[1m\u001b[31mّ\u001b[0mه إن\u001b[1m\u001b[31mّ\u001b[0mما أحي\u001b[1m\u001b[31mي\u001b[0m | 0.1304\n",
            "LER (label error rate):  0.04842920867992973\n",
            "saving checkpoing at /gdrive/My Drive/checkpoints/2020-05-04 12:32:58.607342/ckpt\n",
            "========================================================================================================================================================================================================\n",
            "CONTINUE TRAINING\n",
            "Epoch 2 Batch 0 Loss 7.2027\n",
            "Epoch 2 Batch 10 Loss 5.5073\n",
            "Epoch 2 Batch 20 Loss 6.1981\n",
            "Epoch 2 Batch 30 Loss 4.6598\n",
            "Epoch 2 Batch 40 Loss 6.4944\n",
            "Epoch 2 Batch 50 Loss 4.9574\n",
            "Epoch 2 Batch 60 Loss 7.1667\n",
            "Epoch 2 Batch 70 Loss 4.8674\n",
            "Epoch 2 Batch 80 Loss 4.3820\n",
            "Epoch 2 Batch 90 Loss 7.3323\n",
            "Epoch 2 Batch 100 Loss 5.8888\n",
            "Epoch 2 Batch 110 Loss 4.2429\n",
            "Epoch 2 Batch 120 Loss 4.9897\n",
            "Epoch 2 Batch 130 Loss 5.1589\n",
            "Epoch 2 Batch 140 Loss 5.1039\n",
            "Epoch 2 Batch 150 Loss 5.6872\n",
            "Epoch 2 Batch 160 Loss 5.1204\n",
            "Epoch 2 Batch 170 Loss 5.1727\n",
            "Epoch 2 Batch 180 Loss 4.6024\n",
            "Epoch 2 Batch 190 Loss 4.9363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwpwDinoeIu5",
        "colab_type": "text"
      },
      "source": [
        "#MAIN OUTPUT (ABOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh93f3m4fb3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#PAIST HERE COMPARISMENT FROM LOG\n",
        "text='''MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
        "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
        "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
        "MAIN:  (4) ‫ אלחולמ יתבעה סאכנ ממדוד | الحولم يتبعه ساكن ممدود | الحولم يتبعه ساكن ممدود | 0.0000\n",
        "MAIN:  (5) ‫ , ובנבאהה' אלקלב ענ בקא | , وبنباهة القلب عن بقاء | , وبنباهة القلب عن بقاء | 0.0000\n",
        "MAIN:  (6) ‫ הו כלאמה . פצאר H H H | هو كلامه . فصار H H H | هو كلامه . فصار H H H | 0.0000\n",
        "MAIN:  (7) ‫ בנא , אד' לא יערפה חק | بنا , إذ لا يعرفه حق\u001b[1m\u001b[31mّ\u001b[0m | بنا , إذ لا يعرفه حق | 0.0476\n",
        "MAIN:  (8) ‫ במא כאנ ליס סבבא לכונה | بما كان ليس سبباً لكونه | بما كان ليس سبباً لكونه | 0.0000\n",
        "MAIN:  (9) ‫ H H H . ויתכ'יל אלשכינה | H H H . ويتخيّل \"السكينة\" | H H H . ويتخيّل \"السكينة\" | 0.0000\n",
        "MAIN:  (10) ‫ אנמא הי אלאת ואדואת מוצלה | إنّما هي آلات وأدوات موصّلة | إنّما هي آلات وأدوات موصّلة | 0.0000\n",
        "MAIN:  (11) ‫ במא יעמל , פאנהמ לא יתורעונ | بما يعمل , فإنّهم لا يتورّعون | بما يعمل , فإنّهم لا يتورّعون | 0.0000\n",
        "MAIN:  (12) ‫ . ואמא פמנהא אסתעמאל | . وأمّا فمنها استعمال | . وأمّا فمنها استعمال | 0.0000\n",
        "MAIN:  (13) ‫ עלי אכ'רינ בהדמ דיארהמ | على آخرين بهدم ديارهم | على آخرين بهدم ديارهم | 0.0000\n",
        "MAIN:  (14) ‫ : ליס ימכנ אנ יכונ H | : ليس يمكن أن يكون H | : ليس يمكن أن يكون H | 0.0000\n",
        "MAIN:  (15) ‫ ושיעה' אפלאטונ וגירהמ | وشيعة أفلاطون وغيرهم | وشيعة أفلاطون وغيرهم | 0.0000\n",
        "MAIN:  (16) ‫ אללגאת פי אלכמאל , פאינ | اللغات في الكمال , ف\u001b[1m\u001b[31mأ\u001b[0mين | اللغات في الكمال , ف\u001b[1m\u001b[31mإ\u001b[0mي\u001b[1m\u001b[31mّ\u001b[0mن | 0.0870\n",
        "MAIN:  (17) ‫ , ולמ יבק אלא אלכתב אלשרעיה | , ولم يبق إلاّ الكتب الشرعية | , ولم يبق إلاّ الكتب الشرعية | 0.0000\n",
        "MAIN:  LER (label error rate):  0.0245384859110728\n",
        "MAIN:  (1) ‫ , מנ אלמחאל אנ יג'י הו | , من المحال أن يجي\u001b[1m\u001b[31mء\u001b[0m هو | , من المحال أن يجي هو | 0.0455\n",
        "MAIN:  (2) ‫ H H H H . ואנה ג'על אג'לא | H H H H . وأنه جعل أجلاً | H H H H . وأن\u001b[1m\u001b[31mّ\u001b[0mه جعل أجلاً | 0.0417\n",
        "MAIN:  (3) ‫ נאר וקאל בעצ' הוא וקאל | نار وقال بعض هو\u001b[1m\u001b[31mا\u001b[0mء وقال | نار وقال بعض هوء وقال | 0.0455\n",
        "MAIN:  (4) ‫ יכונ ת'לג'א או מא בל | يكون ثلجاً أو ما\u001b[1m\u001b[31mءً\u001b[0m بل | يكون ثلجاً أو ما بل | 0.0952\n",
        "MAIN:  (5) ‫ תכונ חאלהמ , הל ימותונ | تكون حالهم , هل يموتون | تكون حالهم , هل يموتون | 0.0000\n",
        "MAIN:  (6) ‫ בה עלי ואג'ב וארשאדהמ | به عل\u001b[1m\u001b[31mيّ\u001b[0m واجب\u001b[1m\u001b[31mٌ\u001b[0m وإرشادهم | به عل\u001b[1m\u001b[31mى\u001b[0m واجب وإرشادهم | 0.1364\n",
        "MAIN:  (7) ‫ יאתי בהא רבנא מנ מעני | يأتي بها ربّنا من معنى | يأتي بها ربّنا من معنى | 0.0000\n",
        "MAIN:  (8) ‫ ואכ'רונ לסבב גמוצ'ה וגמוצ' | وآخرون لسبب غموض\u001b[1m\u001b[31mه\u001b[0m وغموض | وآخرون لسبب غموض\u001b[1m\u001b[31mة\u001b[0m وغموض | 0.0435\n",
        "MAIN:  (9) ‫ עלי כל ואחד עמלה , וקד | على كل واحد عمله , وقد | على كل واحد عمله , وقد | 0.0000\n",
        "MAIN:  (10) ‫ אולאיכ אלמחיינ . ובעצ' | أول\u001b[1m\u001b[31mا\u001b[0mئك المحي\u001b[1m\u001b[31mّ\u001b[0mين . وبعض | أولئك المحيين . وبعض | 0.0909\n",
        "MAIN:  (11) ‫ וית'בת אלתחקיק . פהד'ה | ويثبت التحقيق . فهذه | ويثبت التحقيق . فهذه | 0.0000\n",
        "MAIN:  (12) ‫ בנסכ' , לאנ אללה אמר | بنسخ , لأنّ الله أمر | بنسخ , لأنّ الله أمر | 0.0000\n",
        "MAIN:  (13) ‫ מא יאתי ואנ עלמהא ענדה | ما يأتي وإن\u001b[1m\u001b[31mّ\u001b[0m علمه\u001b[1m\u001b[31mم\u001b[0mا عنده | ما يأتي وإن علمها عنده | 0.0833\n",
        "MAIN:  (14) ‫ בפואסיק מנ , פארי אנ | بفواسيق من , فأرى أن | بفواسيق من , فأرى أن\u001b[1m\u001b[31mّ\u001b[0m | 0.0500\n",
        "MAIN:  (15) ‫ אלד'י סאלוא ענה ליס הו | الذي سألوا عنه ليس هو | الذي سألوا عنه ليس هو | 0.0000\n",
        "MAIN:  (16) ‫ אלמחדת' , ועלי אנהא קאלת | المحدث , وعلى أنها قالت | المحدث , وعلى أنها قالت | 0.0000\n",
        "MAIN:  LER (label error rate):  0.033400332030791034'''\n",
        "print(text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_loss()\n",
        "#test_loss(test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   train_loop()\n",
        "#   test_loss(single_words_test_dataset,limit=5)\n",
        "#   test_loss(limit=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # test_loss(only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double1,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe",
        "colab_type": "text"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    }
  ]
}