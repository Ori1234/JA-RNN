{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ori1234/JA-RNN/blob/master/Thesis_8_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY",
        "colab_type": "text"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i)\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window.\n",
        "\n",
        "\n",
        "Note to self: Make sure you dont run anything for more than 12 hrs on Colab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE",
        "colab_type": "text"
      },
      "source": [
        "**SUMMERY**\n",
        "\n",
        "say somthing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTdNSHUmCvD",
        "colab_type": "text"
      },
      "source": [
        "#IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"IMPORTS\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import math\n",
        "#import time\n",
        "\n",
        "\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrDNbLhC7H",
        "colab_type": "text"
      },
      "source": [
        "#GLOBAL VARS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fPaxl8g_BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "this_time=str(datetime.now())\n",
        "GLOBAL_epoch=0\n",
        "DEBUG=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AndYz46gsER",
        "colab_type": "text"
      },
      "source": [
        "#LOGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldvhaPtVF0Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######To clean logs\n",
        "######!rm log*.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxiE-PU7A-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ae805fb-0c6b-44f6-e361-497647235b4a"
      },
      "source": [
        "CELL_NAME=\"START LOG\"\n",
        "\n",
        "\n",
        "log_file=\"LOG___\"+this_time+\".txt\"\n",
        "f_logg= open(log_file,\"w\")\n",
        "print(\"logging to file (will be added to mail)\",log_file)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _print_log(also_print,*txts):  \n",
        "  txt=\"\"\n",
        "  for t in txts:\n",
        "    txt+=\" \"+str(t)\n",
        "  f_logg.write(CELL_NAME+\" \"+str(datetime.now())+\": \"+txt+'\\n') #TODO ADD TIME!!!!\n",
        "  if also_print:\n",
        "    print(*txts)\n",
        "\n",
        "def print_log(*txts):\n",
        "  if (DEBUG):\n",
        "    print(*txts)\n",
        "  _print_log(False,*txts)\n",
        "\n",
        "\n",
        "def print_log_screen(*txts):\n",
        "  _print_log(True,*txts)\n",
        "\n",
        "\n",
        "def log_flush():\n",
        "  f_logg.flush()\n",
        "\n",
        "def close_log():\n",
        "  f_logg.close()\n",
        "\n",
        "\n",
        "\n",
        "#DON'T FORGET TO CLOSE THE FILE AT THE END\n"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logging to file (will be added to mail) LOG___2020-05-07 12:15:49.986380.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh93f3m4fb3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "70338a17-5ae9-4211-d2a8-f8b83e57f4de"
      },
      "source": [
        "\n",
        "#PAIST HERE TO SEE PROPERLY THE SHOW DIFF PART IN LOGS\n",
        "text='''MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
        "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
        "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
        "MAIN:  LER (label error rate):  0.033400332030791034'''\n",
        "print(text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
            "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
            "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
            "MAIN:  LER (label error rate):  0.033400332030791034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp",
        "colab_type": "text"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab_type": "code",
        "outputId": "2236c0c3-7e93-4213-eff9-25f8ed9cd8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "CELL_NAME=\"MOUNT DRIVE\"\n",
        "print(\"mounting to drive at /gdrive\")\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mounting to drive at /gdrive\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY",
        "colab_type": "text"
      },
      "source": [
        "#MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d4d933d-389b-4fe3-faaf-65b7c6d78a73"
      },
      "source": [
        "CELL_NAME=\"BATCH_SIZE and STATEFUL\"\n",
        "\n",
        "STATEFUL=False\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "print_log_screen(\"set batch size to \"+str(BATCH_SIZE))\n",
        "print_log_screen(\"STATEFUL: \"+str(STATEFUL))"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set batch size to 128\n",
            "STATEFUL: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw",
        "colab_type": "text"
      },
      "source": [
        "# INIT RANDOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "\n",
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "def init_random():\n",
        "  np.random.seed(1)\n",
        "  tf.compat.v1.set_random_seed(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUbUyZx3i1OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "init_random()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g",
        "colab_type": "text"
      },
      "source": [
        "#CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n",
        "\n",
        "LTRchar=u'\\u202B'   #align rtl symbole\n",
        "BLANK=\"_\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK",
        "colab_type": "text"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF",
        "colab_type": "text"
      },
      "source": [
        "##send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"SEND MAIL\"\n",
        "\n",
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  files=[log_file,\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP",
        "colab_type": "text"
      },
      "source": [
        "##plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PLOT\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "#my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "##letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz",
        "colab_type": "code",
        "outputId": "0ea93b2a-f6f3-4283-80b1-dcc3fb30cd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "CELL_NAME=\"LETTER MAPPING\"\n",
        "\n",
        "tag=\"'\"\n",
        "\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"+tag\n",
        "\n",
        "#\"א\": \"اإآٱأ\", with wasla\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآٱأ\",\n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "#######################################################\n",
        "for c in additional_letters:\n",
        "  letter_dict[c]=c\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print_log(\"arab_heb_maping\",arab_heb_maping)\n",
        "print_log(\"length:\",len(arab_heb_maping))\n",
        "print_log(\"heb_arab_maping\",heb_arab_maping)\n",
        "print_log(\"length:\",len(heb_arab_maping))\n",
        "\n",
        "#################################################################3\n",
        "#FUNCTIONS:\n",
        "\n",
        "def remove_chars_not_in_map(phrase,map):\n",
        "  res=[]\n",
        "  for c in phrase:  \n",
        "    if c in map:\n",
        "      res.append(c)\n",
        "    else:\n",
        "      print_log(LTRchar+\"Skipping char not in predefined map\\n ( \"+c+\" )\\nin sentences:\\n\"+phrase+'\\n')      \n",
        "  return \"\".join(res)\n",
        "\n",
        "def remove_chars_not_in_JA_map(ja):\n",
        "  return remove_chars_not_in_map(ja,heb_arab_maping)\n",
        "\n",
        "extended_arab_chars=list(arab_heb_maping.keys())\n",
        "extended_arab_chars+=tanween\n",
        "extended_arab_chars.append(shada)\n",
        "extended_arab_chars.append(hamza_on_line)\n",
        "\n",
        "def remove_chars_not_in_arab_map(arr):\n",
        "  return remove_chars_not_in_map(arr,extended_arab_chars)\n",
        "\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n",
        "\n",
        "\n",
        "def reverse_simple_map(arr_str):\n",
        "  ja_str=[]\n",
        "  for c in arr_str:\n",
        "    if c in arab_heb_maping:\n",
        "      ja_str.append(arab_heb_maping[c])\n",
        "    #else:\n",
        "      #print_log(\"reverse_simple_map: char ( \"+c+\" ) not in letter mapping and will be skiped\")\n",
        "      #print_log(arr_str)\n",
        "  return \"\".join(ja_str)"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arab_heb_maping {'ا': 'א', 'إ': 'א', 'آ': 'א', 'ٱ': 'א', 'أ': 'א', 'ب': 'ב', 'غ': 'ג', 'ج': \"ג'\", 'د': 'ד', 'ذ': \"ד'\", 'ه': 'ה', 'ة': \"ה'\", 'و': 'ו', 'ؤ': 'ו', 'ز': 'ז', 'ح': 'ח', 'ط': 'ט', 'ظ': \"ט'\", 'ي': 'י', 'ى': 'י', 'ئ': 'י', 'ك': 'כ', 'خ': \"כ'\", 'ل': 'ל', 'م': 'מ', 'ن': 'נ', 'س': 'ס', 'ع': 'ע', 'ف': 'פ', 'ص': 'צ', 'ض': \"צ'\", 'ق': 'ק', 'ر': 'ר', 'ش': 'ש', 'ت': 'ת', 'ث': \"ת'\", '.': '.', 'H': 'H', ',': ',', '?': '?', ':': ':', ';': ';', '[': '[', ']': ']', '(': '(', ')': ')', '!': '!', '-': '-', '\"': '\"', ' ': ' ', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', \"'\": \"'\"}\n",
            "length: 61\n",
            "heb_arab_maping {'א': 'ا', 'ב': 'ب', 'ג': 'غ', \"ג'\": 'ج', 'ד': 'د', \"ד'\": 'ذ', 'ה': 'ه', \"ה'\": 'ة', 'ו': 'و', 'ז': 'ز', 'ח': 'ح', 'ט': 'ط', \"ט'\": 'ظ', 'י': 'ي', 'כ': 'ك', \"כ'\": 'خ', 'ל': 'ل', 'מ': 'م', 'נ': 'ن', 'ס': 'س', 'ע': 'ع', 'פ': 'ف', 'צ': 'ص', \"צ'\": 'ض', 'ק': 'ق', 'ר': 'ر', 'ש': 'ش', 'ת': 'ت', \"ת'\": 'ث', '.': '.', 'H': 'H', ',': ',', '?': '?', ':': ':', ';': ';', '[': '[', ']': ']', '(': '(', ')': ')', '!': '!', '-': '-', '\"': '\"', ' ': ' ', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', \"'\": \"'\"}\n",
            "length: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtRTahEBmLU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DATA PATHS\"\n",
        "\n",
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "kfir_kuzari_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test.txt\"\n",
        "kfir_rasag_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test.txt\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31",
        "colab_type": "text"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"HELPERS\"\n",
        "\n",
        "##HELPERS\n",
        "\n",
        "\n",
        "# def view_data(data):\n",
        "#   for i,j,l1,l2 in data.take(3):\n",
        "#     print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[0],l1[0])),\" | \",decode_arr(j[0],l2[0]))\n",
        "\n",
        "def view_data(data):\n",
        "  print_log(\"=\"*200)\n",
        "  for i,j,l1,l2 in data.take(1):\n",
        "    for t in range(5):\n",
        "      print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[t],l1[t])),\" | \",decode_arr(j[t],l2[t]))\n",
        "  print_log(\"=\"*200)\n",
        "\n",
        "\n",
        "def clear_blank(s):\n",
        "  return s.replace(BLANK,\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"PREPROCESS SENTENCES\"\n",
        "\n",
        "\n",
        "def normalize_unicode(s):\n",
        "    s = s.strip()\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "#ARABIC:\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def replace_arab_style_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "#CHECK\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "#JUDEO-ARABIC\n",
        "# def preprocess_hebrew(w):\n",
        "#     w = normalize_unicode(w.strip())\n",
        "#     w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "#     w = w.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "#     return w\n",
        "\n",
        "\n",
        "def dropout(ja_str,keep):\n",
        "  res=[]\n",
        "  for c in ja_str:\n",
        "    if c==\" \":\n",
        "      res.append(c)\n",
        "    elif (np.random.binomial(1,keep)):\n",
        "      res.append(c)\n",
        "    else:\n",
        "      res.append(BLANK)\n",
        "  return \"\".join(res)\n",
        "\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "\n",
        "def undouble_hebrew(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq",
        "colab_type": "text"
      },
      "source": [
        "##languageIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print_log(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print_log(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print_log(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print_log(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Ch9lK2o-Jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "d7f63bb5-86c8-452f-eab7-512b06a6f9de"
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys()))\n",
        "targ_lang = LanguageIndex(\"\".join(extended_arab_chars))\n",
        "\n",
        "\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print_log(len(idx))\n",
        "      if leng==-1:\n",
        "       # print_log(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "      return result\n",
        "\n",
        "def decode_JA(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,inp_lang.idx2char,leng)\n",
        "\n",
        "def decode_arr(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,targ_lang.idx2char,leng)\n",
        "\n",
        "\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print_log_screen(\"VECTORIZE: char not in dict - need to call preprocess_lines before calling produce_dataset \")      \n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def encode_JA(ja):\n",
        "  return vectorize(ja,inp_lang.char2idx)\n",
        "\n",
        "def encode_arr(arr):\n",
        "  return vectorize(arr,targ_lang.char2idx)\n",
        "\n"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab:  [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'H', '[', ']', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'כ', 'ל', 'מ', 'נ', 'ס', 'ע', 'פ', 'צ', 'ק', 'ר', 'ש', 'ת']\n",
            "len(self.vocab) 47\n",
            "len(self.char2idx) 48\n",
            "47 _\n",
            "vocab:  [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'H', '[', ']', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ً', 'ٌ', 'ٍ', 'ّ', 'ٱ']\n",
            "len(self.vocab) 66\n",
            "len(self.char2idx) 67\n",
            "66 _\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7wKGxCzxuE",
        "colab_type": "text"
      },
      "source": [
        "##FUCNTION FOR GEN DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xjn9m6nyHv7",
        "colab_type": "text"
      },
      "source": [
        "###SPLIT TEST TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62bZRaH7L8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST TRAIN SPLIT\"\n",
        "\n",
        "def split_test_train(input_file,output_train,output_test,test_percent=0.2):\n",
        "  with open(input_file, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')  \n",
        "  text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  test_size=math.floor(num_of_lines*test_percent)\n",
        "  idx=[0]*test_size + [1]*(num_of_lines-test_size)\n",
        "  random.shuffle(idx)\n",
        "  print(idx)\n",
        "  \n",
        "  f_train=open(output_train,'w+')\n",
        "  f_test=open(output_test,'w+')\n",
        "  \n",
        "  train = []\n",
        "  test = []\n",
        "  for a,i in zip(lines,idx):\n",
        "    if i:\n",
        "      train.append(a)\n",
        "      f_train.write(a+'\\n')\n",
        "    else:\n",
        "      test.append(a)\n",
        "      f_test.write(a+'\\n')\n",
        "  \n",
        "  f_train.close()\n",
        "  f_test.close()\n",
        "\n",
        "  print(len(train),train[0:10])\n",
        "  print(len(test),test[0:10])\n",
        "\n",
        "#split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58vNbHqdTly_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "ba0de84c-fb33-4308-f641-fa1d4ffeeefb"
      },
      "source": [
        "split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")\n",
        "split_test_train(haemunot,haemunot+\".train.txt\",haemunot+\".test.txt\")"
      ],
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
            "8739 [\"עלי מכ'אלפינא מן אלפלאספה\\tعلى مُخالفينا من الفلاسفة\", \"אלכ'וארג' אלד'ין יכ'אלפון\\tالخوارج الذين يخالفون\", \"אלג'מהור . תד'כרת מה\\tالجمهور . تذكّرت ما\", \"אלדאכ'ל פי דין אליהוד\\tالداخل في دين اليهود\", \"כתאב אלתואריך' , אנה\\tكتاب التواريخ , أنه\", \": אן ניתך מרצ'יהֿ ענד\\t: إنَّ نيّتك مرضيّة عند\", \"אלתעבד פי דין אלכ'זר\\tالتعبّد في دين الخَزَر\", \", חתי אנה כאן יכ'דם כ'דמה\\t, حتّى أنه كان يخدم خِدمة\", 'אלהיכל ואלקראבין בנפסה\\tالهيكل والقرابين بنفسه', \"בניה צאפיה כ'אלצה . פכלמא\\tبنيّة صافية خالصة . فكلّما\"]\n",
            "2184 [\"סילת עמא ענדי מן אלאחתג'אג'\\tسُئِلْتُ عمّا عنديَ من الاحتجاج\", \"ואהל אלאדיאן ת'ם עלי\\tوأهل الأديان ثمّ على\", \"קד סמעתה מן חג'ג' אלחבר\\tقد سمعتهُ من حجج الحَبْرِ\", \", אלד'י כאן ענד מלך אלכ'זר\\t, الذي كان عند مَلِك الخَزَرِ\", \". , עלי מא שהד וג'א פי\\t. , على ما شُهِد وجاء في\", 'תכרר עליה רויא , כאן\\tتكرّر عليه رؤيا , كأنَّ', \"מלאכא יכ'אטבה ויקול לה\\tملاكاً يخاطبه ويقول له\", \"אללה לכן עמלך ג'יר מרצ'י\\tالله لكن عملك غير مرضيّ\", \". וכאן יג'תהד ג'דא פי\\t. وكان يجتهد جدّاً في\", \"אג'תהד פי תלך אלאעמאל\\tاجتهد في تلك الأعمال\"]\n",
            "[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "8287 [\"אלמכ'תאר פי אלאמאנאת\\t(المختار في) الأمانات\", 'ואלאעתקאדאת אפתתח מולפה\\tوالاعتقادات افتتح مؤلَّفَه', 'באן קאל תבארך אללה אלאה\\tبأنْ قالَ تبارك الله إله', 'אסראיל אלחקיק במעני אלחק\\tإسرائيل الحقيق بمعنى الحق', 'אלמבין אלמחקק ללנאטקין\\tالمبين المحقِّق للناطقين', \"וג'דאן אנפסהם חקא יקינא\\tوجدان أنفسهم حقًا يقينًا\", \"פוג'דוא בהא מחסוסאתהם\\tفوجدوا بها محسوساتهم\", \"ארתפעת בד'לך ענהם אלשבה\\tارتفعت بذلك عنهم الشُبَه\", 'להם אלדלאיל , וצפת להם\\tلهم الدلائل , وصفت لهم', 'אלבראהין , ותסבח פוק\\tالبراهين , وتسبَّح فوق']\n",
            "2071 [\"וג'דאנא צחיחא פעלמוא\\tوجدانًا صحيحًا فعلموا\", 'בהא מעלומאתהם עלמא צאדקא\\tبها معلوماتهم علمًا صادقًا', \"וזאלת מעה אלשכוך פכ'לצת\\tوزالت معه الشكوك فخلصت\", 'חתי יתם וצולהם אלי אלמטלוב\\tحتّى يتمّ وصولهم إلى المطلوب', \"חתי את'בתוהא חקאיק עלי\\tحتّى أثبتوها حقائق على\", 'לי מא בה אצל אלי טאעתה\\tلي ما به أصل إلى طاعته', 'ויצל בה קאצדה אלי אלעדל\\tويصل به قاصده إلى العدل', 'עלי אלמחסוסאת וכאנת אלאשיא\\tعلى المحسوسات وكانت الأشياء', \"סבבין , וד'לך ، אמא לקלהֿ\\tسببين , وذلك ، إمّا لقلّة\", \"וארכ'י באל פלד'לך לא\\tوأرخى بال فلذلك لا\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_YqhOHyNUs",
        "colab_type": "text"
      },
      "source": [
        "###load_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"LOAD_LINES\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  print_log_screen(\"loading text: \"+input_file)\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    #text=text.replace('ֿ',\"'\")   ##allread doing it inside preprocess_hebrew()\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  print_log(\"first lines:\",lines[0:100])\n",
        "  print_log_screen(\"len(lines)\", len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyI8c-qU-DEe",
        "colab_type": "text"
      },
      "source": [
        "###preprocess_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro14w9OP-Dt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_lines(ja_arr_lines):\n",
        "  res=[]\n",
        "  for l in ja_arr_lines:\n",
        "    ja,arr=l.split('\\t')\n",
        "\n",
        "    ja=replace_arab_style_punctuation(ja)\n",
        "    ja=clear_blank(ja) #acctually should be taken care of by remove_chars_not_in_map ...\n",
        "    ja = normalize_unicode(ja.strip())\n",
        "    ja = ja.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    ja = ja.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "\n",
        "    ja=remove_chars_not_in_JA_map(ja)\n",
        "\n",
        "    \n",
        "    arr=remove_arab_nikud(arr)\n",
        "    arr=standard_nunization(arr)\n",
        "    arr=clear_blank(replace_arab_style_punctuation(arr))\n",
        "    arr=normalize_unicode(arr)     \n",
        "    arr=remove_chars_not_in_arab_map(arr)\n",
        "\n",
        "    res.append(ja+'\\t'+arr)\n",
        "  return res\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jWF3taWySyb",
        "colab_type": "text"
      },
      "source": [
        "###create_parralele_phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvF0XWGTzZ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Takes a file of <heb, arab> phrases separated by tab\n",
        "# Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_parralel_phrases(lines,keep=1):\n",
        "    phrase_pairs=[]\n",
        "    for l in lines:\n",
        "      heb,arr=l.split('\\t')\n",
        "      \n",
        "      # heb=clear_blank(replace_arab_style_punctuation(heb)) #TODO needed?\n",
        "      # heb=preprocess_hebrew(heb)\n",
        "      heb=dropout(heb,keep)\n",
        "      heb=double_hebrew(heb)      \n",
        "\n",
        "      # arr=remove_arab_nikud(arr)\n",
        "      # arr=standard_nunization(arr)\n",
        "      # arr=clear_blank(replace_arab_style_punctuation(arr))\n",
        "      # arr=normalize_unicode(arr) \n",
        "      \n",
        "      phrase_pairs.append([heb,arr])        \n",
        "    return phrase_pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT",
        "colab_type": "text"
      },
      "source": [
        "###produce_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuiTkrWRLYG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"produce_dataset\"\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def produce_dataset(parallel_phrase,to_shuffle=False):\n",
        "  \n",
        "    input_tensor = [encode_JA(heb) for heb, arr in parallel_phrase]\n",
        "    input_lengths=[len(heb) for heb,arr in parallel_phrase]\n",
        "  \n",
        "    target_tensor = [encode_arr(arr) for heb, arr in parallel_phrase]\n",
        "    target_lengths = [len(arr)  for heb,arr in parallel_phrase]\n",
        "  \n",
        "\n",
        "    print_log(\"VECTORIZE EXAMPLE\")\n",
        "    print_log(LTRchar,parallel_phrase[0])\n",
        "    print_log(input_lengths[0])\n",
        "    print_log(target_lengths[0])\n",
        "    print_log(input_tensor[0])\n",
        "    print_log(target_tensor[0])\n",
        "    print_log(\"\\n\")\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                 value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    print_log(len(input_tensor), \n",
        "        len(target_tensor))      \n",
        "    BUFFER_SIZE = len(input_tensor)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, \n",
        "                                                  target_tensor,\n",
        "                                                  input_lengths,\n",
        "                                                  target_lengths))\n",
        "    if to_shuffle:\n",
        "      dataset=dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "                                                  \n",
        "    dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset_double\n",
        "    \n",
        "    return dataset_double\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF create_data_tensors\"\n",
        "\n",
        "# def max_length(tensor):\n",
        "#     return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# def create_data_tensors(pairs):\n",
        "  \n",
        "#     input_tensor = [encode_JA(heb) for heb, arr in pairs]\n",
        "#     input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "  \n",
        "#     target_tensor = [encode_arr(arr) for heb, arr in pairs]\n",
        "#     target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "  \n",
        "\n",
        "#     print_log()\n",
        "#     print_log(LTRchar,pairs[0])\n",
        "#     print_log(input_lenghts[0])\n",
        "#     print_log(target_lengths[0])\n",
        "#     print_log(input_tensor[0])\n",
        "#     print_log(target_tensor[0])\n",
        "\n",
        "#     # Padding the input and output tensor to the maximum length\n",
        "#     max=max_length(input_tensor)\n",
        "#     if max>70:\n",
        "#       max=70\n",
        "#     input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "#                                                                  maxlen=max,\n",
        "#                                                                  padding='post',\n",
        "#                                                                   value=inp_lang.char2idx[BLANK])\n",
        "#     max=max_length(target_tensor)\n",
        "#     if max>70:\n",
        "#       max=70    \n",
        "#     target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "#                                                                   maxlen=max,\n",
        "#                                                                   padding='post',\n",
        "#                                                                   value=targ_lang.char2idx[BLANK])\n",
        "#     return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN",
        "colab_type": "text"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF GEN DATA\"\n",
        "\n",
        "# def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths,_test_size=0.2):  \n",
        "#   input_tensor_train, input_tensor_val, \\\n",
        "#   target_tensor_train, target_tensor_val, \\\n",
        "#   input_lengths_train, input_lengths_val, \\\n",
        "#   target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "#                                                               target_tensor,\n",
        "#                                                               input_lenghts,\n",
        "#                                                               target_lengths, test_size=_test_size)\n",
        "  \n",
        "#   print_log(len(input_tensor_train), \n",
        "#         len(target_tensor_train), \n",
        "#         len(input_tensor_val), \n",
        "#         len(target_tensor_val))\n",
        "  \n",
        "#   BUFFER_SIZE = len(input_tensor_train)\n",
        "  \n",
        "#   dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "#                                                 target_tensor_train,\n",
        "#                                                 input_lengths_train,\n",
        "#                                                 target_lengths_train)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "#   test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "#                                                     target_tensor_val,\n",
        "#                                                     input_lengths_val,\n",
        "#                                                     target_lengths_val)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)                                                  \n",
        "  \n",
        "#   dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   return dataset_double,test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0elYUMb8Ls",
        "colab_type": "text"
      },
      "source": [
        "##activate gen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izFnzJAMynm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "152b664f-144b-4fe1-b153-9720e77023b1"
      },
      "source": [
        "CELL_NAME=\"LOAD DATASET NEW\"\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_train=preprocess_lines(\n",
        "    load_lines(hakuzari+\".train.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "train_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_train,0.9),to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"train_dataset_double_kuzari\")\n",
        "view_data(train_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_test=preprocess_lines(\n",
        "    load_lines(hakuzari+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_kuzari\")\n",
        "view_data(test_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "rasag_lines_test=preprocess_lines(\n",
        "    load_lines(haemunot+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_rasag= produce_dataset(create_parralel_phrases(rasag_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_rasag\")\n",
        "view_data(test_dataset_double_rasag)\n",
        "\n",
        " "
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.train.txt\n",
            "first lines: [\"עלי מכ'אלפינא מן אלפלאספה\\tعلى مُخالفينا من الفلاسفة\", \"אלכ'וארג' אלד'ין יכ'אלפון\\tالخوارج الذين يخالفون\", \"אלג'מהור . תד'כרת מה\\tالجمهور . تذكّرت ما\", \"אלדאכ'ל פי דין אליהוד\\tالداخل في دين اليهود\", \"כתאב אלתואריך' , אנה\\tكتاب التواريخ , أنه\", \": אן ניתך מרצ'יהֿ ענד\\t: إنَّ نيّتك مرضيّة عند\", \"אלתעבד פי דין אלכ'זר\\tالتعبّد في دين الخَزَر\", \", חתי אנה כאן יכ'דם כ'דמה\\t, حتّى أنه كان يخدم خِدمة\", 'אלהיכל ואלקראבין בנפסה\\tالهيكل والقرابين بنفسه', \"בניה צאפיה כ'אלצה . פכלמא\\tبنيّة صافية خالصة . فكلّما\", \"יקול לה : ניתך מרצ'יה\\tيقول له : نيّتك مرضيّة\", \"ועמלך גיר מרצ'י . פסבב\\tوعملك غير مرضيّ . فسبّب\", \"לה ד'לך אלבחת' ען אלאדיאן\\tله ذلك البحث عن الأديان\", \"ואלנחל ותהוד אכ'רא הוא\\tوالنحل وتهوّد آخرا هو\", \"וג'מהור אלכ'זר . וכאן\\tوجمهور الخَزَر . وكان\", \"מן חג'ג' אלחבר מא אקנעני\\tمن حجج الحَبْر ما أقنعني\", \"אן את'בת ד'לך אלאחתג'אג'\\tأن أثبت ذلك الاحتجاج\", 'כמא וקע H H . קיל אן\\tكما وقع H H . قيل إنَّ', \"מלך אלכ'זר למא ראי פי\\tمَلِك الخَزَر لمّا رأى في\", \"ענד אללה ועמלה גיר מרצ'י\\tعند الله وعمله غير مرضيّ\", ', ואמרה פי אלנום אן יטלב\\t, وأمره في النوم أن يطلب', \"אלעמל אלמרצ'י ענד אללה\\tالعمل المرضيّ عند الله\", ', סאל פילסופא ען מעתקדה\\t, سأل فيلسوفاً عن معتقده', '. פקאל לה אלפילסוף :\\t. فقال له الفيلسوف :', \"ליס ענד אללה רצ'י ולא\\tليس عند الله رضا ولا\", \"בג'ץ' . לאנה מנזה ען\\tبغض . لأنّه مُنزَّه عن\", \"אלאראדאת ואלאג'ראץ' .\\tالإرادات والأغراض .\", \"לאן אלג'רץ' ידל עלי נקצאן\\tلأنَّ الغرض يدلّ على نقصان\", \"אלמג'רץ' . ואן תמאם ג'רצ'ה\\tالمُغرض . وأنَّ تمام غرضه\", 'כמאל לה , ומהמא לם יתם\\tكمال له , ومهما لم يتمّ', \"פהו נקצאן . וכד'לך הו\\tفهو نقصان . وكذلك هو\", \"עלם אלג'זאיאת . לאנהא\\tعلم الجزئيات . لأنّها\", \"מתג'ירה מע אלאחיאן .\\tمتغيّرة مع الأحيان .\", 'וליס פי עלם אללה תגיר\\tوليس في علم الله تغيّر', \". פהו לא ידריך , פצ'לא\\t. فهو لا يدريكَ , فضلاً\", 'ען אן ידרי ניתך ואעמאלך\\tعن أن يدري نيّتكَ وأعمالكَ', \", פצ'לא ען אן יסמע צלאתך\\t, فضلاً عن أن يسمع صلاتكَ\", \"קאלת אלפלאספה אנה כ'לקך\\tقالت الفلاسفة إنّه خلقكَ\", \"פעלי אלמג'אז , לאנה עלהֿ\\tفعلى المجاز , لأنّه علّة\", \"אלעלל פי כ'לקהֿ כל מכ'לוק\\tالعلل في خلقة كل مخلوق\", ', לא לאנה מקצוד מן קבלה\\t, لا لأنّه مقصود من قِبَلِهِ', \". נעם , ולא כ'לק קט אנסאנא\\t. نعم , ولا خلق قطّ إنساناً\", 'לאן אלעאלם קדים , לם\\tلأنَّ العالم قديم , لم', 'יזל ינשא אלאנסאן מן אנסאן\\tيزلْ ينشأ الإنسان من إنسان', 'קבלה , תתרכב פיה צור\\tقبله , تتركّب فيه صُوَر', \"וכ'לק [ו]אכ'לאק מן אביה\\tوخُلْق وأخلاق من أبيه\", \"אלאהויה ואלבלדאן ואלאגד'יה\\tالأهوية والبلدان والأغذية\", 'ואלמיאה , מע קוי אלאפלאך\\tوالمياه , مع قوى الأفلاك', \"ואלדרארי ואלברוג' באלנסב\\tوالدَّرَارِيّ والبروج بالنسب\", 'אלחאצלה מנהא . ואלכל\\tالحاصلة منها . والكلّ', \"ראג'ע אלא אלסבב אלאול\\tراجع إلى السبب الأوّل\", \", לא ען גרץ' לה , לכן\\t, لا عن غرض له , لكن\", \"פיץ' פאץ' ענה סבב ת'אן\\tفيض فاض عنه سبب ثانٍ\", ', ותלאזמת אלאסבאב ואלמסבבאת\\t, وتلازمت الأسباب والمسبّبات', 'ותסלסלת , כמא תראהא ,\\tوتسلسلت , كما تراها ,', 'ותלאזמהא קדים כמא אן\\tوتلازمها قديم كما أنَّ', 'אלסבב אלאול קדים , לא\\tالسبب الأوّل قديم , لا', \"אול לה . פלכל שכ'ץ מן\\tأوّل له . فلكلّ شخص من\", \"אשכ'אץ אלדניא אסבאב בהא\\tأشخاص الدنيا أسباب بها\", \"יתם . פשכ'ץ תכאמלת אסבאבה\\tيتمّ . فشخص تكاملت أسبابه\", \"פג'א כאמלא , ושכ'ץ נקצת\\tفجاء كاملاً , وشخص نقصت\", \"אסבאבה פג'א נאקצא , כאלחבשי\\tأسبابه فجاء ناقصاً , كالحبشيّ\", \"אלד'י לם יהיא לאכת'ר\\tالذي لم يُهيَّأ لأكثر\", 'מן קבול צורהֿ אלאנסאן\\tمن قبول صورة الإنسان', 'ואלנטק עלי אנקץ מא ימכן\\tوالنطق على أنقص ما يمكن', \". פאלפילסוף אלד'י תהיאת\\t. والفيلسوف الذي تهيأت\", \"אלפצ'איל אלכ'לקיה ואלכ'לקיה\\tالفضائل الخلَقية والخُلقية\", 'ואלעלמיה ואלעמליה ולם\\tوالعلمية والعملية ولم', \"לכן הד'ה אלכמאלאת באלקוה\\tلكن هذه الكمالات بالقوّة\", \", יחתאג' פי אכ'ראג'הא\\t, يُحتاج في إخراجها\", \"ואלתאדיב , פתט'הר אלהיאת\\tوالتأديب , فتظهر الهيئات\", 'עלי מא היית לה מן כמאל\\tعلى ما هُيّئَت له من كمال', 'ונקצאן ותוסטאת אלי מא\\tونقصان وتوسّطات إلى ما', 'לא נהאיה לה . פאלכאמל\\tلا نهاية له . فالكامل', 'יתצל בה מן אלנמט אלאלאהי\\tيتّصل به من النمط الإلهيّ', 'נור , יסמי אלעקל אלפעאל\\tنور , يسمَّى العقل الفعّال', \"אתצאל אתחאד חתי ירי אלשכ'ץ\\tاتّصال اتّحاد حتّى يرى الشخص\", \"אנה הו ד'לך אלעקל אלפעאל\\tأنّه هو ذلك العقل الفعّال\", \", לא תג'איר בינהמא ,\\t, لا تغايُر بينهما ,\", \"ותציר אלאתה , אעני אעצ'א\\tوتصير آلاته , أعني أعضاء\", \"ד'לך אלשכ'ץ , לא תתצרף\\tذلك الشخص , لا تتصرّف\", 'אלא פי אכמל אלאעמאל ופי\\tإلاّ في أكمل الأعمال وفي', \"אופק אלאוקאת , ועלי אפצ'ל\\tأوفق الأوقات , وعلى أفضل\", 'אלחאלאת , וכאנהא אלאת\\tالحالات , وكأنّها آلات', 'ללעקל אלפעאל לא ללעקל\\tللعقل الفعّال لا للعقل', \"אלהיולאני אלמנפעל אלד'י\\tالهيولانيّ المنفعل الذي\", 'כאן מן קבל יצרפהא , פכאן\\tكان من قبل يصرّفها , فكان', \". והד'א יציב דאימא .\\t. وهذا يصيب دائماً .\", \"והד'א אלדרג'ה אלגאיה\\tوهذه الدرجة الغاية\", \"אלקצוי אלמרג'וה ללאנסאן\\tالقُصوى المرجوّة للإنسان\", 'אלכאמל , בעד אן תציר\\tالكامل , بعد أن تصير', 'נפסה מטהרה מן אלשכוך\\tنفسه مطهَّرة من الشكوك', ', מחצלה ללעלום עלי חקאיקהא\\t, محصّلة للعلوم على حقائقها', ', פתציר כאנהא מלך , פתציר\\t, فتصير كأنّها مَلَك , فتصير', \"אלמפארקה ללאג'סאד , והי\\tالمفارقة للأجساد , وهي\", 'רתבה אלעקל אלפעאל והו\\tرتبة العقل الفعّال وهو', 'מלך רתבתה דון אלמלך אלמוכל\\tمَلَك رتبته دون المَلَك الموكَّل', 'בפלך אלקמר . והי עקול\\tبفلك القمر . وهي عقول', \"מג'רדה ען אלמואד קדימה\\tمجرَّدة عن المَوادّ قديمة\", \"מע אלסבב אלאול , לא תכ'אף\\tمع السبب الأوّل , لا تخاف\"]\n",
            "len(lines) 8739\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "إنّ ذلك القربان والـ\"طعام\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ׁ )\n",
            "in sentences:\n",
            "קאל אלחבר : אנ ישׁראל\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ׂ )\n",
            "in sentences:\n",
            "בלג עציאנ ישׂראל אלי\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            ", والتحفّظ , والـ\"خلط\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "كالـ\"ختان\" مع \"السبت\" , و\"الفصح\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "بالـ\"ملء\" و\"الترديد\" .\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الحقيقة فلا ظهور للـ\"مجد\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "وللـ\"ملكوت\" إلاّ على أوليائه\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "للـ\"عجول\" , فأصحاب \"العجل\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الربيع بحسب حسابهم بالـ\"دورة\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( … )\n",
            "in sentences:\n",
            ". [……] קאל אלכ'זרי :\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( … )\n",
            "in sentences:\n",
            ". [……] קאל אלכ'זרי :\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "متعلّق بالـ\"جلد\" . ثمّ انتقل\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "يردّ من ذلك بالـ\"توبة\"\n",
            "\n",
            "\n",
            "‫ [\"עעלליי ממככ''אאללפפייננאא ממננ אאללפפללאאסספפ__\", 'على مخالفينا من الفلاسفة']\n",
            "47\n",
            "24\n",
            "[40, 40, 36, 36, 34, 34, 0, 37, 37, 35, 35, 3, 3, 25, 25, 36, 36, 41, 41, 34, 34, 38, 38, 25, 25, 0, 37, 37, 38, 38, 0, 25, 25, 36, 36, 41, 41, 36, 36, 25, 25, 39, 39, 41, 41, 47, 47]\n",
            "[49, 54, 59, 0, 55, 38, 31, 54, 51, 60, 56, 31, 0, 55, 56, 0, 31, 54, 51, 54, 31, 43, 51, 33]\n",
            "8739 8739\n",
            "train_dataset_double_kuzari\n",
            "========================================================================================================================================================================================================\n",
            "‫ עלי מכ'אלפינא מנ אלפלאספ_  |  على مخالفينا من الفلاسفة\n",
            "‫ אלכ'וארג' א_ד'ינ יכ'_לפונ  |  الخوارج الذين يخالفون\n",
            "‫ א_ג'מהור . תד'כרת מה  |  الجمهور . تذكّرت ما\n",
            "‫ אלדאכ'_ פ_ דינ אל_הוד  |  الداخل في دين اليهود\n",
            "‫ כתאב אלת_ארי_' , אנ_  |  كتاب التواريخ , أنه\n",
            "========================================================================================================================================================================================================\n",
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.test.txt\n",
            "first lines: [\"סילת עמא ענדי מן אלאחתג'אג'\\tسُئِلْتُ عمّا عنديَ من الاحتجاج\", \"ואהל אלאדיאן ת'ם עלי\\tوأهل الأديان ثمّ على\", \"קד סמעתה מן חג'ג' אלחבר\\tقد سمعتهُ من حجج الحَبْرِ\", \", אלד'י כאן ענד מלך אלכ'זר\\t, الذي كان عند مَلِك الخَزَرِ\", \". , עלי מא שהד וג'א פי\\t. , على ما شُهِد وجاء في\", 'תכרר עליה רויא , כאן\\tتكرّر عليه رؤيا , كأنَّ', \"מלאכא יכ'אטבה ויקול לה\\tملاكاً يخاطبه ويقول له\", \"אללה לכן עמלך ג'יר מרצ'י\\tالله لكن عملك غير مرضيّ\", \". וכאן יג'תהד ג'דא פי\\t. وكان يجتهد جدّاً في\", \"אג'תהד פי תלך אלאעמאל\\tاجتهد في تلك الأعمال\", \", ג'א אלמלאך פי אלליל\\t, جاءه الملاك في الليل\", ', וטאבק אעתקאדי , פראית\\t, وطابق اعتقادي , فرأيت', \"רויאה , אן ניתה מרצ'יה\\tرؤياه , أنَّ نيّته مرضيّة\", 'מנזה ענד אלפלאספה ען\\tمُنزَّه عند الفلاسفة عن', 'וירא חרכאתך . נעם , ואן\\tويرى حركاتكَ . نعم , وإنْ', 'ואמה וקראבתה , וכיפיאת\\tوأمّه وقرابته , وكيفيّات', \", ת'ם ת'ואלת' ורואבע\\t, ثمّ ثوالث وروابع\", 'לה אסתעדאדאת יקבל בהא\\tله استعدادات يقبل بها', 'ינקצה שי מן אלכמאל .\\tينقصه شيء من الكمال .', 'אלי אלפעל אלי אלתעלים\\tإلى الفعل إلى التعليم', ', יתצל בה עקלה אלמנפעל\\t, يتّصل به عقله المنفعل', \"יציב מרה ויכ'טי מראת\\tيُصيب مرّة ويُخطئ مرّات\", 'באדון רתבה מן אלמלכותיה\\tبأَدْون رتبة من المَلَكوتية', \"אלאנסאן אלכאמל וד'לך\\tالإنسان الكامل وذلك\", 'ואחדא , וטאבת נפסה פי\\tواحداً , وطابت نفسه في', \"אלחיאה , אד' צאר פי זמרה\\tالحياة , إذ صار في زُمرة\", 'הרמס ואסקלאביוס וסקראט\\tهرمس وأسقلابيوس وسقراط', ', בל הו והם וכל מן כאן\\t, بل هو وهم وكلّ من كان', 'בחקאיק אלאמור , ליציר\\tبحقائق الأمور , ليصير', 'עקלך פעלא לא מנפעלא ,\\tعقلك فاعلاً لا مُنفَعِلاً ,', \"ואלכ'שוע וכל כ'לק פאצ'ל\\tوالخشوع وكلّ خُلْق فاضل\", ', אן [כנת] מקבולא מנהם\\t, إن كنت مقبولاً منهم', 'עלי חקאיקהא , פתצאדף\\tعلى حقائقها , فتصادف', ': אן כלאמך למקנע , לכנה\\t: إنَّ كلامك لمقنع , لكنّه', 'אעלם מן נפסי אני צאפי\\tأعلم من نفسي أنّي صافي', \"מא מרצ'יא בד'אתה לא בחסב\\tما مرضياً بذاته لا بحسب\", 'ואחד מנהמא קד אצפי ניתה\\tواحد منهما قد أصفى نيّته', 'וכל ואחד מנהמא יעתקד\\tوكلّ واحد منهما يعتقد', \"אן מסירה אלי אלג'נה ואלפרדוס\\tأنَّ مسيره إلى الجنّة والفردَوْس\", \"אעט'ם מן אעתקאדהם אלחד'ת\\tأعظم من اعتقادهم الحَدَث\", 'באלרוחאניאת , ואן יוצף\\tبالروحانيات , وأن يوصف', \"באצפא נפסה . ונגד צ'ד\\tبإصفاء نفسه . ونجد ضدّ\", 'אן ללאמר אלאלאהי וללנפוס\\tأنَّ للأمر الإلهيّ وللنفوس', \"אליהוד פכפי מא ט'הר מן\\tاليهود فكفى ما ظهر من\", 'אלנצארי פסאלה ען עלמה\\tالنصارى فسأله عن علمه', \"מומן באלחדת' ללמכ'לוקאת\\tمؤمن بالحَدَث للمخلوقات\", \". ואן ללה ענאיה באלכ'לק\\t. وأنَّ لله عناية بالخلق\", \"וט'הורא ותגליא לאנביאה\\tوظهوراً وتجلّياً لأنبيائه\", 'ודואמהא ועלאניתהא פי\\tودوامها وعلانيّتها في', \"אלג'מאהיר אלעט'אם . ופי\\tالجماهير العظام . وفي\", 'אלאהותי אלבאטן , נביא\\tلاهوتيّ الباطن , نبيّاً', 'מרסלא פי באטנה , והו\\tمُرسِلاً في باطنه , وهو', '. נומן בה ובחלולה פי\\t. نؤمن به وبحلوله في', 'אלאלאהי יתצל בהם , חתי\\tالإلهيّ يتّصل بهم , حتّى', \"ואלרצ'א עלי אלאפראד אלתאבעין\\tوالرضا على الأفراد التابعين\", \"מדעוון אלי הד'א אלדין\\tمدعوّون إلى هذا الدين\", ', מכלפון אלעמל בה מן\\t, مكلَّفون العمل به من', 'מן אלתוראה אלתי נקראהא\\tمن التوراة التي نقرأها', ', ולא מדפע פי חקיקתהא\\t, ولا مدفع في حقيقتها', \", ולא יג'ד מנדוחה פי\\t, ولا يجد مندوحة في\", \"פי אלכ'ואץ אלגריבה אלתי\\tفي الخواصّ الغريبة التي\", \"לא אג'דני טיב אלנפס לקבול\\tلا أجدني طيّب النفس لقبول\", \"הד'ה אלאמור , לאני טראת\\tهذه الأمور , لأنّي طرأتُ\", \". פקאל : אנא נת'בת אלוחדאניה\\t. فقال : إنّا نُثبت الوحدانية\", 'באן כתאבנא כלאם אללה\\tبأنَّ كتابنا كلام الله', \"במת'ל איה מן איאתה ,\\tبمثل آية من آياته ,\", 'לא מדפע פיהא . ובלאחרי\\tلا مدفع فيها . وبالحَرَى', 'אלערבי . פקאל לה אלעאלם\\tالعربيّ . فقال له العالِمُ', 'אלי אן תקר אן אלאלאה\\tإلى أن تقرّ أنَّ الإله', 'ברואיה ואסנאד , ובאן\\tبرواية وإسناد , وبأن', \"פי אלט'ן אן הנאך תכ'יילא\\tفي الظنّ أنَّ هناك تخييلاً\", ', אעני אלאנסאן , ויכלמה\\t, أعني الإنسان , ويكلّمه', \"מן סכ'ט עליה , ת'ם אלמן\\tمن سخط عليه , ثُمّ المنّ\", 'מסאילהֿ אליהוד , לאנהם\\tمُساءلة اليهود , لأنّهم', '. פקאל לה : אנה מומן\\t. فقال له : أنا مؤمن', \"ומעטיהם ארץ' [אלשאם]\\tومُعطيهم أرض الشأم\", \", בעד תג'ויזהם אלים ואלארדן\\t, بعد تجويزهم اليَمّ والأردن\", 'עאזמא אלא אסאל יהודיא\\tعازماً ألاّ أسأل يهوديّاً', \"אלתי הי חג'הֿ כל ד'י\\tالتي هي حُجّة كلّ ذي\", \"דין ומן אג'להא יתבע אלחק\\tدين ومن أجلها يتبع الحقّ\", 'פי חכמתה ועדלה ? קאל\\tفي حكمته وعدله ? قالَ', 'אלפלאספה ענה , ולן תגדהם\\tالفلاسفة عنه , ولن تجدهم', 'דעאוי , מנהא מא יקדרון\\tدَعاوى , منها ما يقدرون', \"ד'לך ? קאל אלחבר : תאד'ן\\tذلك ? قالَ الحَبْرُ : تأذن\", 'לי פי מקדמאת אקדמהא ,\\tلي في مُقدّمات أُقدّمها ,', ': קדם מקדמאתך חתי אסמע\\t: قدّمْ مقدّماتك حتّى أسمع', \", ינבגי לך תעט'ימה ואלתנויה\\t, ينبغي لك تعظيمه والتنويه\", 'יתצל בך מן עדל אהל בלאדה\\tيتّصل بك من عدل أهل بلاده', \", הל כאן הד'א ילזמך ?\\t, هل كان هذا يُلزمك ?\", ', ואלשך פי אהל אלהנד\\t, والشكّ في أهل الهند', \"אלחבר : פלו ג'אך רסולה\\tالحَبْرُ : فلو جاءك رسوله\", 'פי קצור אלמלוך , בכתאב\\tفي قُصور الملوك , بكتاب', 'צחתך , וסמום לאעדאך ומחארביך\\tصحّتك , وسموم لأعدائك ومحاربيك', 'הל כנת תלתזם טאעתה ?\\tهل كنتَ تلتزم طاعته ?', \"אלאואכ'ר . קאל אלחבר\\tالأواخِر . قالَ الحَبْرُ\", \": ובמת'ל הד'א אג'בתך\\t: وبمثل هذا أجبتك\", \"אד' כאן אמרהם משהורא\\tإذ كان أمرهم مشهوراً\", \", וקצ'י להם אלעג'איב\\t, وقضى لهم العجائب\", \"אמיר אלכ'זר , אד' סאלתני\\tأمير الخزر , إذ سألتني\", 'ילזמני ומעשר בני אסראיל\\tيلزمني ومعشرَ بني إسرائيل']\n",
            "len(lines) 2184\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "للـ\"سكينة\" , H H يفلح ويحرث\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "والـملء يقدر أن يلحّن H\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "إمّا H وإمّا \"هـ\" مثل H\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "فكيف تمكّن بالـ\"كسرة , بل\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الوضع إلى الـ\"كسرة عند\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "القياس , فرضه الله بـ\"العشور\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "H , فإنّهم مؤيّدون بالـ\"سكينة\"\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "أشبّههم بـ\"المتهوّدين\" الذين\n",
            "\n",
            "\n",
            "‫ [\"ססייללתת עעממאא עעננדדיי ממננ אאללאאחחתתגג''אאגג''\", 'سئلت عمّا عندي من الاحتجاج']\n",
            "50\n",
            "26\n",
            "[39, 39, 34, 34, 36, 36, 46, 46, 0, 40, 40, 37, 37, 25, 25, 0, 40, 40, 38, 38, 28, 28, 34, 34, 0, 37, 37, 38, 38, 0, 25, 25, 36, 36, 25, 25, 32, 32, 46, 46, 27, 27, 3, 3, 25, 25, 27, 27, 3, 3]\n",
            "[43, 30, 54, 34, 0, 49, 55, 64, 31, 0, 49, 56, 39, 60, 0, 55, 56, 0, 31, 54, 31, 37, 34, 36, 31, 36]\n",
            "2184 2184\n",
            "test_dataset_double_kuzari\n",
            "========================================================================================================================================================================================================\n",
            "‫ סילת עמא ענדי מנ אלאחתג'אג'  |  سئلت عمّا عندي من الاحتجاج\n",
            "‫ ואהל אלאדיאנ ת'מ עלי  |  وأهل الأديان ثمّ على\n",
            "‫ קד סמעתה מנ חג'ג' אלחבר  |  قد سمعته من حجج الحبر\n",
            "‫ , אלד'י כאנ ענד מלכ אלכ'זר  |  , الذي كان عند ملك الخزر\n",
            "‫ . , עלי מא שהד וג'א פי  |  . , على ما شهد وجاء في\n",
            "========================================================================================================================================================================================================\n",
            "loading text: /gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt.test.txt\n",
            "first lines: [\"וג'דאנא צחיחא פעלמוא\\tوجدانًا صحيحًا فعلموا\", 'בהא מעלומאתהם עלמא צאדקא\\tبها معلوماتهم علمًا صادقًا', \"וזאלת מעה אלשכוך פכ'לצת\\tوزالت معه الشكوك فخلصت\", 'חתי יתם וצולהם אלי אלמטלוב\\tحتّى يتمّ وصولهم إلى المطلوب', \"חתי את'בתוהא חקאיק עלי\\tحتّى أثبتوها حقائق على\", 'לי מא בה אצל אלי טאעתה\\tلي ما به أصل إلى طاعته', 'ויצל בה קאצדה אלי אלעדל\\tويصل به قاصده إلى العدل', 'עלי אלמחסוסאת וכאנת אלאשיא\\tعلى المحسوسات وكانت الأشياء', \"סבבין , וד'לך ، אמא לקלהֿ\\tسببين , وذلك ، إمّا لقلّة\", \"וארכ'י באל פלד'לך לא\\tوأرخى بال فلذلك لا\", \"אלאסתדלאל ، פהו יג'על\\tالاستدلال ، فهو يجعل\", 'בעד מן מטלובה או עלי\\tبُعد من مطلوبه أو على', \"אלאולין מן מד'כורינא\\tالأوّلين من مذكورينا\", \"פאנה יכון חיניד' אבעד\\tفإنه يكون حينئذ أبعد\", 'בל לא כם מן אלדראהם לה\\tبل لا كم مِن الدراهم له', 'אלואזנה ואלכמיה אלמוזונה\\tالوازنة والكمية الموزونة', \"מא יאכ'ד' אלרדי וירד\\tما يأخذ الرديء ويردّ\", 'עלמהם בצנאעהֿ אלנקד קליל\\tعلمهم بصناعة النقد قليل', \"، כקולה : H H H וג'על\\t، كقوله : H H H وجعل\", 'וצברהם בתקדימה H H H\\tوصبرهم بتقديمه H H H', 'עליהם אלשכוך בצברהם עלי\\tعليهم الشُّكوك بصبرهم على', \": H H H H H H H . ואלד'י\\t: H H H H H H H . والذي\", \"אלקול הו אן ראית כת'ירא\\tالقول هو أنْ رأيتُ كثيرًا\", '. ומנהם מן קד וצל אלי\\t. ومنهم من قد وصل إلى', 'יקול אלנבי : H H H H\\tيقول النَّبي : H H H H', 'ומנהם מן אסתעמל נפסה\\tومنهم من استعمل نفسه', \"פירג'ע ויסלך פי אכ'רי\\tفيرجع ويسلكُ في أخرى\", \"יתט'אהרון באלפסאד וקד\\tيتظاهرون بالفساد وقد\", 'טלוא עלי אהל אלחק והם\\tطَلُوا على أهل الحقّ وهم', 'אלשכוך ، וקד גמרתהם אמוא\\tالشُّكوك ، وقد غمرتهم أمواء', \"מן אעמאקהא ולא סאבח יאכ'ד'\\tمن أعماقها ولا سابحٌ يأخذُ\", 'H H H H H H H H H H H\\tH H H H H H H H H H H', 'בנקץ מערפתי ען אלכמאל\\tبنقص معرفتي عن الكمال', 'וכמא קאל אלולי : H H\\tوكما قالَ الوليُّ : H H', 'H H H H H H H H H H H\\tH H H H H H H H H H H', 'לה , או אני סבקתה אלי\\tله , أو أنِّي سبقته إلى', 'כמא קאל : H H H H ، H\\tكما قال : H H H H ، H', \"בד'לך נחו קצדי ויתרך\\tبذلك نحو قصدي ويتركُ\", \"אלתעצב ואלתג'אזף ואלתכ'ליט\\tالتعصُّب والتَّجازف والتَّخليط\", 'תחקקא ואנכשף ען אלשאך\\tتحقيقًا وانكشف عن الشاك', 'שכה וצאר אלמומן תקלידא\\tشكَّه وصار المؤمن تقليدًا', 'אלטאען בתלביס ואסתחא\\tالطاعنُ بتلبيس واستحى', 'אלצאלחון ואלמסתקימון\\tالصَّالحون والمستقيمون', \"תנצלח בואטן אלנאס מת'ל\\tتَصلح بواطنَ النَّاسِ مثل\", \"ט'ואהרהם ، ותכ'לץ צלואתהם\\tظواهرهم ، وتخلص صلواتهم\", 'וכמא קאל אלולי : H H\\tوكما قالَ الوليُّ : H H', \"אלג'ואב האהנא ען ד'לך\\tالجَّوابَ هاهنا عن ذلك\", \"ונקול אן כונהם מכ'לוקין\\tونقول إنَّ كونهم مخلوقين\", \", הו בד'אתה אוג'ב להם\\t, هو بذاته أوجبَ لهم\", \"אלשבה ויכ'לץ להם אלכ'אלץ\\tالشُبَه ويخلص لهم الخالص\", \"צנאיעהם אג'זא אן הם קטעוא\\tصنائعهم أجزاء إنْ هم قطعوا\", 'עמלהם קבל אתמאמהא לם\\tعملهم قبل إتمامها لم', 'אלצנאיע אלתי אנמא תתם\\tالصَّنائع التي إنّما تتم', \"וג'ד אלאצואת משבהה משככה\\tوجد الأصوات مشبَّهة مُشكَّكة\", 'ואלהד ומא אשבההמא פעלם\\tوالهد وما أشباههما فعلم', 'מן צותהא ברהאנא . ויחצל\\tمن صوتها برهانًا . ويحصل', 'אלתי פי נועהא כל עלם\\tالتي في نوعها كلّ عِلْم', 'פי אלראבע אלי צות אלאנסאן\\tفي الرابع إلى صوت الإنسان', 'אלכ\"ב ת\\'ם יפצל מנהא אלחרוף\\tالـ\"22\" ثمّ يفصل منها الحروف', 'ואחד עלי חדה ויחצל פי\\tواحد على حدة ويحصل في', \"כל ואחד ד'ו חרפין או\\tكلّ واحد ذو حرفين أو\", \"ידל עלי אכת'ר מן אלמסמי\\tيدلّ على أكثر من المسمّى\", \"ומא אשבה ד'לך מן כלמתין\\tوما أشبه ذلك من كلمتين\", \"או מא זאד עלי ד'לך פאנה\\tأو ما زاد على ذلك فإنَّه\", 'פי אלמנזלה אלסאבעה עלי\\tفي المنزلة السابعة على', \"אן אלאכ'באר עלי ת'לאת'ה\\tأنَّ الأخبار على ثلاثة\", \"הל כאן כמא אכ'בר ענה\\tهل كان كما أخبر عنه\", \"לה וכ'לץ ואסקט ג'מיע\\tله وخلص وأسقط جميع\", \"מכ'תלטה פלם יזל יצפיהא\\tمختلطة فلم يزل يصفِّيها\", 'אלכתאב : H H ، H H H\\tالكتاب : H H ، H H H', 'H H H H , פדלנא קולהם\\tH H H H , فدلَّنا قولهم', \"יכון פי מא בינהם כ'לף\\tيكون في ما بينهم خلف\", \"אלכ'לק פלא ימכן עלמהם\\tالخلق فلا يمكن علمهم\", 'עלי מא שרחנא ואלמחמודון\\tعلى ما شرحنا والمَحمودون', \"אלפצ'ה מן אלזיף כקולה\\tالفضَّة من الزِّيف كقوله\", 'אנה מעני יקום פי אלנפס\\tإنَّه معنى يقوم في النَّفس', \"זבדהֿ אלנט'ר אכתנפתהא\\tزبدة النَّظر اكتنفتها\", 'אלחקיקי הו אן יעתקד אלשי\\tالحقيقي هو أنْ يعتقد الشيء', \"עלי מא הו אלכת'יר כת'ירא\\tعلى ما هو الكثير كثيرًا\", \"אלד'מים מן ג'על אעתקאדה\\tالذميم مَن جعل اعتقاده\", 'אלאשיא תתבע אעתקאדה ומע\\tالأشياء تتبع اعتقاده ومع', \"וקד בלגוא אלי חצ'יץ'\\tوقد بلغو إلى حضيض\", \"ממלוה מאלא פירי מא אלד'י\\tمملوءة مالاً فيرى ما الذي\", \"עטשאן ואלאסתתאר אד'א\\tعطشان والاستتار إذا\", \"כאן עריאנא פינט'ר חאלה\\tكان عريانًا فينظر حاله\", 'עדוה קד מאת והלך פלא\\tعدوَّه قد مات وهلك فلا', 'ונהיה וועדה וועידה וסאיר\\tونهيه ووعده ووعيده وسائر', \"אלמתפתיין יתג'לד עלי\\tالمتفتِّيين يتجلَّد على\", 'מערוף ונתכלם עליהא במקדאר\\tمعروف ونتكلَّم عليها بمقدار', 'עלם אלעקל פהו מא יקום\\tعِلْم العقل فهو ما يقوم', \"פי עקל אלאנסאן פקט מת'ל\\tفي عقل الإنسان فقط مثل\", \"או מחסוס פאד' לא סביל\\tأو محسوس فإذ لا سبيل\", \". והד'ה אלת'לאת'ה אצול\\t. وهذه الثلاثة أصول\", \"הד'א אלכתאב ונרד עליהם\\tهذا الكتاب ونردُّ عليهم\", \"אד' המא מבניאן עליה .\\tإذ هما مُبنيان عليه .\", \"וג'חד אלת'אני ואלת'אלת'\\tوجحد الثاني والثالث\", \"מן אלאול וכד'לך אלת'אלת'\\tمن الأوَّل وكذلك الثالث\", \"מן אלט'אהר . וקום ג'חדוא\\tمن الظاهر . وقوم جحدوا\", \"הד'א אלעלם אנצ'יאפא עלי\\tهذا العِلم انضيافًا على\", \"מא נפאה כ'צמה ויחתג'\\tما نفاه خصمه ويحتج\"]\n",
            "len(lines) 2071\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الـ\"22\" ثمّ يفصل منها الحروف\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الخالق , [كـ]قوله : H H\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "*فإن كان للحاسّة فهي لا\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "سواه , *[ولأنّ النصبة إنما\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "هي تـ]ـمادّ جسم على آخر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "هي تـ]ـمادّ جسم على آخر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "سلطان *للأبصار على إدراكه\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( פ )\n",
            "in sentences:\n",
            "H H H H פעלו وأنه لا يصنع\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ע )\n",
            "in sentences:\n",
            "H H H H פעלו وأنه لا يصنع\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ל )\n",
            "in sentences:\n",
            "H H H H פעלו وأنه لا يصنع\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ו )\n",
            "in sentences:\n",
            "H H H H פעלו وأنه لا يصنع\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "الفلانية *والنفع في الصناعة\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "والأرض *فبالضرورة شرائعها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "لو *كان إنما شرع لينسخ\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "وحكى بعضها بقول *كأنه\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "يجوز *له أن يقول انكشف\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( מ )\n",
            "in sentences:\n",
            "H : H : H H H H מפניך وكذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( פ )\n",
            "in sentences:\n",
            "H : H : H H H H מפניך وكذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( נ )\n",
            "in sentences:\n",
            "H : H : H H H H מפניך وكذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( י )\n",
            "in sentences:\n",
            "H : H : H H H H מפניך وكذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ך )\n",
            "in sentences:\n",
            "H : H : H H H H מפניך وكذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ע )\n",
            "in sentences:\n",
            "H : H : H H H H H עולם فعينها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ו )\n",
            "in sentences:\n",
            "H : H : H H H H H עולם فعينها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ל )\n",
            "in sentences:\n",
            "H : H : H H H H H עולם فعينها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ם )\n",
            "in sentences:\n",
            "H : H : H H H H H עולם فعينها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "والـשופר وما أشبه ذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ש )\n",
            "in sentences:\n",
            "والـשופר وما أشبه ذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ו )\n",
            "in sentences:\n",
            "والـשופר وما أشبه ذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( פ )\n",
            "in sentences:\n",
            "والـשופר وما أشبه ذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ר )\n",
            "in sentences:\n",
            "والـשופר وما أشبه ذلك\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            ", كلّ ما *كان مقصّراً عن\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "والحجر الصّلد *في الحجرية\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "كون الأشياء على] *حقائقها\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "[يـ]ـتوهّمه على العظام فقط\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "[يـ]ـتوهّمه على العظام فقط\n",
            "\n",
            "\n",
            "‫ [\"ווגג''דדאאננאא צצחחייחחאא פפעעללממוואא\", 'وجداناً صحيحاً فعلموا']\n",
            "38\n",
            "21\n",
            "[30, 30, 27, 27, 3, 3, 28, 28, 25, 25, 38, 38, 25, 25, 0, 42, 42, 32, 32, 34, 34, 32, 32, 25, 25, 0, 41, 41, 40, 40, 36, 36, 37, 37, 30, 30, 25, 25]\n",
            "[58, 36, 39, 31, 56, 31, 61, 0, 45, 37, 60, 37, 31, 61, 0, 51, 49, 54, 55, 58, 31]\n",
            "2071 2071\n",
            "test_dataset_double_rasag\n",
            "========================================================================================================================================================================================================\n",
            "‫ וג'דאנא צחיחא פעלמוא  |  وجداناً صحيحاً فعلموا\n",
            "‫ בהא מעלומאתהמ עלמא צאדקא  |  بها معلوماتهم علماً صادقاً\n",
            "‫ וזאלת מעה אלשכוכ פכ'לצת  |  وزالت معه الشكوك فخلصت\n",
            "‫ חתי יתמ וצולהמ אלי אלמטלוב  |  حتّى يتمّ وصولهم إلى المطلوب\n",
            "‫ חתי את'בתוהא חקאיק עלי  |  حتّى أثبتوها حقائق على\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"LOAD DATASET\"\n",
        "\n",
        "\n",
        "# lines=load_lines(hakuzari)\n",
        "# pairs = create_parralel_phrases(lines,1)  \n",
        "# input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "# print_log_screen(\"test_dataset_double_kuzari\")\n",
        "# view_data(test_dataset_double_kuzari)\n",
        "\n",
        "# lines1=load_lines(haemunot)\n",
        "# pairs1 = create_parralel_phrases(lines1,1)  \n",
        "# input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = create_data_tensors(pairs1)\n",
        "# dataset_double_rasag,test_dataset_double_rasag=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "# print_log_screen(\"test_dataset_double_rasag\")\n",
        "# view_data(test_dataset_double_rasag)\n",
        "\n",
        "\n",
        "# # def gen kuzari_drop(lines,keep):\n",
        "# #   pairs = create_parralel_phrases(lines,0.90)  \n",
        "# #   input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# #   dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8",
        "colab_type": "text"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpKhndwM10b0",
        "colab_type": "text"
      },
      "source": [
        "###load_lines_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOU-bSFtpEgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"load_lines_synth\"\n",
        "\n",
        "#TODO edit when time permits\n",
        "\n",
        "def load_lines_synth(file_path=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(file_path, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  text=replace_arab_style_punctuation(text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)    \n",
        "\n",
        "  words=text.split()  \n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      #sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "      sentences.append(\" \".join(res))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "  return sentences\n",
        "\n",
        "# res=load_lines_synth()\n",
        "# print(len(res))\n",
        "#res[-5:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9CAcJjeyB_E",
        "colab_type": "text"
      },
      "source": [
        "###preprocess_synth_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdHcLRT0uhA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_synth_lines(arab_sentences):\n",
        "  res_sentences=[]\n",
        "  for arr in arab_sentences:    \n",
        "    arr=remove_arab_nikud(arr)\n",
        "    arr=standard_nunization(arr)\n",
        "    arr=clear_blank(replace_arab_style_punctuation(arr))\n",
        "    arr=normalize_unicode(arr) \n",
        "    \n",
        "    arr=remove_chars_not_in_arab_map(arr)\n",
        "    res_sentences.append(arr)\n",
        "  return res_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"GEN SYNTH\"\n",
        "\n",
        "# #TODO edit when time permits\n",
        "\n",
        "# def load_lines_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "#   with open(ibnsina, 'rb') as f:\n",
        "#       ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "#   ibnsina_text=replace_arab_style_punctuation(ibnsina_text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "#   #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "#   #add space before and after punctuation signs\n",
        "#   ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "#   ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "#   return ibnsina_text\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF gen_synth_sentences\"\n",
        "\n",
        "# def gen_synth_sentences(sina_words):\n",
        "#   #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "#   SENTENCE_LIMIT=20\n",
        "#   sentences=[]\n",
        "#   char_count=0\n",
        "#   res=[]\n",
        "#   for w in sina_words:\n",
        "#   #  w=w.rstrip(\" \").strip(\" \")\n",
        "#     char_count+=len(w)+1 #len of word + space afterwards\n",
        "#     res.append(w)\n",
        "#     if char_count>SENTENCE_LIMIT:    \n",
        "#       sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "#       res=[]\n",
        "#       char_count=0\n",
        "#     #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "#   return sentences\n",
        "\n",
        "# # #ACTIVATE\n",
        "# # sentences=gen_synth_sentences(sina_words)\n",
        "# # len(sentences)\n",
        "# # sentences[:5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AqGe0R1_a_",
        "colab_type": "text"
      },
      "source": [
        "###create_parralel_phrases_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF gen_dropout\"\n",
        "\n",
        "def create_parralel_phrases_synth(sentences,keep=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)           \n",
        "    heb=reverse_simple_map(arr)\n",
        "    heb=dropout(heb,keep)\n",
        "    heb_sentences.append(double_hebrew(heb))\n",
        "\n",
        "  print_log(heb_sentences[:5]) \n",
        "  print_log(arab_setences[:5])\n",
        "\n",
        "  print_log(len(arab_setences))\n",
        "  \n",
        "  return list(zip(heb_sentences,arab_setences))\n",
        "\n",
        "  #input_tensor_synth, target_tensor_synth, \\  \n",
        "  # input_lenghts_synth,target_lengths_synth = create_data_tensors(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # # Show length\n",
        "  # print_log(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  # print_log(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  # BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  # dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "  #                                                     target_tensor_synth,\n",
        "  #                                                     input_lenghts_synth,\n",
        "  #                                                     target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  # dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  # return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF gen_dropout_all\"\n",
        "\n",
        "# def gen_dropout_all(sntcs1,*sentences,keep=1):\n",
        "#   result_dataset=gen_dropout(sntcs1,keep)\n",
        "#   for sntcs in sentences:\n",
        "#     result_dataset.concatenate(gen_dropout(sntcs,keep))\n",
        "#   return result_dataset.shuffle(1000)\n",
        "\n",
        "# #   BUFFER_SIZE=1000 #TODO get size\n",
        "# #   return dataset_double_synt3.concatenate(dataset_double_kuzari).shuffle(BUFFER_SIZE)  ##TODO: this is without dropout. \n",
        "# # #view_data(gen_dropout_all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auU5EkGq5fW9",
        "colab_type": "text"
      },
      "source": [
        "##activate gen synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WTc9QYMawy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cfc1652-135f-4edd-b6c6-a2928ef436c0"
      },
      "source": [
        "synth_path0=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"\n",
        "synth_sentences0=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path0)\n",
        ")\n",
        "\n",
        "\n",
        "synth_phrase_pairs0=create_parralel_phrases_synth(synth_sentences0,\n",
        "                                                  0.9)\n",
        "synth_dataset0=produce_dataset(synth_phrase_pairs0,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset0)\n",
        "\n",
        "#####################3\n",
        "synth_path1=\"/gdrive/My Drive/JUDEO-ARAB/daruri-IR.txt\"\n",
        "synth_sentences1=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path1)\n",
        ")\n",
        "synth_phrase_pairs1=create_parralel_phrases_synth(synth_sentences1,\n",
        "                                                  0.9)\n",
        "synth_dataset1=produce_dataset(synth_phrase_pairs1,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset1)\n",
        "\n",
        "\n",
        "#######################33\n",
        "synth_path2=\"/gdrive/My Drive/JUDEO-ARAB/farabi-tahsil.txt\"\n",
        "synth_sentences2=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path2)\n",
        ")\n",
        "synth_phrase_pairs2=create_parralel_phrases_synth(synth_sentences2,\n",
        "                                                  0.9)\n",
        "synth_dataset2=produce_dataset(synth_phrase_pairs2,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset2)\n",
        "\n",
        "##########################3\n",
        "synth_path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "synth_sentences3=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path3)\n",
        ")\n",
        "\n",
        "synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                  0.9)\n",
        "synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset3)\n",
        "\n",
        "\n"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‫Skipping char not in predefined map\n",
            " ( ‏ )\n",
            "in sentences:\n",
            "صار نوعاً . ‏ وإن كانت\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ‎ )\n",
            "in sentences:\n",
            "لأنها زمانية . ‎ فلا\n",
            "\n",
            "['__תתאאבב אאללששפפ__ אאללאאללההייאאתת', \"אאללששייככ'' אא__רריי__סס אאבבננ ססייננאא\", \"ממווקק__ אאללפפללסספפהה'' אאלל__ססללאאממייהה''\", \"אאלל__הה__ססתת אאללממקקאאללהה'' אאללאאוולליי\", ':: אאללפפצצלל אאללאאוולל :: פפיי אאבבתתדדאא']\n",
            "['كتاب الشفاء الإلهيات', 'الشيخ الرئيس ابن سينا', 'موقع الفلسفة الإسلامية', 'الفهرست المقالة الأولى', ': الفصل الأول : في ابتداء']\n",
            "21322\n",
            "\n",
            "‫ ('__תתאאבב אאללששפפ__ אאללאאללההייאאתת', 'كتاب الشفاء الإلهيات')\n",
            "36\n",
            "20\n",
            "[47, 47, 46, 46, 25, 25, 26, 26, 0, 25, 25, 36, 36, 45, 45, 41, 41, 47, 47, 0, 25, 25, 36, 36, 25, 25, 36, 36, 29, 29, 34, 34, 25, 25, 46, 46]\n",
            "[53, 34, 31, 32, 0, 31, 54, 44, 51, 31, 25, 0, 31, 54, 29, 54, 57, 60, 31, 34]\n",
            "21322 21322\n",
            "========================================================================================================================================================================================================\n",
            "‫ _תאב אלשפ_ אלאלהיאת  |  كتاب الشفاء الإلهيات\n",
            "‫ אלשיכ' א_רי_ס אבנ סינא  |  الشيخ الرئيس ابن سينا\n",
            "‫ מוק_ אלפלספה' אל_סלאמיה'  |  موقع الفلسفة الإسلامية\n",
            "‫ אל_ה_סת אלמקאלה' אלאולי  |  الفهرست المقالة الأولى\n",
            "‫ : אלפצל אלאול : פי אבתדא  |  : الفصل الأول : في ابتداء\n",
            "========================================================================================================================================================================================================\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "القسم الأول 11 – أما\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            ". 12ـ وأما المعتزلة فاستدلوا\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "13ـ والقول في هذه المسألة\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "حصول العلم به . 14ـ وقد\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "نهاية . 15ـ والذي ينبغي\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            ". ] . 16ـ أما من ذهب\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            "الأول 17ـ وهو يتضمن النظر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". 76 /1 مسألة ثالثة :\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "الحرام ضد الواجب 28 –\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( { )\n",
            "in sentences:\n",
            ": {لا تقربوا الصلاة وأنتم\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( } )\n",
            "in sentences:\n",
            "سكارى} فإنه إن سلم ظهور\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "من هذا –كما زعموا-وجوب\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "فصول : 93/1 الفصل الأول\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ذلك . 95/1 الفصل الثالث\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ـ )\n",
            "in sentences:\n",
            ": 63 62ـ وهو يتضمن النظر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "المجتهد) ص 8990/ ج 1\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( A )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( b )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( a )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( n )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( S )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( t )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( a )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( t )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( U )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( n )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( v )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( r )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( t )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            "1977 : Albany State University\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( f )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( N )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( Y )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( r )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( k )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( P )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( r )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "of New York Press . 194189]\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "للزركشي 4/239 . أنظر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            ". 77 –وإذا وضع هذا ,\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "78 – فهذا ما ينبغي أن\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            ". 80 –ويلحق بخبر الواحد\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "بخبر الواحد شرعا 81 –\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ﴿ )\n",
            "in sentences:\n",
            "عزوجل : ﴿ فلولا نفر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ﴿ )\n",
            "in sentences:\n",
            "وجل ﴿إن الذين يكتمون\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( ﴾ )\n",
            "in sentences:\n",
            "والهدى ﴾ وبقوله) : نضر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "للزركشي 4/364 . أنظر\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "بقوله عليه السلام- *لا\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( * )\n",
            "in sentences:\n",
            "وصية لوارث*- لكن في هذا\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "السلام – لا وصية للوارث\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "– وكذلك احتجوا بقوله\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". ص326/ج2 . ] وهو نسخ\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "قال : ( ص74/جI) (وقد\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( I )\n",
            "in sentences:\n",
            "قال : ( ص74/جI) (وقد\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "من البداية (ص126/جi)\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "من البداية (ص126/جi)\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "عليه . [221/1] 142- والاستصحاب\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( | )\n",
            "in sentences:\n",
            "| . . . على الأمة أن\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( | )\n",
            "in sentences:\n",
            "بدليل العصمة | [ كلمة\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "المرسل . انظر ص197/ج2\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ", 327/ج2 وسننقل في الهوامش\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( | )\n",
            "in sentences:\n",
            "سبب خاص فأخرج مخرج |\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( | )\n",
            "in sentences:\n",
            "العام | [ كتب الناسخ\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "القاهر الجرجاني 2/719\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ومجاز القرآن 1/328 وجمل\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "القاهر الجرجانى 1/720\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "للزركشي 3/280 , وعلق\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "–وأما هل يكون المستثنى\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ص : 288 / ج ا . –] .\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "ص : 288 / ج ا . –] .\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ص 3/ ج 1 . ] . والشىء\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "أو الطوافات ص 21 / ج\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ص 143 / ج ا من بداية\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "ص 275 / ج 2 . ] , فان\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "بداية المجتهد ص 273 /\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". انظرالبداية ص202-220/\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". . . ص 15/ 2 . ] وهو\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "انظر ص 101/ج ا . ] ,\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". ص 80 /ج 2 . ] وربما\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". ص3 /ج ا . ] , ولا يوجد\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            "من خطاب العرب ص 3-4/ج1\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". انظر 42-108-347/ج ا\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ". 142 / ج2 . ] . 240\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ": 105-136-138-140-311/ج1-45-55-75-96-104-111-116-131-147-152-165-173-189-255-274-289-290-291-332/ج2\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( / )\n",
            "in sentences:\n",
            ": 105-136-138-140-311/ج1-45-55-75-96-104-111-116-131-147-152-165-173-189-255-274-289-290-291-332/ج2\n",
            "\n",
            "['בבססממ אאללללהה __ללררחחממננ אאללררחחייממ', 'אאלל____ררוו__יי פפ__ אאצצ__לל אאללפפקק__', 'אא__ ממככ__תתצצ__ אאללממססתתצצפפ__ ללאאבביי', 'אאללוולליידד ממ__ממדד בבננ __ששדד אאללחחפפיידד', \"אאללממ__וופפיי ססננהה'' 559955הה תת__דדייממ\"]\n",
            "['بسم الله الرحمن الرحيم', 'الضروري في أصول الفقه', 'أو مختصر المستصفى لأبي', 'الوليد محمد بن رشد الحفيد', 'المتوفى سنة 595ه تقديم']\n",
            "5610\n",
            "\n",
            "‫ ('בבססממ אאללללהה __ללררחחממננ אאללררחחייממ', 'بسم الله الرحمن الرحيم')\n",
            "41\n",
            "22\n",
            "[26, 26, 39, 39, 37, 37, 0, 25, 25, 36, 36, 36, 36, 29, 29, 0, 47, 47, 36, 36, 44, 44, 32, 32, 37, 37, 38, 38, 0, 25, 25, 36, 36, 44, 44, 32, 32, 34, 34, 37, 37]\n",
            "[32, 43, 55, 0, 31, 54, 54, 57, 0, 31, 54, 41, 37, 55, 56, 0, 31, 54, 41, 37, 60, 55]\n",
            "5610 5610\n",
            "========================================================================================================================================================================================================\n",
            "‫ בסמ אללה _לרחמנ אלרחימ  |  بسم الله الرحمن الرحيم\n",
            "‫ אל__רו_י פ_ אצ_ל אלפק_  |  الضروري في أصول الفقه\n",
            "‫ א_ מכ_תצ_ אלמסתצפ_ לאבי  |  أو مختصر المستصفى لأبي\n",
            "‫ אלוליד מ_מד בנ _שד אלחפיד  |  الوليد محمد بن رشد الحفيد\n",
            "‫ אלמ_ופי סנה' 595ה ת_דימ  |  المتوفى سنة 595ه تقديم\n",
            "========================================================================================================================================================================================================\n",
            "['__ססממ אאללללהה אאללררחחממננ __ללררחחייממ', 'וובבהה ננססתתעעייננ ככתתאאבב תתחחצציילל', \"אאללססעעאאדדהה'' אאבבוו ננ__רר אאללפפאארראאבביי\", \"אא__אא__ייאא אאללאאננססאאננייהה'' אאללתתיי\", \"אאדד''__ חחצצללתת פפיי אאללאאממממ וופפיי\"]\n",
            "['بسم الله الرحمن الرحيم', 'وبه نستعين كتاب تحصيل', 'السعادة ابو نصر الفارابي', 'الأشياء الإنسانية التي', 'إذا حصلت في الأمم وفي']\n",
            "2274\n",
            "\n",
            "‫ ('__ססממ אאללללהה אאללררחחממננ __ללררחחייממ', 'بسم الله الرحمن الرحيم')\n",
            "41\n",
            "22\n",
            "[47, 47, 39, 39, 37, 37, 0, 25, 25, 36, 36, 36, 36, 29, 29, 0, 25, 25, 36, 36, 44, 44, 32, 32, 37, 37, 38, 38, 0, 47, 47, 36, 36, 44, 44, 32, 32, 34, 34, 37, 37]\n",
            "[32, 43, 55, 0, 31, 54, 54, 57, 0, 31, 54, 41, 37, 55, 56, 0, 31, 54, 41, 37, 60, 55]\n",
            "2274 2274\n",
            "========================================================================================================================================================================================================\n",
            "‫ _סמ אללה אלרחמנ _לרחימ  |  بسم الله الرحمن الرحيم\n",
            "‫ ובה נסתעינ כתאב תחציל  |  وبه نستعين كتاب تحصيل\n",
            "‫ אלסעאדה' אבו נ_ר אלפאראבי  |  السعادة ابو نصر الفارابي\n",
            "‫ א_א_יא אלאנסאניה' אלתי  |  الأشياء الإنسانية التي\n",
            "‫ אד'_ חצלת פי אלאממ ופי  |  إذا حصلت في الأمم وفي\n",
            "========================================================================================================================================================================================================\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "الأبيض – وهو شخص الأبيض\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "– يعنون به الوجود والوجدان\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( u )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( c )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( b )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( a )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( t )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( r )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( @ )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( u )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "[\"ככתתאא__ ררססאא____'' אאללחחררוופפ ללללפפייללססוופפ\", '__בביי ננצצרר אאללפפאארר__בביי בבססממ', 'אאללללהה אאללררחחממננ אאללרר____ממ וובבהה', 'ננססתתעעייננ __ללחחממ__ לללל__ ררבב', 'אאללעעאאללממייננ וואאללססללאאממ עעלליי']\n",
            "['كتاب رسالة الحروف للفيلسوف', 'أبي نصر الفارابيّ بسم', 'الله الرحمن الرحيم وبه', 'نستعين الحمد لله ربّ', 'العالمين والسلام على']\n",
            "9940\n",
            "\n",
            "‫ (\"ככתתאא__ ררססאא____'' אאללחחררוופפ ללללפפייללססוופפ\", 'كتاب رسالة الحروف للفيلسوف')\n",
            "51\n",
            "26\n",
            "[35, 35, 46, 46, 25, 25, 47, 47, 0, 44, 44, 39, 39, 25, 25, 47, 47, 47, 47, 3, 3, 0, 25, 25, 36, 36, 32, 32, 44, 44, 30, 30, 41, 41, 0, 36, 36, 36, 36, 41, 41, 34, 34, 36, 36, 39, 39, 30, 30, 41, 41]\n",
            "[53, 34, 31, 32, 0, 41, 43, 31, 54, 33, 0, 31, 54, 37, 41, 58, 51, 0, 54, 54, 51, 60, 54, 43, 58, 51]\n",
            "9940 9940\n",
            "========================================================================================================================================================================================================\n",
            "‫ כתא_ רסא__' אלחרופ ללפילסופ  |  كتاب رسالة الحروف للفيلسوف\n",
            "‫ _בי נצר אלפאר_בי בסמ  |  أبي نصر الفارابيّ بسم\n",
            "‫ אללה אלרחמנ אלר__מ ובה  |  الله الرحمن الرحيم وبه\n",
            "‫ נסתעינ _לחמ_ לל_ רב  |  نستعين الحمد لله ربّ\n",
            "‫ אלעאלמינ ואלסלאמ עלי  |  العالمين والسلام على\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9PbEec88luZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf22870e-3793-44f9-942b-0460b6051c53"
      },
      "source": [
        "synth_path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "synth_sentences3=preprocess_synth_lines(\n",
        "    load_lines_synth(synth_path3)\n",
        ")\n",
        "################\n",
        "synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                  0.9)\n",
        "synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset3)\n",
        "\n",
        "#################\n",
        "synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                  0.9)\n",
        "synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset3)\n",
        "\n",
        "##################\n",
        "synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                  0.9)\n",
        "synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                               to_shuffle=False)\n",
        "view_data(synth_dataset3)\n"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "الأبيض – وهو شخص الأبيض\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( – )\n",
            "in sentences:\n",
            "– يعنون به الوجود والوجدان\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( u )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            "(www . muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( c )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( w )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( b )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( a )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( t )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( e )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( r )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( @ )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( u )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( m )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( i )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( l )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( s )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( o )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( p )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( h )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "‫Skipping char not in predefined map\n",
            " ( y )\n",
            "in sentences:\n",
            ". com) للمراسلة : webmaster@muslimphilosophy\n",
            "\n",
            "['__תתאא__ ררססאאלל____ אאללחחררוופפ לל__פפ__ללססוופפ', 'אאבביי ננ____ אאללפפאא__אא__יי בבסס__', 'אאללללהה אאללררחחממננ אאללררחחיי__ וובבהה', 'ננססתתעעייננ אאללחחממדד לל__הה רר__', 'אאללעעאא__ממייננ וואאללססללאאממ __לליי']\n",
            "['كتاب رسالة الحروف للفيلسوف', 'أبي نصر الفارابيّ بسم', 'الله الرحمن الرحيم وبه', 'نستعين الحمد لله ربّ', 'العالمين والسلام على']\n",
            "9940\n",
            "\n",
            "‫ ('__תתאא__ ררססאאלל____ אאללחחררוופפ לל__פפ__ללססוופפ', 'كتاب رسالة الحروف للفيلسوف')\n",
            "51\n",
            "26\n",
            "[47, 47, 46, 46, 25, 25, 47, 47, 0, 44, 44, 39, 39, 25, 25, 36, 36, 47, 47, 47, 47, 0, 25, 25, 36, 36, 32, 32, 44, 44, 30, 30, 41, 41, 0, 36, 36, 47, 47, 41, 41, 47, 47, 36, 36, 39, 39, 30, 30, 41, 41]\n",
            "[53, 34, 31, 32, 0, 41, 43, 31, 54, 33, 0, 31, 54, 37, 41, 58, 51, 0, 54, 54, 51, 60, 54, 43, 58, 51]\n",
            "9940 9940\n",
            "========================================================================================================================================================================================================\n",
            "‫ _תא_ רסאל__ אלחרופ ל_פ_לסופ  |  كتاب رسالة الحروف للفيلسوف\n",
            "‫ אבי נ__ אלפא_א_י בס_  |  أبي نصر الفارابيّ بسم\n",
            "‫ אללה אלרחמנ אלרחי_ ובה  |  الله الرحمن الرحيم وبه\n",
            "‫ נסתעינ אלחמד ל_ה ר_  |  نستعين الحمد لله ربّ\n",
            "‫ אלעא_מינ ואלסלאמ _לי  |  العالمين والسلام على\n",
            "========================================================================================================================================================================================================\n",
            "[\"ככתתאא__ __ססאאללהה'' אאללחח__וופפ ללללפפייללססוופפ\", 'אאבביי ננצצרר אאללפפאארראאבביי בבססממ', 'אאללללהה אאללרר__ממ__ אאללררחחייממ וו__הה', 'ננססתתעעייננ אאללחחממדד ללללהה ררבב', '__ללעעאאללממייננ וואאללססללאאממ עעלליי']\n",
            "['كتاب رسالة الحروف للفيلسوف', 'أبي نصر الفارابيّ بسم', 'الله الرحمن الرحيم وبه', 'نستعين الحمد لله ربّ', 'العالمين والسلام على']\n",
            "9940\n",
            "\n",
            "‫ (\"ככתתאא__ __ססאאללהה'' אאללחח__וופפ ללללפפייללססוופפ\", 'كتاب رسالة الحروف للفيلسوف')\n",
            "51\n",
            "26\n",
            "[35, 35, 46, 46, 25, 25, 47, 47, 0, 47, 47, 39, 39, 25, 25, 36, 36, 29, 29, 3, 3, 0, 25, 25, 36, 36, 32, 32, 47, 47, 30, 30, 41, 41, 0, 36, 36, 36, 36, 41, 41, 34, 34, 36, 36, 39, 39, 30, 30, 41, 41]\n",
            "[53, 34, 31, 32, 0, 41, 43, 31, 54, 33, 0, 31, 54, 37, 41, 58, 51, 0, 54, 54, 51, 60, 54, 43, 58, 51]\n",
            "9940 9940\n",
            "========================================================================================================================================================================================================\n",
            "‫ כתא_ _סאלה' אלח_ופ ללפילסופ  |  كتاب رسالة الحروف للفيلسوف\n",
            "‫ אבי נצר אלפאראבי בסמ  |  أبي نصر الفارابيّ بسم\n",
            "‫ אללה אלר_מ_ אלרחימ ו_ה  |  الله الرحمن الرحيم وبه\n",
            "‫ נסתעינ אלחמד ללה רב  |  نستعين الحمد لله ربّ\n",
            "‫ _לעאלמינ ואלסלאמ עלי  |  العالمين والسلام على\n",
            "========================================================================================================================================================================================================\n",
            "[\"ככ__אאבב ררססאאלל__'' אאללחחררוופפ לללל__ייללססוופפ\", 'אאבביי ננצצ__ אאללפפ__רראא__יי בבססממ', 'אאללללהה אאללררחחממננ אאללררחחייממ וובבהה', 'ננססתתעעייננ אא__חחממדד ללללהה ררבב', '__ללעעאא__ממ____ וואאללססלל__ממ עעלליי']\n",
            "['كتاب رسالة الحروف للفيلسوف', 'أبي نصر الفارابيّ بسم', 'الله الرحمن الرحيم وبه', 'نستعين الحمد لله ربّ', 'العالمين والسلام على']\n",
            "9940\n",
            "\n",
            "‫ (\"ככ__אאבב ררססאאלל__'' אאללחחררוופפ לללל__ייללססוופפ\", 'كتاب رسالة الحروف للفيلسوف')\n",
            "51\n",
            "26\n",
            "[35, 35, 47, 47, 25, 25, 26, 26, 0, 44, 44, 39, 39, 25, 25, 36, 36, 47, 47, 3, 3, 0, 25, 25, 36, 36, 32, 32, 44, 44, 30, 30, 41, 41, 0, 36, 36, 36, 36, 47, 47, 34, 34, 36, 36, 39, 39, 30, 30, 41, 41]\n",
            "[53, 34, 31, 32, 0, 41, 43, 31, 54, 33, 0, 31, 54, 37, 41, 58, 51, 0, 54, 54, 51, 60, 54, 43, 58, 51]\n",
            "9940 9940\n",
            "========================================================================================================================================================================================================\n",
            "‫ כ_אב רסאל_' אלחרופ לל_ילסופ  |  كتاب رسالة الحروف للفيلسوف\n",
            "‫ אבי נצ_ אלפ_רא_י בסמ  |  أبي نصر الفارابيّ بسم\n",
            "‫ אללה אלרחמנ אלרחימ ובה  |  الله الرحمن الرحيم وبه\n",
            "‫ נסתעינ א_חמד ללה רב  |  نستعين الحمد لله ربّ\n",
            "‫ _לעא_מ__ ואלסל_מ עלי  |  العالمين والسلام على\n",
            "========================================================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4",
        "colab_type": "text"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL\"\n",
        "\n",
        "#NETWORK PARAMS\n",
        "# The embedding dimension\n",
        "embedding_dim = 8 #256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 #1024\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  #rnn = tf.keras.layers.CuDNNGRU\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "  #rnn = tf.keras.layers.LSTM #see https://stackoverflow.com/questions/55761337/module-tensorflow-python-keras-api-v2-keras-layers-has-no-attribute-cudnnlst\n",
        "\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "#FUNCTION TO BUILD MODEL\n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    \n",
        "  \n",
        "\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL1\"\n",
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "model=rebuild()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE",
        "colab_type": "text"
      },
      "source": [
        "##CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEFINE CHECKPOINT\"\n",
        "\n",
        "checkpoint_path='/gdrive/My Drive/checkpoints/'+this_time+\"/ckpt\"\n",
        "def save_checkpoint(massage):\n",
        "  print_log_screen(\"saving checkpoing at \"+checkpoint_path)\n",
        "  model.save_weights(checkpoint_path)\n",
        "  f_check= open(checkpoint_path+\".txt\",\"a+\")  \n",
        "  f_check.write(\"saving chekcpoing at epoch\"+ str(GLOBAL_epoch) +'\\n')\n",
        "  f_check.write(massage+'\\n')\n",
        "  f_check.close()\n",
        "  \n",
        "def load_checkpoint(checkpoint_path=checkpoint_path):\n",
        "  print_log_screen(\"loading checkpoing from \"+checkpoint_path)\n",
        "  model1=rebuild()\n",
        "  model1.load_weights(checkpoint_path)\n",
        "  return model1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG",
        "colab_type": "text"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF show diff\"\n",
        "from termcolor import colored\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "# #USEAGE:\n",
        "# s1=\"لامة النصارى واستحقو\" \n",
        "# s2=\"لأمّة النصارى واستحقوّا\"\n",
        "# a,b=show_diff(s1,s2,'blue')\n",
        "# #print_log(\"\".join(b))\n",
        "# print_log(a,\"|\",b)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo",
        "colab_type": "text"
      },
      "source": [
        "## forward run each letter seperatly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST single letters\"\n",
        "def test__CTC_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print_log(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "       print(\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print(\"####max is the blank symbole\")\n",
        "      print_log('-'*10)\n",
        "  \n",
        "#test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF",
        "colab_type": "text"
      },
      "source": [
        "##forward run a sentence (with top n beam search results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM2_GVYmOwdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF test__CTC_word_multiline\"\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    l = double_hebrew(l)\n",
        "    v=encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  #print_log(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "  #inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  # if print_deteils:\n",
        "  #   print_log('#####deteils')\n",
        "  # for jj in range(num_of_letters):\n",
        "  #     if print_deteils:\n",
        "  #       print_log(word_str[jj])\n",
        "      \n",
        "  #     pred_distr=predict_ltrs[0][jj]\n",
        "  #     pred_distr=tf.nn.softmax(pred_distr)\n",
        "  #     if print_deteils:\n",
        "  #       for ii in range(num_of_arab_letters):\n",
        "  #         print_log(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "  #       print_log(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "  #       print_log()\n",
        "  #     maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "  #     if print_deteils:\n",
        "  #       print_log(maximum)\n",
        "  #     if (maximum<42):\n",
        "  #       if print_deteils:\n",
        "  #         print_log(targ_lang.vocab[maximum])\n",
        "  #       res.append(targ_lang.vocab[maximum])\n",
        "  #     else:\n",
        "  #       if print_deteils:\n",
        "  #         print_log(\"####max is the blank symbole\")\n",
        "  #       res.append(\"-\")\n",
        "  #     if print_deteils:\n",
        "  #       print_log('-'*10)\n",
        "  # print_log(word_str)\n",
        "  #print_log(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print_log(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_ar(dense[t])\n",
        "      print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "# lines='''כמאלא\n",
        "# כמאלא'''\n",
        "\n",
        "# print_log_screen(test__CTC_word_multiline(lines,3,BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K6i5QWGO4DS",
        "colab_type": "text"
      },
      "source": [
        "##TEST KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF TEST KFIR\"\n",
        "import editdistance\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline_kfir(word_str,targ_str,indexes=None,num_of_paths=1,_BATCH_SIZE=BATCH_SIZE,SHOW_PRINT=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  targ_lines=targ_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  num_of_letters= 0\n",
        "  assert(num_of_lines==len(targ_lines))\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    #l = double_hebrew(l)\n",
        "    v=encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  \n",
        "  total_res=[]\n",
        "  total_edit_dist=0\n",
        "  total_normalized_edit_dist=0\n",
        "  line_counter=1\n",
        "  for t in range(num_of_lines):\n",
        "    real=targ_lines[t]\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_arr(dense[t]).strip() \n",
        "      # print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "      total_res.append(prediction)\n",
        "    if indexes:\n",
        "      real=real.split()[indexes[t]]\n",
        "      prediction=prediction.split()[indexes[t]]\n",
        "    ed_dist=editdistance.eval(real, prediction)\n",
        "    #print(real, len(real))\n",
        "    num_of_letters+=len(real)\n",
        "\n",
        "    normalized_ed_dist=ed_dist/len(real)\n",
        "    real,prediction=show_diff(real,prediction,'red')\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"({0})\".format(line_counter),LTRchar,undouble_hebrew(lines[t]),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(ed_dist))\n",
        "    line_counter+=1\n",
        "    total_normalized_edit_dist+=normalized_ed_dist\n",
        "    total_edit_dist+=ed_dist\n",
        "  return total_res,total_normalized_edit_dist,num_of_lines,total_edit_dist,num_of_letters\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''יכ'אלף\n",
        "עלי\n",
        "הד'ה\n",
        "אלאמאנה\n",
        "ולא'''\n",
        "\n",
        "lines='''ייככ''אאללףף\n",
        "עעלליי\n",
        "ההדד''הה\n",
        "אאללאאממאאננהה\n",
        "ווללאא'''\n",
        "\n",
        "lines='''ייככ''אאללףף\n",
        "אאללאאממאאננהה ששללווםם'''\n",
        "\n",
        "# targ_lines='''يخالف\n",
        "# على\n",
        "# هذه\n",
        "# الأمانة\n",
        "# ولا'''\n",
        "\n",
        "targ_lines='''يخالف\n",
        "الأمانة الأمانة'''\n",
        "indexes=[0,1]\n",
        "#indexes=None\n",
        "\n",
        "#test__CTC_word_multiline_kfir(lines,targ_lines,indexes,SHOW_PRINT=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5e5-59jCxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF test_kfir1\"\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "\n",
        "def test_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False,index_path=None):\n",
        "\n",
        "  lines=load_lines(data_path)\n",
        "  pairs = create_parralel_phrases(lines)\n",
        "  if index_path:\n",
        "    indexes=[]\n",
        "    f_indexes=open(index_path,'r')\n",
        "    ind_lines=f_indexes.readlines()\n",
        "    assert(len(ind_lines)==len(lines))\n",
        "    for il in ind_lines:\n",
        "      indexes.append(int(il))\n",
        "    f_indexes.close()\n",
        "   # print(indexes)\n",
        "  if replace_GAIN:\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs]\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "\n",
        "  num_of_lines=len(pairs)\n",
        "  index=0\n",
        "  total_num_of_examples=0\n",
        "  total_num_of_letters=0\n",
        "  total_sum_e_d_normalized=0\n",
        "  total_sum_e_d=0\n",
        "  while index<=num_of_lines:\n",
        "    batch_hebrew=hebrew_lines[index:index+BATCH_SIZE]\n",
        "    batch_arab=arab_lines[index:index+BATCH_SIZE]\n",
        "    if index_path:\n",
        "      batch_indexes=indexes[index:index+BATCH_SIZE]\n",
        "    else:\n",
        "      batch_indexes=None\n",
        "    _,sum_of_e_d_normalized,num_of_examples,sum_of_e_d,num_of_letters=test__CTC_word_multiline_kfir('\\n'.join(batch_hebrew),'\\n'.join(batch_arab),batch_indexes,SHOW_PRINT=SHOW_PRINT)\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"BATCH (sum_of_e_d_normalized,num_of_examples): \",sum_of_e_d_normalized,num_of_examples)\n",
        "      print_log_screen(\"BATCH (sum_of_e_d,num_of_letters): \",sum_of_e_d,num_of_letters)\n",
        "    total_num_of_examples+=num_of_examples\n",
        "    total_num_of_letters+=num_of_letters\n",
        "    total_sum_e_d+=sum_of_e_d\n",
        "    total_sum_e_d_normalized+=sum_of_e_d_normalized\n",
        "    index+=BATCH_SIZE\n",
        "\n",
        "  print_log_screen(\"#examples:\",total_num_of_examples,\", accuracy:\",1-total_sum_e_d_normalized/total_num_of_examples)\n",
        "  print_log_screen(\"#letters:\",total_num_of_letters,\", accuracy1:\",1-total_sum_e_d/total_num_of_letters)\n",
        "  return total_sum_e_d_normalized/total_num_of_examples,total_sum_e_d/total_num_of_letters\n",
        "\n",
        "# test_kfir(kfir_kuzari_test,SHOW_PRINT=True)\n",
        "\n",
        "# test_kfir(kfir_rasag_test,SHOW_PRINT=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qhhb8OaD1QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR\"\n",
        "\n",
        "import re\n",
        "from termcolor import colored, cprint\n",
        "\n",
        "\n",
        "\n",
        "#adjust test kfir to comp edits only for index word!\n",
        "DO_EMUNOT=True\n",
        "\n",
        "if DO_EMUNOT:\n",
        "  file_all = open(haemunot, \"r\")\n",
        "  file = open(kfir_rasag_test, \"r\")\n",
        "else:\n",
        "  file_all = open(hakuzari, \"r\")\n",
        "  file = open(kfir_kuzari_test, \"r\")\n",
        "\n",
        "arabic_lines=[]\n",
        "for l in file_all:\n",
        "  arabic_lines.append(remove_arab_nikud(standard_nunization(normalize_unicode(l))))\n",
        "\n",
        "greped=[]\n",
        "for l in file:\n",
        "  greped.append(remove_arab_nikud(standard_nunization(normalize_unicode(l))))\n",
        "\n",
        "\n",
        "def grep(reg,ja,full_f,single_f,index_f):\n",
        "  #file_all = open(haemunot, \"r\")\n",
        "  for line in arabic_lines:\n",
        "      line=remove_arab_nikud(line)     \n",
        "      JA_line,arab_line=line.split('\\t')\n",
        "      if re.search(reg, arab_line):          \n",
        "          arab_words=arab_line.split()\n",
        "          JA_words=JA_line.split()\n",
        "          reg_ind=arab_words.index(reg.strip(\" $^\"))\n",
        "          if (not JA_words[reg_ind]==ja):\n",
        "            #TODO search for other option in text....\n",
        "             print(\"ja:\"+ja+ \"JA_words[reg_ind]:\" +JA_words[reg_ind])\n",
        "             print(\"JA_WORDS[reg_ind] (my version) will be outputed to both tests (with and without context)\")\n",
        "          assert(len(JA_words)==len(arab_words)) #MAYBE NOT ALWAYS THE CASE!!!          \n",
        "          arab_text=[]\n",
        "          JA_text=[]\n",
        "          for i in range(len(arab_words)):\n",
        "            if i==reg_ind:\n",
        "              arab_text.append(colored(arab_words[i],'red'))\n",
        "              JA_text.append(colored(JA_words[i],'red'))\n",
        "            else:\n",
        "              arab_text.append(arab_words[i]) \n",
        "              JA_text.append(JA_words[i])\n",
        "          full_line=\" \".join(JA_text)+'\\t'+\" \".join(arab_text)+'\\t'+str(reg_ind)\n",
        "          full_f.write(\" \".join(JA_words)+'\\t'+\" \".join(arab_words)+'\\n')\n",
        "          index_f.write(str(reg_ind)+'\\n')\n",
        "          single_words=JA_words[reg_ind]+'\\t'+arab_words[reg_ind]\n",
        "          single_f.write(single_words+'\\n')\n",
        "          return full_line\n",
        "  return\n",
        "          \n",
        "\n",
        "not_found_counter=0\n",
        "\n",
        "f1=open(\"with_context.txt\",'w+')\n",
        "f2=open(\"no_context.txt\",'w+')\n",
        "f3=open(\"context_idexes.txt\",'w+')\n",
        "\n",
        "for l in greped:  \n",
        "  arab=l.split('\\t')[1].strip()\n",
        "  ja=l.split('\\t')[0].strip()\n",
        " # print(ja)\n",
        "  if DO_EMUNOT:\n",
        "    ja=ja.replace(\"ג\",\"ג'\").replace(\"ג''\",\"ג\")    ##only for kuzari - change to regular (or won't grep)\n",
        " # print(ja)\n",
        "  arab=' '+arab+' ' \n",
        "  found=grep(arab,ja,f1,f2,f3)\n",
        " # print(\"grep: \"+arab)\n",
        "  if not found:\n",
        "    found=grep(\"^\"+arab.lstrip(),ja,f1,f2,f3)\n",
        "    if not found:\n",
        "      found=grep(arab.rstrip()+\"$\",ja,f1,f2,f3)\n",
        "      if not found:\n",
        "        not_found_counter+=1\n",
        "  if not found:\n",
        "    print(\"not found:\"+arab)\n",
        "  else:\n",
        "    print(found)\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "print(\"#not founds:\"+str(not_found_counter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNrQSIk2f9iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR 1\"\n",
        "\n",
        "\n",
        "PRINT_DETAILED=True\n",
        "\n",
        "\n",
        "\n",
        "model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-04 12:32:58.607342/ckpt')  #THIS IS THE FIRST ACTIVATIION, BY WHICH I PRODUCED THE EXCEL FOR NACHUM\n",
        "\n",
        "#model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-05 13:01:42.636420/ckpt')\n",
        "#if STATEFUL:\n",
        "model.reset_states()\n",
        "test_kfir(\"with_context.txt\",replace_GAIN=True,SHOW_PRINT=PRINT_DETAILED,index_path=\"context_idexes.txt\")\n",
        "\n",
        "print(\"=\"*200)\n",
        "#if STATEFUL:\n",
        "model.reset_states()\n",
        "\n",
        "test_kfir(\"no_context.txt\",replace_GAIN=True,SHOW_PRINT=PRINT_DETAILED)\n",
        "\n",
        "# print(\"=\"*200)\n",
        "# model.reset_states()\n",
        "\n",
        "# test_kfir(\"no_context.txt\",True)\n",
        "\n",
        "# print(\"=\"*200)\n",
        "# model.reset_states()\n",
        "\n",
        "# test_kfir(\"no_context.txt\",True)\n",
        "\n",
        "#model.reset_states()\n",
        "#test_kfir(kfir_rasag_test,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L",
        "colab_type": "text"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TEST_BASELINE\"\n",
        "import editdistance\n",
        "def test_loss_baseline(this_dataset=test_dataset_double_kuzari,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_example_batch[i])\n",
        "\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=decode_arr(target_example_batch[i],targets_len[i].numpy()).strip(BLANK)\n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"baseline accuracy: \",1-total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 1-total_accuracy\n",
        "test_loss_baseline(limit=3)\n",
        "# print_log_screen(\"KUZARI TEST\")\n",
        "# test_loss_baseline(test_dataset_double_kuzari)\n",
        "# print_log_screen(\"RASAG TEST\")\n",
        "# test_loss_baseline(test_dataset_double_rasag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWMVLTywwIW",
        "colab_type": "text"
      },
      "source": [
        "##TEST BASELINE KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrcOFoswu-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF TEST_BASELINE_KFIR\"\n",
        "\n",
        "import editdistance\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "def test_loss_baseline_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False):\n",
        "  lines=load_lines(data_path) #THIS IS DOUBLED ALLREADY\n",
        "  pairs = create_parralel_phrases(lines)  \n",
        "  \n",
        "  if replace_GAIN:\n",
        "    print_log_screen(\"replaceing gimel with jain to match my train convention\")\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs] #ייננבבגג''יי\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "  total_examples=len(pairs)\n",
        "  total_loss=0\n",
        "  sum_of_e_dist=0\n",
        "  num_of_letters=0\n",
        "  for l in arab_lines:\n",
        "    num_of_letters+=len(l)\n",
        "  total_accuracy=0\n",
        "  line_counter=1\n",
        "  for heb_input,real in zip(hebrew_lines,arab_lines):\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)                \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                normalized_accuracy=accuracy/len(real)\n",
        "                total_accuracy+=normalized_accuracy\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                sum_of_e_dist+=accuracy\n",
        "                if SHOW_PRINT:\n",
        "                  print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",str(accuracy))\n",
        "                line_counter+=1\n",
        "\t\t\t\t\t\t\t   \n",
        "  total_accuracy/=total_examples\n",
        "  sum_of_e_dist/num_of_letters\n",
        "  print_log_screen(data_path ,\"accuracy: \",1-total_accuracy)\n",
        "  print_log_screen(data_path ,\"accuracy1: \",1-sum_of_e_dist/num_of_letters)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy,sum_of_e_dist/num_of_letters\n",
        "#test_loss_baseline(limit=3)\n",
        "#test_loss_baseline_kfir(kfir_kuzari_test)\n",
        "test_loss_baseline_kfir(kfir_kuzari_test,False,False)\n",
        "#test_loss_baseline_kfir(kfir_rasag_test)\n",
        "test_loss_baseline_kfir(kfir_rasag_test,False,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74",
        "colab_type": "text"
      },
      "source": [
        "##test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF general TEST_LOSS\"\n",
        "\n",
        "import editdistance\n",
        "def test_loss(this_dataset=test_dataset_double_kuzari,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_example_batch[i])\n",
        "\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=decode_arr(dense[i]).strip() #SHOULD BE STRING(BLANKS)?\n",
        "                real=decode_arr(target_example_batch[i],targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"LER (label error rate): \",total_accuracy)\n",
        "#  print_log(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "\n",
        "#test_loss(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f",
        "colab_type": "text"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DGjAhEUfxbRc",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GUIDE TEXT\"\n",
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "###TODO : change hebrew insertion to \"H\"\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guid_text='''כנת איהא אלתלמיד' אלעזיז עברית-ר' עברית-יוסף עברית-ש\"צ עברית-ב\"ר \n",
        "עברית-יהודה עברית-נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "import re\n",
        "guid_text=re.sub(r'עברית-[^\\s]+', 'H',guid_text)\n",
        "print(guid_text)\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר עברית-פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF TEST_GUIDE\"\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return test__CTC_word_multiline(guid_text,num_of_paths,BATCH_SIZE)\n",
        "print_log_screen(test_guide())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ",
        "colab_type": "text"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF pretrain_letters\"\n",
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    if STATEFUL:\n",
        "      hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)       \n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test__CTC_letters()\n",
        "    print_log_screen(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"TRAIN SINGLE LETTERS AND TEST LETTERS\"\n",
        "# model=rebuild()\n",
        "# test__CTC_letters()\n",
        "# optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "# pretrain_letters(10,BATCH_SIZE)\n",
        "# test__CTC_letters()\n",
        "# save_checkpoint(\"testing1\")\n",
        "# model=rebuild()\n",
        "# test__CTC_letters()\n",
        "# model=load_checkpoint()\n",
        "# test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du",
        "colab_type": "text"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"DEF main TRAIN_LOOP\"\n",
        "\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train_loop(cur_dataset=dataset_double_kuzari,stop_loop=10000000000):\n",
        "  # Training step  \n",
        "  global GLOBAL_epoch\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initially hidden is None\n",
        "  if STATEFUL:\n",
        "    hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?  \n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print_log_screen(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA",
        "colab_type": "text"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"MAIN\"\n",
        "\n",
        "\n",
        "\n",
        "BEST_ACCURACY=1\n",
        "f= open(\"my_log.txt\",\"w+\")\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "#tf.random.set_seed(1)\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "mail_subject=\"NOT STATEFULL: pretrain letters. synt DROPOUT 0.9 no KUZARI\"\n",
        "mail_subject=this_time+\":\"+mail_subject\n",
        "\n",
        "pretrain_letter=15\n",
        "synth=True\n",
        "keep_percent=0.85\n",
        "\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print_log_screen(description)\n",
        "\n",
        "#LOG Stats\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "\n",
        "model=rebuild()\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "if pretrain_letter>0:\n",
        "  #optimizer = tf.train.AdamOptimizer() #adam is a bit better for this #didn't work\n",
        "  print_log_screen(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print_log_screen('-'*200)\n",
        "\n",
        "print_log_screen(\"START TRAIN\")\n",
        "\n",
        "\n",
        "for jjj in range(10): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(3): #iter without sendmail and only partial test\n",
        "    log_flush()\n",
        "    if synth:\n",
        "      dataset_double_synt=gen_dropout_all(sentences,sentences1,sentences2,sentences3,keep=keep_percent)\n",
        "      loss=train_loop(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train_loop(dataset_double_kuzari)\n",
        "    #total_test_loss,total_accuracy=test_loss(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    total_test_loss,total_accuracy=test_loss(limit=5)\n",
        "    test_loss(this_dataset=test_dataset_double_rasag,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "\n",
        "\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print_log_screen('-'*200)\n",
        "    test_kfir(kfir_kuzari_test,False)\n",
        "    #test_kfir(kfir_kuzari_test)\n",
        "    test_kfir(kfir_rasag_test,False)\n",
        "    #test_kfir(kfir_rasag_test)\n",
        "    print_log_screen('-'*200)\n",
        "  print_log('FULL STATISTICS')\n",
        "  print_log('='*200)\n",
        "  test_kfir(kfir_kuzari_test,False,True)\n",
        "  #test_kfir(kfir_kuzari_test)\n",
        "  test_kfir(kfir_rasag_test,False,True)\n",
        "  #test_kfir(kfir_rasag_test)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test_loss()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test_loss(test_dataset_double_rasag)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  if all_accuracy<BEST_ACCURACY:\n",
        "    save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "    BEST_ACCURACY=all_accuracy\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print_log(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print_log(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print_log(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  f.write(\"EPOCH \"+str(GLOBAL_epoch)+'\\n')\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  f.flush()\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "f.close()\n",
        "close_log()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwpwDinoeIu5",
        "colab_type": "text"
      },
      "source": [
        "#MAIN OUTPUT (ABOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_loss()\n",
        "#test_loss(test_dataset_double_rasag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   train_loop()\n",
        "#   test_loss(single_words_test_dataset,limit=5)\n",
        "#   test_loss(limit=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # test_loss(only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double_kuzari,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double_rasag,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe",
        "colab_type": "text"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zcZ9IdHq23e",
        "colab_type": "text"
      },
      "source": [
        "#OLD STAFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"SMALL TRY\"\n",
        "\n",
        "# this_string='وأهل الأديان ثمّ على'\n",
        "# this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "# this_string='ثمّ'\n",
        "\n",
        "# #this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string=normalize_unicode(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"TRY SYNTH\"\n",
        "\n",
        "# #ACTIVATE\n",
        "# ibnsina_text=load_lines_synth()\n",
        "\n",
        "# #STATSTICS OF SYNTH TEXT\n",
        "# sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "# print_log(\"NOT IN LETTER LIST:\")\n",
        "# for c in sina_vocab:\n",
        "#    if c not in targ_lang.char2idx:\n",
        "#       print_log(\"(\",c,\")\")\n",
        "\n",
        "# print_log(\"\\nLETTER COUNTS\")\n",
        "# for i in range(len(sina_vocab)):\n",
        "#   print_log(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   \n",
        "\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "# #sina_words=ibnsina_text.split(\" \")\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "# sina_words=ibnsina_text.split() #for removing also newlines\n",
        "# print_log(ibnsina_text[:100])\n",
        "# sina_words[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA",
        "colab_type": "text"
      },
      "source": [
        "##Shuffled test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"SHUFFLED TEST\"\n",
        "\n",
        "# def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "#   val_inputs_list=[]\n",
        "#   val_outputs_list=[]\n",
        "\n",
        "#   for i,j,l1,l2 in test_dataset:\n",
        "#     for tt in range(BATCH_SIZE):\n",
        "#      # print_log(i[tt],j[tt])\n",
        "#       i_prediction=decode_JA(tf.constant(i[tt]),l1[tt])\n",
        "#       j_prediction=decode_arr(tf.constant(j[tt]),l2[tt])      \n",
        "#       if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "#         val_inputs_list+=i_prediction.split()\n",
        "#         val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "#   print_log(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "#   word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "#   print_log(\"BEFORE SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   random.shuffle(word_pairs)\n",
        "#   print_log(\"AFTER SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   return word_pairs\n",
        "\n",
        "# #TESTING\n",
        "# #word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag.take(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF GET SHUFFLE DATA\"\n",
        "# def get_shuffled_data(word_pairs):\n",
        "\n",
        "#   accum=0\n",
        "#   heb_acum=\"\"\n",
        "#   arab_acum=\"\"\n",
        "#   results_line=[]\n",
        "#   for i,j in word_pairs:\n",
        "#     if accum>19:\n",
        "#       results_line.append(undouble_hebrew(heb_acum)+'\\t'+arab_acum)\n",
        "#       assert(len(i)%2==0)\n",
        "#       accum=len(i)/2\n",
        "#       heb_acum=i\n",
        "#       arab_acum=j\n",
        "#     else:\n",
        "#       heb_acum+=\" \"+i\n",
        "#       arab_acum+=\" \"+j \n",
        "#       assert(len(i)%2==0)\n",
        "#       accum += len(i)/2 + 1;\n",
        "#   results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "#   print_log(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "#   input_tensor_shuffle, target_tensor_shuffle \\\n",
        "#   ,input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "#   print_log(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "#   print_log(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "#   BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "#   shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "#                                                             target_tensor_shuffle,\n",
        "#                                                             input_lenghts_shuffle,\n",
        "#                                                             target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#   shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#   return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CELL_NAME=\"GEN SHUFFLED DATA\"\n",
        "\n",
        "# word_pairs=get_shuffled_word_pairs(test_dataset_double_kuzari)\n",
        "# shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "# view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "# word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag)\n",
        "# shuffle_test_dataset_double_rasag=get_shuffled_data(word_pairs1)\n",
        "# view_data(shuffle_test_dataset_double_rasag)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9",
        "colab_type": "text"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"DEF TEST_SHUFFLE\"\n",
        "# def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "#   return test_loss(this_dataset=data,limit=limit)\n",
        "# #shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Single words test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CELL_NAME=\"GEN SINGEL WORDS\"\n",
        "\n",
        "# # results_line=[]\n",
        "# # for i,j in word_pairs:\n",
        "# #     results_line.append(undouble_hebrew(i)+'\\t'+j)\n",
        "\n",
        "# # print_log(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "# # input_tensor_shuffle, target_tensor_shuffle \\\n",
        "# # , input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "# # print_log(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "# # BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "# # single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "# #                                                                 target_tensor_shuffle,\n",
        "# #                                                                 input_lenghts_shuffle,\n",
        "# #                                                                 target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# # single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# # view_data(single_words_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}