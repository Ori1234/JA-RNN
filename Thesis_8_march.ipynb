{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY",
        "colab_type": "text"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i) :\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
        "\n",
        "THIS console.log(\"Working\");document.querySelector(\"colab-connect-button\")\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window. It would keep on clicking the page and prevent it from disconnecting.\n",
        "\n",
        "\n",
        "\n",
        "Note: Although I did the same thing, I forgot abt it for 12 hours and got my GPU privileges suspended temporarily. Make sure you dont run anything for more than 12 hrs on Colab!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE",
        "colab_type": "text"
      },
      "source": [
        "**SUMMERY**\n",
        "\n",
        "\n",
        "\n",
        "this is based on https://colab.research.google.com/drive/1m6VuABiVrkKmhUGitHFDl99r6-qe_ZoJ#scrollTo=OHn4Dct23jEm at tirza's account titled:\n",
        "FINAL 4CURRENT__CLEAN_Copy_of_CTC_UnEquel_Input_Output_heb_to_ar.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp",
        "colab_type": "text"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY",
        "colab_type": "text"
      },
      "source": [
        "Global Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64*2 #original 64*2 but don't have enough data right now\n",
        "\n",
        "this_notebook=\"/gdrive/My Drive/RMSPROP Back to Thesis.ipynb\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH_iDyGn0mWh",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "#The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw",
        "colab_type": "text"
      },
      "source": [
        "Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g",
        "colab_type": "text"
      },
      "source": [
        "Arabic Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK",
        "colab_type": "text"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l17b5XJR_5Mj",
        "colab_type": "text"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LTRchar=u'\\u202B'\n",
        "#https://stackoverflow.com/questions/50975763/how-right-to-left-rtl-google-colaboratory\n",
        "#https://stackoverflow.com/questions/51576756/display-render-an-html-file-inside-jupyter-notebook-on-google-colab-platform\n",
        "#https://stackoverflow.com/questions/42556063/right-to-left-and-left-to-right-printed-nicely\n",
        "\n",
        "\n",
        "##HELPERS\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print(len(idx))\n",
        "      if leng==-1:\n",
        "       # print(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "   #   print(result,\"#\"+str(leng))\n",
        "      return result\n",
        "\n",
        "def view_data(data):\n",
        "  for i,j,l1,l2 in data.take(3):\n",
        "    print(LTRchar,print_by_idx_CTC(i[0],inp_lang.idx2char,l1[0]),\" | \",print_by_idx_CTC(j[0],targ_lang.idx2char,l2[0]))\n",
        "\n",
        "\n",
        "#s is a sentence\n",
        "#dict as a dictionary that maps chars to ints\n",
        "# def vectorize(s,dict):\n",
        "#   return [dict[c] for c in s]\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print(LTRchar,s,\":\", c,\"not in dict\")\n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def un_double_letters(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "def init_log():\n",
        "  global fLog\n",
        "  fLog= open(\"NEW_all_log.txt\",\"w+\")\n",
        "\n",
        "\n",
        "def print_to_log(s):  \n",
        "  print(s)\n",
        "  fLog.write(s+\"\\n\")\n",
        "  fL.flush\n",
        "\n",
        "BLANK=\"_\"\n",
        "def clear_blank(s):\n",
        "  return s.replace(\"_\",\"\")\n",
        "\n",
        "def clear_arab_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF",
        "colab_type": "text"
      },
      "source": [
        "send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "  #msg = \"COLAB WORK FINISH ALERT!\"\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  #files=[\"/content/sample_data/README.md\",\"/content/train.png\"]  #list of graphs to send or logs....\n",
        "  files=[\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  #files.append(this_notebook)\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  #server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg)\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()\n",
        "#send_results(\"test\",\"test body\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP",
        "colab_type": "text"
      },
      "source": [
        "plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    text=text.replace('ֿ',\"'\")   ##I ADDED THIS. than did the replacement in the file uploaded to drive\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n') \n",
        "  print(lines)\n",
        "  print(len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "###letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all letters\n",
        "\n",
        "\n",
        "\n",
        "#arab_letters=\"آأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي\" \n",
        "#heb_by_order=\"אאואיאבה'תת'ג'חכ'דד'רזסשצצ'טט'עגפקכלמנהויי\"\n",
        "#TODO EDIT: simple map will take first char in maped list for each hebrew letter\n",
        "\n",
        "\n",
        "tag=\"'\"\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"\n",
        "\n",
        "\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآأ\", # mising alif wasla  \n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "\n",
        "for i in additional_letters:\n",
        "  letter_dict[i]=i\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print(len(arab_heb_maping))\n",
        "print(arab_heb_maping)\n",
        "print(len(heb_arab_maping))\n",
        "print(heb_arab_maping)\n",
        "\n",
        "\n",
        "\n",
        "# arab_letters+=additional_letters\n",
        "# heb_by_order+=additional_letters\n",
        "\n",
        "\n",
        "# heb_by_order_tag=[]\n",
        "# iterator = iter(range(len(heb_by_order)))\n",
        "\n",
        "# for i in iterator:\n",
        "#   if i+1!=len(heb_by_order) and heb_by_order[i+1]==tag:\n",
        "#     heb_by_order_tag.append(heb_by_order[i]+tag)\n",
        "#     next(iterator, None)\n",
        "#   else:\n",
        "#     heb_by_order_tag.append(heb_by_order[i])\n",
        "\n",
        "# len(arab_letters)\n",
        "\n",
        "\n",
        "# arab_heb_maping=dict(zip(arab_letters,heb_by_order_tag))\n",
        "\n",
        "# print(len(arab_heb_maping))\n",
        "# print(arab_heb_maping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adVEVZ_vYdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TODO need to edit this. this is for baseline\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31",
        "colab_type": "text"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    s = s.rstrip().strip()#.translate(str.maketrans('', '', \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"))\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "arr=\"بأفراد كانوا لباباً \"\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "\n",
        "def preprocess_hebrew(w):\n",
        "    w = unicode_to_ascii(w.strip())    \n",
        "    w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    w = w.replace('ֿ',\"'\")\n",
        "    return w\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "#    Takes a file of <heb, arab> phrases separated by tab\n",
        "#    Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(lines,num_examples=200000000):\n",
        "    word_pairs=[]\n",
        "    for l in lines[:num_examples]:\n",
        "      splited=l.split('\\t')\n",
        "      heb=splited[0]\n",
        "      heb=unicode_to_ascii(heb)\n",
        "      heb=preprocess_hebrew(heb)\n",
        "      heb=double_hebrew(heb)\n",
        "      heb=clear_blank(clear_arab_punctuation(heb))\n",
        "      arr=splited[1]\n",
        "      arr=remove_arab_nikud(arr)\n",
        "      arr=standard_nunization(arr)\n",
        "      arr=clear_blank(clear_arab_punctuation(arr))\n",
        "      arr=unicode_to_ascii(arr) \n",
        "      word_pairs.append([heb,arr])        \n",
        "    return word_pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWRQ3jGAesuv",
        "colab_type": "text"
      },
      "source": [
        "small example for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "this_string='وأهل الأديان ثمّ على'\n",
        "this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "#this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "print(len(this_string))\n",
        "this_string=unicode_to_ascii(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "print(len(this_string))\n",
        "this_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq",
        "colab_type": "text"
      },
      "source": [
        "languageIndex class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ALTERNATIVE language index\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT",
        "colab_type": "text"
      },
      "source": [
        "load_dataset method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#inp_lang = LanguageIndex(heb_by_order)\n",
        "inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys()))\n",
        "#targ_lang = LanguageIndex(arab_letters+u\"\\u0651\"+\"\".join(tanween)+ u\"\\u0621\")# shadda and hamza on line  (arab_nikud)\n",
        "targ_lang = LanguageIndex(\"\".join(arab_heb_maping.keys())+\"\".join(tanween)+ shada + hamza_on_line)\n",
        "\n",
        "\n",
        "def load_dataset(pairs):\n",
        "    # creating cleaned input, output pairs    \n",
        "   # pairs = create_dataset(lines,num_examples)  \n",
        "  \n",
        "    input_tensor = [vectorize(heb,inp_lang.char2idx) for heb, arr in pairs]\n",
        "    input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "    # English sentences\n",
        "    target_tensor = [vectorize(arr,targ_lang.char2idx) for heb, arr in pairs]\n",
        "    target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "    print()\n",
        "    print(LTRchar,pairs[0])\n",
        "    print(input_lenghts[0])\n",
        "    print(target_lengths[0])\n",
        "    print(input_tensor[0])\n",
        "    print(target_tensor[0])\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                  value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    \n",
        "    print()\n",
        "    print(LTRchar,pairs[0])\n",
        "    print(input_lenghts[0])\n",
        "    print(target_lengths[0])\n",
        "    print(input_tensor[0])\n",
        "    print(len(input_tensor[0]))\n",
        "    print(target_tensor[0])\n",
        "    print(len(target_tensor[0]))\n",
        "\n",
        "    return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlQJhoggxw3",
        "colab_type": "text"
      },
      "source": [
        "call load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines=load_lines()\n",
        "#num_examples = 2000000000  #don't limit\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor, target_tensor ,input_lenghts,target_lengths = load_dataset(pairs)\n",
        "lines=load_lines(haemunot)\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = load_dataset(pairs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN",
        "colab_type": "text"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths):\n",
        "  \n",
        "  input_tensor_train, input_tensor_val, \\\n",
        "  target_tensor_train, target_tensor_val, \\\n",
        "  input_lengths_train, input_lengths_val, \\\n",
        "  target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "                                                              target_tensor,\n",
        "                                                              input_lenghts,\n",
        "                                                              target_lengths, test_size=0.2)\n",
        "  print(len(input_tensor_train), \n",
        "        len(target_tensor_train), \n",
        "        len(input_tensor_val), \n",
        "        len(target_tensor_val))\n",
        "  \n",
        "  BUFFER_SIZE = len(input_tensor_train)\n",
        "  #N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "  #vocab_inp_size = len(inp_lang.char2idx)\n",
        "  #vocab_tar_size = len(targ_lang.char2idx)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "                                                target_tensor_train,\n",
        "                                                input_lengths_train,\n",
        "                                                target_lengths_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "                                                     target_tensor_val,\n",
        "                                                     input_lengths_val,\n",
        "                                                     target_lengths_val)).shuffle(BUFFER_SIZE)                                                  \n",
        "  \n",
        "  dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return dataset_double,test_dataset_double\n",
        "\n",
        "#ACTIVATE\n",
        "dataset_double,test_dataset_double=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "view_data(test_dataset_double)\n",
        "dataset_double1,test_dataset_double1=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "view_data(test_dataset_double1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA",
        "colab_type": "text"
      },
      "source": [
        "##Shuffled test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "  val_inputs_list=[]\n",
        "  val_outputs_list=[]\n",
        "\n",
        "  for i,j,l1,l2 in test_dataset:\n",
        "    for tt in range(BATCH_SIZE):\n",
        "     # print(i[tt],j[tt])\n",
        "      i_prediction=print_by_idx_CTC(tf.constant(i[tt]),inp_lang.idx2char,l1[tt])\n",
        "      j_prediction=print_by_idx_CTC(tf.constant(j[tt]),targ_lang.idx2char,l2[tt])      \n",
        "      if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "        val_inputs_list+=i_prediction.split()\n",
        "        val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "  print(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "  word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "  print(\"BEFORE SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print(i)\n",
        "  random.shuffle(word_pairs)\n",
        "  print(\"AFTER SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print(i)\n",
        "  return word_pairs\n",
        "\n",
        "#TESTING\n",
        "#word_pairs1=get_shuffled_word_pairs(test_dataset_double1.take(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shuffled_data(word_pairs):\n",
        "\n",
        "  accum=0\n",
        "  heb_acum=\"\"\n",
        "  arab_acum=\"\"\n",
        "  results_line=[]\n",
        "  for i,j in word_pairs:\n",
        "    if accum>19:\n",
        "      results_line.append(un_double_letters(heb_acum)+'\\t'+arab_acum)\n",
        "      assert(len(i)%2==0)\n",
        "      accum=len(i)/2\n",
        "      heb_acum=i\n",
        "      arab_acum=j\n",
        "    else:\n",
        "      heb_acum+=\" \"+i\n",
        "      arab_acum+=\" \"+j \n",
        "      assert(len(i)%2==0)\n",
        "      accum += len(i)/2 + 1;\n",
        "  results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "  print(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "  input_tensor_shuffle, target_tensor_shuffle \\\n",
        "  ,input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "  print(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "  print(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "  BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "  shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "                                                            target_tensor_shuffle,\n",
        "                                                            input_lenghts_shuffle,\n",
        "                                                            target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "  shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ACTIVATE\n",
        "word_pairs=get_shuffled_word_pairs(test_dataset_double)\n",
        "shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "word_pairs1=get_shuffled_word_pairs(test_dataset_double1)\n",
        "shuffle_test_dataset_double1=get_shuffled_data(word_pairs1)\n",
        "view_data(shuffle_test_dataset_double1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Single words test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_line=[]\n",
        "for i,j in word_pairs:\n",
        "    results_line.append(un_double_letters(i)+'\\t'+j)\n",
        "\n",
        "print(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "input_tensor_shuffle, target_tensor_shuffle \\\n",
        ", input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "print(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "                                                                target_tensor_shuffle,\n",
        "                                                                input_lenghts_shuffle,\n",
        "                                                                target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "view_data(single_words_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8",
        "colab_type": "text"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO edit when time permits\n",
        "\n",
        "def load_text_for_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(ibnsina, 'rb') as f:\n",
        "      ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  ibnsina_text=clear_arab_punctuation(ibnsina_text)\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  import re\n",
        "  ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "  ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "  return ibnsina_text\n",
        "\n",
        "\n",
        "#ACTIVATE\n",
        "ibnsina_text=load_text_for_synth()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBvUvPHKLCIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ibnsina_text[:250]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STATSTICS OF SYNTH TEXT\n",
        "sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "print(\"NOT IN LETTER LIST:\")\n",
        "for c in sina_vocab:\n",
        "   if c not in targ_lang.char2idx:\n",
        "      print(\"(\",c,\")\")\n",
        "\n",
        "print(\"\\nLETTER COUNTS\")\n",
        "for i in range(len(sina_vocab)):\n",
        "  print(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM7xIKZBLOd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "#sina_words=ibnsina_text.split(\" \")\n",
        "#ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "sina_words=ibnsina_text.split() #for removing also newlines\n",
        "print(ibnsina_text[:100])\n",
        "sina_words[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_synth_sentences(sina_words):\n",
        "\n",
        "  #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in sina_words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      sentences.append(unicode_to_ascii(remove_arab_nikud(\" \".join(res))))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "    #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "  return sentences\n",
        "\n",
        "#ACTIVATE\n",
        "sentences=gen_synth_sentences(sina_words)\n",
        "len(sentences)\n",
        "sentences[:5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# arab_setences=[]\n",
        "# heb_sentences=[]\n",
        "\n",
        "def gen_dropout(sentences,keep_prob=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)        \n",
        "    heb=[]\n",
        "    for c in arr:\n",
        "      if c in arab_heb_maping:\n",
        "        if (np.random.binomial(1,keep_prob)) or c==\" \":\n",
        "          heb.append(arab_heb_maping[c])\n",
        "        else:\n",
        "          heb.append(BLANK)\n",
        "\n",
        "    #print(\"\".join(heb))\n",
        "    heb_sentences.append(double_hebrew(preprocess_hebrew(\"\".join(heb))))\n",
        "\n",
        "  print(heb_sentences[:5]) \n",
        "  print(arab_setences[:5])\n",
        "\n",
        "  print(len(arab_setences))\n",
        "  input_tensor_synth, target_tensor_synth, \\\n",
        "  input_lenghts_synth,target_lengths_synth = load_dataset(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # Show length\n",
        "  print(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  print(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "                                                      target_tensor_synth,\n",
        "                                                      input_lenghts_synth,\n",
        "                                                      target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  #dataset_double_synt=dataset_double_synt.concatenate(dataset_double).shuffle(BUFFER_SIZE)\n",
        "  return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8r6AS2jMtQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # def remove_chars_not_in_dict(s,dict):\n",
        "# #   #print(s)\n",
        "# #   res=''\n",
        "# #   for c in s:\n",
        "# #     if c in dict:\n",
        "# #       res+=c\n",
        "# #   return res\n",
        "\n",
        "# def print_show(i):\n",
        "#     print(len(heb_sentences),len(arab_setences))\n",
        "#     print(heb_sentences[i],arab_setences[i])\n",
        "#     print(input_lenghts[i])\n",
        "#     print(target_lengths[i])\n",
        "#     print(input_tensor[i])\n",
        "#     print(target_tensor[i])\n",
        "\n",
        "\n",
        "# def load_dataset_synth(arab_setences,heb_sentences):\n",
        "#     # creating cleaned input, output pairs    \n",
        "#     print(heb_sentences[0])\n",
        "#     input_tensor = [vectorize(heb,inp_lang.char2idx) for heb in heb_sentences]\n",
        "#     print(input_tensor[0])\n",
        "#     input_lenghts=[len(heb) for heb in heb_sentences]\n",
        "  \n",
        "#     target_tensor = [vectorize(arr,targ_lang.char2idx) for arr in arab_setences]\n",
        "#     target_lengths = [len(arr)  for  arr in arab_setences]\n",
        "#     print(len(arab_setences))\n",
        "#     print(len(target_tensor))\n",
        "\n",
        "#     # Padding the input and output tensor to the maximum length\n",
        "#     input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "#                                                                  maxlen=max_length(input_tensor),\n",
        "#                                                                  padding='post',\n",
        "#                                                                   value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "#     target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "#                                                                   maxlen=max_length(target_tensor), \n",
        "#                                                                   padding='post',\n",
        "#                                                                   value=targ_lang.char2idx[BLANK])\n",
        "#     print(len(target_tensor))\n",
        "#    # print_show(0)\n",
        "\n",
        "#     return input_tensor, target_tensor , input_lenghts,target_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_dropout_all(keep=1):\n",
        "  synth_text=load_text_for_synth()\n",
        "  sentences=gen_synth_sentences(synth_text.split())\n",
        "  dataset_double_synt=gen_dropout(sentences,keep)\n",
        "\n",
        "  path1=\"/gdrive/My Drive/JUDEO-ARAB/daruri-IR.txt\"\n",
        "  synth_text1=load_text_for_synth(path1)\n",
        "  sentences1=gen_synth_sentences(synth_text1.split())\n",
        "  dataset_double_synt1=gen_dropout(sentences1,keep).concatenate(dataset_double_synt)\n",
        "\n",
        "  path2=\"/gdrive/My Drive/JUDEO-ARAB/farabi-tahsil.txt\"\n",
        "  synth_text2=load_text_for_synth(path2)\n",
        "  sentences2=gen_synth_sentences(synth_text2.split())\n",
        "  dataset_double_synt2=gen_dropout(sentences2,keep).concatenate(dataset_double_synt1)\n",
        "\n",
        "\n",
        "  path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "  synth_text3=load_text_for_synth(path3)\n",
        "  sentences3=gen_synth_sentences(synth_text3.split())\n",
        "  dataset_double_synt3=gen_dropout(sentences2,keep).concatenate(dataset_double_synt2)\n",
        "\n",
        "\n",
        "  return dataset_double_synt3.concatenate(dataset_double).shuffle(BUFFER_SIZE)\n",
        "view_data(gen_dropout_all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4",
        "colab_type": "text"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NETWORK PARAMS\n",
        "# The embedding dimension\n",
        "embedding_dim = 8 #256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 #1024\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  #rnn = tf.keras.layers.CuDNNGRU\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "  #rnn = tf.keras.layers.LSTM #see https://stackoverflow.com/questions/55761337/module-tensorflow-python-keras-api-v2-keras-layers-has-no-attribute-cudnnlst\n",
        "\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "#FUNCTION TO BUILD MODEL\n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    \n",
        "  \n",
        "\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "model=rebuild()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG",
        "colab_type": "text"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from termcolor import colored\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "#USEAGE:\n",
        "s1=\"لامة النصارى واستحقو\" \n",
        "s2=\"لأمّة النصارى واستحقوّا\"\n",
        "a,b=show_diff(s1,s2,'blue')\n",
        "#print(\"\".join(b))\n",
        "print(a,\"|\",b)\n",
        "\n",
        "#print('\\x1b[31mّ\\x1b[0m')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo",
        "colab_type": "text"
      },
      "source": [
        "## forward run each letter seperatly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test__CTC_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "       print(\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print(\"####max is the blank symbole\")\n",
        "      print('-'*10)\n",
        "  \n",
        "#test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF",
        "colab_type": "text"
      },
      "source": [
        "##forward run a sentence (with top n beam search results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    l = double_hebrew(l)\n",
        "    v=vectorize(l,inp_lang.char2idx)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "#  inp=[] \n",
        "#  double=[]\n",
        "  #num_of_letters=0\n",
        "  #for c in word_str:\n",
        "  #  double.append(inp_lang.char2idx[c])\n",
        "  #  num_of_letters+=1\n",
        "  #  if not c==' ':\n",
        "  #    double.append(inp_lang.char2idx[c])\n",
        "  #    num_of_letters+=1\n",
        "  #inp.append(double)\n",
        "  #inp.append([inp_lang.char2idx[c] for c in word_str])\n",
        "  #for i in range(_BATCH_SIZE-1):\n",
        "  #  inp.append([0]*num_of_letters)\n",
        "  \n",
        " # num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "  #print(num_of_letters)\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  #print(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "  #inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  # if print_deteils:\n",
        "  #   print('#####deteils')\n",
        "  # for jj in range(num_of_letters):\n",
        "  #     if print_deteils:\n",
        "  #       print(word_str[jj])\n",
        "      \n",
        "  #     pred_distr=predict_ltrs[0][jj]\n",
        "  #     pred_distr=tf.nn.softmax(pred_distr)\n",
        "  #     if print_deteils:\n",
        "  #       for ii in range(num_of_arab_letters):\n",
        "  #         print(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "  #       print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "  #       print()\n",
        "  #     maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "  #     if print_deteils:\n",
        "  #       print(maximum)\n",
        "  #     if (maximum<42):\n",
        "  #       if print_deteils:\n",
        "  #         print(targ_lang.vocab[maximum])\n",
        "  #       res.append(targ_lang.vocab[maximum])\n",
        "  #     else:\n",
        "  #       if print_deteils:\n",
        "  #         print(\"####max is the blank symbole\")\n",
        "  #       res.append(\"-\")\n",
        "  #     if print_deteils:\n",
        "  #       print('-'*10)\n",
        "  # print(word_str)\n",
        "  #print(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=print_by_idx_CTC(dense[t],targ_lang.idx2char)\n",
        "      print(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''כמאלא\n",
        "כמאלא'''\n",
        "\n",
        "print(test__CTC_word_multiline(lines,3,BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl24mg1r2EKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  res=[]\n",
        "  inp=[] \n",
        "  double=[] \n",
        "  num_of_letters=0\n",
        "  for c in word_str:\n",
        "    double.append(inp_lang.char2idx[c])\n",
        "    num_of_letters+=1\n",
        "    if not c==' ':\n",
        "      double.append(inp_lang.char2idx[c])\n",
        "      num_of_letters+=1\n",
        "  inp.append(double)\n",
        "  #inp.append([inp_lang.char2idx[c] for c in word_str])\n",
        "  for i in range(_BATCH_SIZE-1):\n",
        "    inp.append([0]*num_of_letters)\n",
        "  \n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "  #print(num_of_letters)\n",
        "  \n",
        "  letters_tensor=tf.convert_to_tensor(inp)\n",
        "    \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  #print(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "  inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  if print_deteils:\n",
        "    print('#####deteils')\n",
        "  for jj in range(num_of_letters):\n",
        "      if print_deteils:\n",
        "        print(word_str[jj])\n",
        "      \n",
        "      pred_distr=predict_ltrs[0][jj]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      if print_deteils:\n",
        "        for ii in range(num_of_arab_letters):\n",
        "          print(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "        print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "        print()\n",
        "      maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "      if print_deteils:\n",
        "        print(maximum)\n",
        "      if (maximum<42):\n",
        "        if print_deteils:\n",
        "          print(targ_lang.vocab[maximum])\n",
        "        res.append(targ_lang.vocab[maximum])\n",
        "      else:\n",
        "        if print_deteils:\n",
        "          print(\"####max is the blank symbole\")\n",
        "        res.append(\"-\")\n",
        "      if print_deteils:\n",
        "        print('-'*10)\n",
        "  print(word_str)\n",
        "  print(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "  for i in reversed(range(num_of_paths)):\n",
        "    dense=tf.sparse.to_dense(decoded[i])   \n",
        "    prediction=print_by_idx_CTC(dense[0],targ_lang.idx2char)\n",
        "    print(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "  return prediction\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "#test__CTC_word(\"כמאלא\",3,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L",
        "colab_type": "text"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "def test_loss_baseline(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print(\"LER (label error rate): \",total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy\n",
        "#test_loss_baseline(limit=3)\n",
        "test_loss_baseline(test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74",
        "colab_type": "text"
      },
      "source": [
        "##test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "def test_loss(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=print_by_idx_CTC(dense[i],targ_lang.idx2char).strip()\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print(\"LER (label error rate): \",total_accuracy)\n",
        "#  print(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "#test_loss(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f",
        "colab_type": "text"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DGjAhEUfxbRc",
        "colab": {}
      },
      "source": [
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guid_text='''כנת איהא אלתלמיד' אלעזיז ר' יוסף ש\"צ ב\"ר \n",
        "יהודה נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guid_lines=guid_text.split('\\n')\n",
        "\n",
        "# def test_guide(limit=1000000,num_of_paths=1):\n",
        "#   line_res=[]\n",
        "#   count=0\n",
        "#   for l in guid_lines:    \n",
        "#     if count>limit:\n",
        "#       break\n",
        "#     l = l.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ').replace('\"',\"\").replace('ֿ',\"'\")\n",
        "#     l_res=test__CTC_word(l,num_of_paths=num_of_paths)\n",
        "#     line_res.append(l_res)\n",
        "#     count+=1\n",
        "#   print('\\n'.join(line_res))\n",
        "#   return '\\n'.join(line_res)\n",
        "\n",
        "# print(test_guide(limit=1))\n",
        "# #print(test_guide())\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return test__CTC_word_multiline(guid_text,num_of_paths,BATCH_SIZE)\n",
        "print(test_guide())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9",
        "colab_type": "text"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "  return test_loss(this_dataset=data,limit=limit)\n",
        "#shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ",
        "colab_type": "text"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)\n",
        "          # print(heb_res)\n",
        "          # print(arab_res)\n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            #cost = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test__CTC_letters()\n",
        "    print(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model=rebuild()\n",
        "# optimizer = tf.train.AdamOptimizer()  #need to reset oprimazer between trains????\n",
        "# pretrain_letters(10,BATCH_SIZE)\n",
        "# test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du",
        "colab_type": "text"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBAL_epoch=0\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train_loop(cur_dataset=dataset_double,stop_loop=10000000000):\n",
        "  # Training step  \n",
        "  global GLOBAL_epoch\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initially hidden is None\n",
        "  hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?  \n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            #seqs_lens=logit_lens=[101]*BATCH_SIZE  \n",
        "            #loss=tf.nn.ctc_loss_v2(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE",
        "colab_type": "text"
      },
      "source": [
        "CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #DEFINE CHECKPOINT CALLBACK\n",
        "\n",
        "# # Directory where the checkpoints will be saved\n",
        "# checkpoint_dir = './training_checkpoints'\n",
        "# # Name of the checkpoint files\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# # checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "# #     filepath=checkpoint_prefix,\n",
        "# #     save_weights_only=True)\n",
        "\n",
        "\n",
        "# 2)\n",
        "#       #       model.save_weights(checkpoint_prefix.format(epoch=epoch)+'synth')\n",
        "# 3)\n",
        "\n",
        "# # print (tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# # model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# # model.build(tf.TensorShape([BATCH_SIZE, None]))\n",
        "\n",
        "\n",
        "# #ref :https://colab.research.google.com/github/tensorflow/models/blob/master/samples/core/tutorials/keras/save_and_restore_models.ipynb#scrollTo=R7W5plyZ-u9X\n",
        "\n",
        "# # Save the weights\n",
        "# model.save_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# # Restore the weights\n",
        "# model = create_model()\n",
        "# model.load_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# loss,acc = model.evaluate(test_images, test_labels)\n",
        "# print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA",
        "colab_type": "text"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBAL_epoch=0\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "mail_subject=\"pretrain letters. synt DROPOUT 0.85. STANDARD TANWIN\"\n",
        "mail_subject=\"JUST_TESTING\"+mail_subject\n",
        "\n",
        "pretrain_letter=15\n",
        "synth=True\n",
        "keep_percent=0.85\n",
        "\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print(description)\n",
        "\n",
        "#LOG Stats\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer()  #need to reset oprimazer between trains????\n",
        "\n",
        "model=rebuild()\n",
        "#optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "if pretrain_letter>0:\n",
        "  #optimizer = tf.train.AdamOptimizer() #adam is a bit better for this #didn't work\n",
        "  print(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print('-'*200)\n",
        "\n",
        "print(\"START TRAIN\")\n",
        "#optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "\n",
        "#dataset_double_synt=gen_dropout(keep_percent)\n",
        "for jjj in range(10): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(5): #iter without sendmail and only partial test\n",
        "    if synth:\n",
        "      dataset_double_synt=gen_dropout_all(keep_percent)\n",
        "      loss=train_loop(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train_loop(dataset_double)\n",
        "    #total_test_loss,total_accuracy=test_loss(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    #total_test_loss,total_accuracy=test_loss(limit=5)\n",
        "    #test_loss(this_dataset=test_dataset_double1,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "\n",
        "\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print('-'*200)\n",
        "  print('FULL STATISTICS')\n",
        "  print('='*200)\n",
        "\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test_loss()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test_loss(test_dataset_double1)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  f= open(\"my_log.txt\",\"w+\")\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "  f.close()\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_loss()\n",
        "#test_loss(test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   train_loop()\n",
        "#   test_loss(single_words_test_dataset,limit=5)\n",
        "#   test_loss(limit=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # test_loss(only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double1,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe",
        "colab_type": "text"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    }
  ]
}