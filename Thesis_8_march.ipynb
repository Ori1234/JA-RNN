{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ori1234/JA-RNN/blob/master/Thesis_8_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i)\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window.\n",
        "\n",
        "\n",
        "Note to self: Make sure you dont run anything for more than 12 hrs on Colab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE"
      },
      "source": [
        "# GLOBALS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-HXxiSUHDC9"
      },
      "source": [
        "#@title GLOBALS { run: \"auto\" }\n",
        "INSPECT = False #@param {type:\"boolean\"}\n",
        "no_tags = True #@param {type:\"boolean\"}\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTdNSHUmCvD"
      },
      "source": [
        "#IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr"
      },
      "source": [
        "CELL_NAME=\"IMPORTS\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import math\n",
        "from termcolor import colored, cprint\n",
        "import editdistance\n",
        "\n",
        "\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrDNbLhC7H"
      },
      "source": [
        "#GLOBAL VARS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fPaxl8g_BK"
      },
      "source": [
        "this_time=str(datetime.now())\n",
        "GLOBAL_epoch=0\n",
        "DEBUG=False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AndYz46gsER"
      },
      "source": [
        "#LOGGING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldvhaPtVF0Kd"
      },
      "source": [
        "######To clean logs\n",
        "######!rm log*.log"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxiE-PU7A-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5ce02b-c231-4ffe-c0da-99454e7072ba"
      },
      "source": [
        "CELL_NAME=\"START LOG\"\n",
        "\n",
        "\n",
        "log_file=\"LOG___\"+this_time+\".txt\"\n",
        "f_logg= open(log_file,\"w\")\n",
        "print(\"logging to file (will be added to mail)\",log_file)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _print_log(also_print,*txts):  \n",
        "  txt=\"\"\n",
        "  for t in txts:\n",
        "    txt+=\" \"+str(t)\n",
        "  f_logg.write(CELL_NAME+\" \"+str(datetime.now())+\": \"+txt+'\\n') #TODO ADD TIME!!!!\n",
        "  if also_print:\n",
        "    print(*txts)\n",
        "\n",
        "def print_log(*txts):\n",
        "  if (DEBUG):\n",
        "    print(*txts)\n",
        "  _print_log(False,*txts)\n",
        "\n",
        "\n",
        "def print_log_screen(*txts):\n",
        "  _print_log(True,*txts)\n",
        "\n",
        "\n",
        "def log_flush():\n",
        "  f_logg.flush()\n",
        "\n",
        "def close_log():\n",
        "  f_logg.close()\n",
        "\n",
        "\n",
        "\n",
        "#DON'T FORGET TO CLOSE THE FILE AT THE END\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logging to file (will be added to mail) LOG___2021-11-11 15:27:43.761762.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh93f3m4fb3Q"
      },
      "source": [
        "\n",
        "#PAIST HERE TO SEE PROPERLY THE SHOW DIFF PART IN LOGS\n",
        "text='''MAIN:  (1) ‫ אלא באד'נה . פכיפ לא | إلاّ بإذنه . فكيف لا | إلاّ بإذنه . فكيف لا | 0.0000\n",
        "MAIN:  (2) ‫ להמ , ולטלבוא וג'והא | لهم , ولطلبوا وجوها\u001b[1m\u001b[31mً\u001b[0m | لهم , ولطلبوا وجوها | 0.0500\n",
        "MAIN:  (3) ‫ , ודפעהמא אלי מוסי H | , ودفعهما إلى موسى H | , ودفعهما إلى موسى H | 0.0000\n",
        "MAIN:  LER (label error rate):  0.033400332030791034'''\n",
        "if INSPECT:\n",
        "  print(text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421a6990-7c60-4e65-b146-4bdae9c32522"
      },
      "source": [
        "CELL_NAME=\"MOUNT DRIVE\"\n",
        "print(\"mounting to drive at /gdrive\")\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mounting to drive at /gdrive\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY"
      },
      "source": [
        "#MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c6343d-a42f-4c33-9f3f-e849829f7abc"
      },
      "source": [
        "CELL_NAME=\"BATCH_SIZE and STATEFUL\"\n",
        "\n",
        "STATEFUL=False\n",
        "BATCH_SIZE = 128\n",
        "TO_SHUFFLE=True\n",
        "\n",
        "embedding_dim = 8\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "print_log_screen(\"set batch size to \"+str(BATCH_SIZE))\n",
        "print_log_screen(\"STATEFUL: \"+str(STATEFUL))\n",
        "print_log_screen(\"embedding_dim: \"+str(embedding_dim))\n",
        "print_log_screen(\"rnn_units: \"+str(rnn_units))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set batch size to 128\n",
            "STATEFUL: False\n",
            "embedding_dim: 8\n",
            "rnn_units: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw"
      },
      "source": [
        "# INIT RANDOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU"
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "\n",
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "def init_random():\n",
        "  print_log_screen(\"init random to 1\")\n",
        "  np.random.seed(1)\n",
        "  tf.compat.v1.set_random_seed(1)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUbUyZx3i1OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b456acca-d22d-4628-9f3e-361a1c823776"
      },
      "source": [
        "CELL_NAME=\"RANDOM SEED\"\n",
        "init_random()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init random to 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g"
      },
      "source": [
        "#CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz"
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n",
        "\n",
        "LTRchar=u'\\u202B'   #align rtl symbole\n",
        "BLANK=\"_\"\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXctuG0ZIwCT"
      },
      "source": [
        "##show diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X"
      },
      "source": [
        "CELL_NAME=\"DEF show diff\"\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "# #USEAGE:\n",
        "# s1=\"لامة النصارى واستحقو\" \n",
        "# s2=\"لأمّة النصارى واستحقوّا\"\n",
        "# a,b=show_diff(s1,s2,'blue')\n",
        "# #print_log(\"\".join(b))\n",
        "# print_log(a,\"|\",b)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF"
      },
      "source": [
        "##send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s"
      },
      "source": [
        "CELL_NAME=\"SEND MAIL\"\n",
        "\n",
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  files=[log_file,\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP"
      },
      "source": [
        "##plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi"
      },
      "source": [
        "CELL_NAME=\"PLOT\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "#my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "##letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz"
      },
      "source": [
        "CELL_NAME=\"LETTER MAPPING\"\n",
        "\n",
        "tag=\"'\"\n",
        "\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"+tag\n",
        "\n",
        "#\"א\": \"اإآٱأ\", with wasla\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآٱأ\",\n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "#######################################################\n",
        "for c in additional_letters:\n",
        "  letter_dict[c]=c\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print_log(\"arab_heb_maping\",arab_heb_maping)\n",
        "print_log(\"length:\",len(arab_heb_maping))\n",
        "print_log(\"heb_arab_maping\",heb_arab_maping)\n",
        "print_log(\"length:\",len(heb_arab_maping))\n",
        "\n",
        "#################################################################3\n",
        "#FUNCTIONS:\n",
        "\n",
        "def remove_chars_not_in_map(phrase,map):\n",
        "  res=[]\n",
        "  for c in phrase:  \n",
        "    if c in map:\n",
        "      res.append(c)\n",
        "    else:\n",
        "      print_log(LTRchar+\"Skipping char not in predefined map\\n ( \"+c+\" )\\nin sentences:\\n\"+phrase+'\\n')      \n",
        "  return \"\".join(res)\n",
        "\n",
        "def remove_chars_not_in_JA_map(ja):\n",
        "  return remove_chars_not_in_map(ja,heb_arab_maping)\n",
        "\n",
        "extended_arab_chars=list(arab_heb_maping.keys())\n",
        "extended_arab_chars+=tanween\n",
        "extended_arab_chars.append(shada)\n",
        "extended_arab_chars.append(hamza_on_line)\n",
        "\n",
        "def remove_chars_not_in_arab_map(arr):\n",
        "  return remove_chars_not_in_map(arr,extended_arab_chars)\n",
        "\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n",
        "\n",
        "\n",
        "def reverse_simple_map(arr_str):\n",
        "  ja_str=[]\n",
        "  for c in arr_str:\n",
        "    if c in arab_heb_maping:\n",
        "      ja_str.append(arab_heb_maping[c])\n",
        "    #else:\n",
        "      #print_log(\"reverse_simple_map: char ( \"+c+\" ) not in letter mapping and will be skiped\")\n",
        "      #print_log(arr_str)\n",
        "  return \"\".join(ja_str)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtRTahEBmLU-"
      },
      "source": [
        "CELL_NAME=\"DATA PATHS\"\n",
        "\n",
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "kfir_kuzari_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_ORIGINAL.txt\"\n",
        "kfir_rasag_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_ORIGINAL.txt\"\n",
        "kfir_kuzari_test_SWITCH=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\"\n",
        "kfir_rasag_test_SWITCH=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\"\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm"
      },
      "source": [
        "CELL_NAME=\"HELPERS\"\n",
        "\n",
        "##HELPERS\n",
        "\n",
        "\n",
        "# def view_data(data):\n",
        "#   for i,j,l1,l2 in data.take(3):\n",
        "#     print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[0],l1[0])),\" | \",decode_arr(j[0],l2[0]))\n",
        "\n",
        "def view_data(data):\n",
        "  print_log(\"=\"*200)\n",
        "  for i,j,l1,l2 in data.take(1):\n",
        "    for t in range(5):\n",
        "      print_log_screen(LTRchar,undouble_hebrew(decode_JA(i[t],l1[t])),\" | \",decode_arr(j[t],l2[t]))\n",
        "  print_log(\"=\"*200)\n",
        "\n",
        "\n",
        "def clear_blank(s):\n",
        "  return s.replace(BLANK,\"\")\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go"
      },
      "source": [
        "CELL_NAME=\"PREPROCESS SENTENCES\"\n",
        "\n",
        "\n",
        "def normalize_unicode(s):\n",
        "    s = s.strip()\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "#ARABIC:\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def replace_arab_style_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "#CHECK\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "#JUDEO-ARABIC\n",
        "# def preprocess_JA(w):\n",
        "#     w = normalize_unicode(w.strip())\n",
        "#     w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "#     w = w.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "#     return w\n",
        "\n",
        "\n",
        "def dropout(ja_str,keep):\n",
        "  res=[]\n",
        "  for c in ja_str:\n",
        "    if c==\" \":\n",
        "      res.append(c)\n",
        "    elif (np.random.binomial(1,keep)):\n",
        "      res.append(c)\n",
        "    else:\n",
        "      res.append(BLANK)\n",
        "  return \"\".join(res)\n",
        "\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "\n",
        "def undouble_hebrew(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq"
      },
      "source": [
        "##languageIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk"
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print_log(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print_log(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print_log(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print_log(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Ch9lK2o-Jp"
      },
      "source": [
        "CELL_NAME=\"LANGUAGE INDEX\"\n",
        "\n",
        "inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys()))\n",
        "targ_lang = LanguageIndex(\"\".join(extended_arab_chars))\n",
        "inp_lang = LanguageIndex(heb_arab_maping.keys())\n",
        "targ_lang = LanguageIndex(extended_arab_chars)\n",
        "\n",
        "\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print_log(len(idx))\n",
        "      if leng==-1:\n",
        "       # print_log(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "      return result\n",
        "\n",
        "def decode_JA(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,inp_lang.idx2char,leng)\n",
        "\n",
        "def decode_arr(idx,leng=-1):\n",
        "  return print_by_idx_CTC(idx,targ_lang.idx2char,leng)\n",
        "\n",
        "\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print_log_screen(\"VECTORIZE: char not in dict - need to call preprocess_lines before calling produce_dataset \")      \n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def encode_JA(ja):\n",
        "  return vectorize(ja,inp_lang.char2idx)\n",
        "\n",
        "def encode_arr(arr):\n",
        "  return vectorize(arr,targ_lang.char2idx)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7wKGxCzxuE"
      },
      "source": [
        "##FUCNTION FOR GEN DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xjn9m6nyHv7"
      },
      "source": [
        "###SPLIT TEST TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e62bZRaH7L8z"
      },
      "source": [
        "CELL_NAME=\"TEST TRAIN SPLIT\"\n",
        "\n",
        "def split_test_train(input_file,output_train,output_test,test_percent=0.2):\n",
        "  with open(input_file, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')  \n",
        "  text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  test_size=math.floor(num_of_lines*test_percent)\n",
        "  idx=[0]*test_size + [1]*(num_of_lines-test_size)\n",
        "  random.shuffle(idx)\n",
        "  print(idx)\n",
        "  \n",
        "  f_train=open(output_train,'w+')\n",
        "  f_test=open(output_test,'w+')\n",
        "  \n",
        "  train = []\n",
        "  test = []\n",
        "  for a,i in zip(lines,idx):\n",
        "    if i:\n",
        "      train.append(a)\n",
        "      f_train.write(a+'\\n')\n",
        "    else:\n",
        "      test.append(a)\n",
        "      f_test.write(a+'\\n')\n",
        "  \n",
        "  f_train.close()\n",
        "  f_test.close()\n",
        "\n",
        "  print(len(train),train[0:10])\n",
        "  print(len(test),test[0:10])\n",
        "\n",
        "#split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58vNbHqdTly_"
      },
      "source": [
        "#SHOLD ONLY BE CALLED TO RESPLIT TEST AND TRAIN\n",
        "#split_test_train(hakuzari,hakuzari+\".train.txt\",hakuzari+\".test.txt\")\n",
        "#split_test_train(haemunot,haemunot+\".train.txt\",haemunot+\".test.txt\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_YqhOHyNUs"
      },
      "source": [
        "###load_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4"
      },
      "source": [
        "CELL_NAME=\"LOAD_LINES\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  print_log_screen(\"loading text: \"+input_file)\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    #text=text.replace('ֿ',\"'\")   ##allread doing it inside preprocess_JA()\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n')\n",
        "  print_log(\"first lines:\",lines[0:100])\n",
        "  print_log_screen(\"len(lines)\", len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyI8c-qU-DEe"
      },
      "source": [
        "###preprocess_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro14w9OP-Dt_"
      },
      "source": [
        "def preprocess_JA(ja):\n",
        "    ja=replace_arab_style_punctuation(ja)\n",
        "    ja=clear_blank(ja) #acctually should be taken care of by remove_chars_not_in_map ...\n",
        "    ja = normalize_unicode(ja.strip())\n",
        "    ja = ja.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    ja = ja.replace('ֿ',\"'\") #sometimes instead of tag there is a horizontal line above letter\n",
        "    if no_tags:\n",
        "      ja=ja.replace(\"'\",'')\n",
        "    ja=remove_chars_not_in_JA_map(ja)\n",
        "    return ja\n",
        "\n",
        "def preprocess_arr(arr):\n",
        "    arr=remove_arab_nikud(arr)\n",
        "    arr=standard_nunization(arr)\n",
        "    arr=replace_arab_style_punctuation(arr)\n",
        "    arr=clear_blank(arr) #acctually should be taken care of by remove_chars_not_in_map ...\n",
        "    arr=normalize_unicode(arr)     \n",
        "    arr=remove_chars_not_in_arab_map(arr)\n",
        "    return arr\n",
        "\n",
        "def preprocess_lines(ja_arr_lines):\n",
        "  res=[]\n",
        "  for l in ja_arr_lines:\n",
        "    ja,arr=l.split('\\t')\n",
        "\n",
        "    ja=preprocess_JA(ja)\n",
        "      \n",
        "    arr=preprocess_arr(arr)\n",
        "    \n",
        "    res.append(ja+'\\t'+arr)\n",
        "  return res\n",
        "    \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jWF3taWySyb"
      },
      "source": [
        "###create_parralele_phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvF0XWGTzZ7s"
      },
      "source": [
        "# Takes a file of <heb, arab> phrases separated by tab\n",
        "# Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_parralel_phrases(lines,keep=1):\n",
        "    phrase_pairs=[]\n",
        "    for l in lines:\n",
        "      heb,arr=l.split('\\t')\n",
        "      \n",
        "      # heb=clear_blank(replace_arab_style_punctuation(heb)) #TODO needed?\n",
        "      # heb=preprocess_JA(heb)\n",
        "      heb=dropout(heb,keep)\n",
        "      heb=double_hebrew(heb)      \n",
        "\n",
        "      # arr=remove_arab_nikud(arr)\n",
        "      # arr=standard_nunization(arr)\n",
        "      # arr=clear_blank(replace_arab_style_punctuation(arr))\n",
        "      # arr=normalize_unicode(arr) \n",
        "      \n",
        "      phrase_pairs.append([heb,arr])        \n",
        "    return phrase_pairs\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT"
      },
      "source": [
        "###produce_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuiTkrWRLYG1"
      },
      "source": [
        "CELL_NAME=\"produce_dataset\"\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def produce_dataset(parallel_phrase,to_shuffle=False):\n",
        "  \n",
        "    input_tensor = [encode_JA(heb) for heb, arr in parallel_phrase]\n",
        "    input_lengths=[len(heb) for heb,arr in parallel_phrase]\n",
        "  \n",
        "    target_tensor = [encode_arr(arr) for heb, arr in parallel_phrase]\n",
        "    target_lengths = [len(arr)  for heb,arr in parallel_phrase]\n",
        "  \n",
        "\n",
        "    print_log(\"VECTORIZE EXAMPLE\")\n",
        "    print_log(LTRchar,parallel_phrase[0])\n",
        "    print_log(input_lengths[0])\n",
        "    print_log(target_lengths[0])\n",
        "    print_log(input_tensor[0])\n",
        "    print_log(target_tensor[0])\n",
        "    print_log(\"\\n\")\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                 value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    print_log(len(input_tensor), \n",
        "        len(target_tensor))      \n",
        "    BUFFER_SIZE = len(input_tensor)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, \n",
        "                                                  target_tensor,\n",
        "                                                  input_lengths,\n",
        "                                                  target_lengths))\n",
        "    if to_shuffle:\n",
        "      dataset=dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "                                                  \n",
        "    dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset_double\n",
        "    \n",
        "    return dataset_double\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m"
      },
      "source": [
        "# CELL_NAME=\"DEF create_data_tensors\"\n",
        "\n",
        "# def max_length(tensor):\n",
        "#     return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# def create_data_tensors(pairs):\n",
        "  \n",
        "#     input_tensor = [encode_JA(heb) for heb, arr in pairs]\n",
        "#     input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "  \n",
        "#     target_tensor = [encode_arr(arr) for heb, arr in pairs]\n",
        "#     target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "  \n",
        "\n",
        "#     print_log()\n",
        "#     print_log(LTRchar,pairs[0])\n",
        "#     print_log(input_lenghts[0])\n",
        "#     print_log(target_lengths[0])\n",
        "#     print_log(input_tensor[0])\n",
        "#     print_log(target_tensor[0])\n",
        "\n",
        "#     # Padding the input and output tensor to the maximum length\n",
        "#     max=max_length(input_tensor)\n",
        "#     if max>70:\n",
        "#       max=70\n",
        "#     input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "#                                                                  maxlen=max,\n",
        "#                                                                  padding='post',\n",
        "#                                                                   value=inp_lang.char2idx[BLANK])\n",
        "#     max=max_length(target_tensor)\n",
        "#     if max>70:\n",
        "#       max=70    \n",
        "#     target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "#                                                                   maxlen=max,\n",
        "#                                                                   padding='post',\n",
        "#                                                                   value=targ_lang.char2idx[BLANK])\n",
        "#     return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-"
      },
      "source": [
        "# CELL_NAME=\"DEF GEN DATA\"\n",
        "\n",
        "# def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths,_test_size=0.2):  \n",
        "#   input_tensor_train, input_tensor_val, \\\n",
        "#   target_tensor_train, target_tensor_val, \\\n",
        "#   input_lengths_train, input_lengths_val, \\\n",
        "#   target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "#                                                               target_tensor,\n",
        "#                                                               input_lenghts,\n",
        "#                                                               target_lengths, test_size=_test_size)\n",
        "  \n",
        "#   print_log(len(input_tensor_train), \n",
        "#         len(target_tensor_train), \n",
        "#         len(input_tensor_val), \n",
        "#         len(target_tensor_val))\n",
        "  \n",
        "#   BUFFER_SIZE = len(input_tensor_train)\n",
        "  \n",
        "#   dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "#                                                 target_tensor_train,\n",
        "#                                                 input_lengths_train,\n",
        "#                                                 target_lengths_train)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=True)\n",
        "\n",
        "\n",
        "#   test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "#                                                     target_tensor_val,\n",
        "#                                                     input_lengths_val,\n",
        "#                                                     target_lengths_val)).shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)                                                  \n",
        "  \n",
        "#   dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "#   return dataset_double,test_dataset_double\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0elYUMb8Ls"
      },
      "source": [
        "##activate gen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izFnzJAMynm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a438365d-6d6a-4ff2-9bb0-615a106d6de2"
      },
      "source": [
        "#TODO move to main\n",
        "CELL_NAME=\"LOAD DATASET NEW\"\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_train=preprocess_lines(\n",
        "    load_lines(hakuzari+\".train.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "train_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_train,1),to_shuffle=TO_SHUFFLE)\n",
        "\n",
        "print_log_screen(\"train_dataset_double_kuzari\")\n",
        "view_data(train_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "kuzari_lines_test=preprocess_lines(\n",
        "    load_lines(hakuzari+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_kuzari= produce_dataset(create_parralel_phrases(kuzari_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_kuzari\")\n",
        "view_data(test_dataset_double_kuzari)\n",
        "\n",
        "\n",
        "#only once per file\n",
        "rasag_lines_test=preprocess_lines(\n",
        "    load_lines(haemunot+\".test.txt\")\n",
        ")\n",
        "\n",
        "#to recalc dropout:\n",
        "test_dataset_double_rasag= produce_dataset(create_parralel_phrases(rasag_lines_test,1)\n",
        "                                      ,to_shuffle=False)\n",
        "\n",
        "print_log_screen(\"test_dataset_double_rasag\")\n",
        "view_data(test_dataset_double_rasag)\n",
        "\n",
        " "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.train.txt\n",
            "len(lines) 8739\n",
            "train_dataset_double_kuzari\n",
            "‫ אעטמ מנ אעתקאדהמ אלחדת  |  أعظم من اعتقادهم الحدث\n",
            "‫ אלי הדה אלדרגה אלרוחאניה  |  إلى هذه الدرجة الروحانية\n",
            "‫ אלאלאהי אלדי ראמ קרבה  |  الإلهيّ الذي رام قربه\n",
            "‫ . ולא אלתקלל מנ אלמאל  |  . ولا التقلّل من المال\n",
            "‫ , ותלתד ברויה אלנור  |  , وتلتذّ برؤية النور\n",
            "loading text: /gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt.test.txt\n",
            "len(lines) 2184\n",
            "test_dataset_double_kuzari\n",
            "‫ ואהל אלאדיאנ תמ עלי  |  وأهل الأديان ثمّ على\n",
            "‫ , אלדי כאנ ענד מלכ אלכזר  |  , الذي كان عند ملك الخزر\n",
            "‫ אלדאכל פי דינ אליהוד  |  الداخل في دين اليهود\n",
            "‫ כתאב אלתואריכ , אנה  |  كتاب التواريخ , أنه\n",
            "‫ תכרר עליה רויא , כאנ  |  تكرّر عليه رؤيا , كأنّ\n",
            "loading text: /gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt.test.txt\n",
            "len(lines) 2071\n",
            "test_dataset_double_rasag\n",
            "‫ באנ קאל תבארכ אללה אלאה  |  بأن قال تبارك الله إله\n",
            "‫ אמא עלי אתר מא אפתתחנא  |  أمّا على إثر ما افتتحنا\n",
            "‫ פי מטאלבהמ וענ וגה זואלהא  |  في مطالبهم وعن وجه زوالها\n",
            "‫ H H H . וארי אנ אגעל  |  H H H . وأرى أن أجعل\n",
            "‫ אלשבה באי סבב כאנ מנ  |  الشبه بأي سبب كان من\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc"
      },
      "source": [
        "# CELL_NAME=\"LOAD DATASET\"\n",
        "\n",
        "\n",
        "# lines=load_lines(hakuzari)\n",
        "# pairs = create_parralel_phrases(lines,1)  \n",
        "# input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "# print_log_screen(\"test_dataset_double_kuzari\")\n",
        "# view_data(test_dataset_double_kuzari)\n",
        "\n",
        "# lines1=load_lines(haemunot)\n",
        "# pairs1 = create_parralel_phrases(lines1,1)  \n",
        "# input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = create_data_tensors(pairs1)\n",
        "# dataset_double_rasag,test_dataset_double_rasag=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "# print_log_screen(\"test_dataset_double_rasag\")\n",
        "# view_data(test_dataset_double_rasag)\n",
        "\n",
        "\n",
        "# # def gen kuzari_drop(lines,keep):\n",
        "# #   pairs = create_parralel_phrases(lines,0.90)  \n",
        "# #   input_tensor, target_tensor ,input_lenghts,target_lengths = create_data_tensors(pairs)\n",
        "# #   dataset_double_kuzari,test_dataset_double_kuzari=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpKhndwM10b0"
      },
      "source": [
        "###load_lines_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOU-bSFtpEgu"
      },
      "source": [
        "CELL_NAME=\"load_lines_synth\"\n",
        "\n",
        "#TODO edit when time permits\n",
        "\n",
        "def load_lines_synth(file_path=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(file_path, 'rb') as f:\n",
        "      text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  text=replace_arab_style_punctuation(text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)    \n",
        "\n",
        "  words=text.split()  \n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      #sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "      sentences.append(\" \".join(res))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "  return sentences\n",
        "\n",
        "# res=load_lines_synth()\n",
        "# print(len(res))\n",
        "#res[-5:]\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9CAcJjeyB_E"
      },
      "source": [
        "###preprocess_synth_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdHcLRT0uhA4"
      },
      "source": [
        "def preprocess_synth_lines(arab_sentences):\n",
        "  res_sentences=[]\n",
        "  for arr in arab_sentences:    \n",
        "    arr=preprocess_arr(arr)\n",
        "    res_sentences.append(arr)\n",
        "  return res_sentences"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR"
      },
      "source": [
        "# CELL_NAME=\"GEN SYNTH\"\n",
        "\n",
        "# #TODO edit when time permits\n",
        "\n",
        "# def load_lines_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "#   with open(ibnsina, 'rb') as f:\n",
        "#       ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "#   ibnsina_text=replace_arab_style_punctuation(ibnsina_text) #TODO WHAT ABOUT OTHER ARAB NORMALIZATION???\n",
        "#   #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "#   #add space before and after punctuation signs\n",
        "#   ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "#   ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "#   return ibnsina_text\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX"
      },
      "source": [
        "# CELL_NAME=\"DEF gen_synth_sentences\"\n",
        "\n",
        "# def gen_synth_sentences(sina_words):\n",
        "#   #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "#   SENTENCE_LIMIT=20\n",
        "#   sentences=[]\n",
        "#   char_count=0\n",
        "#   res=[]\n",
        "#   for w in sina_words:\n",
        "#   #  w=w.rstrip(\" \").strip(\" \")\n",
        "#     char_count+=len(w)+1 #len of word + space afterwards\n",
        "#     res.append(w)\n",
        "#     if char_count>SENTENCE_LIMIT:    \n",
        "#       sentences.append(normalize_unicode(remove_arab_nikud(\" \".join(res))))\n",
        "#       res=[]\n",
        "#       char_count=0\n",
        "#     #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "#   return sentences\n",
        "\n",
        "# # #ACTIVATE\n",
        "# # sentences=gen_synth_sentences(sina_words)\n",
        "# # len(sentences)\n",
        "# # sentences[:5]\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AqGe0R1_a_"
      },
      "source": [
        "###create_parralel_phrases_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA"
      },
      "source": [
        "CELL_NAME=\"DEF gen_dropout\"\n",
        "\n",
        "def create_parralel_phrases_synth(sentences,keep=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)           \n",
        "    heb=reverse_simple_map(arr)\n",
        "    if no_tags:\n",
        "      heb=heb.replace(\"'\",'')\n",
        "    heb=dropout(heb,keep)\n",
        "    heb_sentences.append(double_hebrew(heb))\n",
        "\n",
        "  print_log(heb_sentences[:5]) \n",
        "  print_log(arab_setences[:5])\n",
        "\n",
        "  print_log(len(arab_setences))\n",
        "  \n",
        "  return list(zip(heb_sentences,arab_setences))\n",
        "\n",
        "  #input_tensor_synth, target_tensor_synth, \\  \n",
        "  # input_lenghts_synth,target_lengths_synth = create_data_tensors(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # # Show length\n",
        "  # print_log(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  # print_log(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  # BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  # dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "  #                                                     target_tensor_synth,\n",
        "  #                                                     input_lenghts_synth,\n",
        "  #                                                     target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  # dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  # return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J"
      },
      "source": [
        "# CELL_NAME=\"DEF gen_dropout_all\"\n",
        "\n",
        "# def gen_dropout_all(sntcs1,*sentences,keep=1):\n",
        "#   result_dataset=gen_dropout(sntcs1,keep)\n",
        "#   for sntcs in sentences:\n",
        "#     result_dataset.concatenate(gen_dropout(sntcs,keep))\n",
        "#   return result_dataset.shuffle(1000)\n",
        "\n",
        "# #   BUFFER_SIZE=1000 #TODO get size\n",
        "# #   return dataset_double_synt3.concatenate(dataset_double_kuzari).shuffle(BUFFER_SIZE)  ##TODO: this is without dropout. \n",
        "# # #view_data(gen_dropout_all())"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auU5EkGq5fW9"
      },
      "source": [
        "##activate gen synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WTc9QYMawy_"
      },
      "source": [
        "\n",
        "\n",
        "if INSPECT:\n",
        "  synth_path0=\"/gdrive/My Drive/JUDEO-ARAB/for_synth/ibn_sina_ilhyat.txt\"\n",
        "  synth_sentences0=preprocess_synth_lines(\n",
        "      load_lines_synth(synth_path0)\n",
        "  )\n",
        "\n",
        "\n",
        "  synth_phrase_pairs0=create_parralel_phrases_synth(synth_sentences0,\n",
        "                                                    0.9)\n",
        "  synth_dataset0=produce_dataset(synth_phrase_pairs0,\n",
        "                                to_shuffle=TO_SHUFFLE)\n",
        "  view_data(synth_dataset0)\n",
        "\n",
        "  #####################3\n",
        "  synth_path1=\"/gdrive/My Drive/JUDEO-ARAB/for_synth/daruri-IR.txt\"\n",
        "  synth_sentences1=preprocess_synth_lines(\n",
        "      load_lines_synth(synth_path1)\n",
        "  )\n",
        "  synth_phrase_pairs1=create_parralel_phrases_synth(synth_sentences1,\n",
        "                                                    0.9)\n",
        "  synth_dataset1=produce_dataset(synth_phrase_pairs1,\n",
        "                                to_shuffle=TO_SHUFFLE)\n",
        "  view_data(synth_dataset1)\n",
        "\n",
        "\n",
        "  #######################33\n",
        "  synth_path2=\"/gdrive/My Drive/JUDEO-ARAB/for_synth/farabi-tahsil.txt\"\n",
        "  synth_sentences2=preprocess_synth_lines(\n",
        "      load_lines_synth(synth_path2)\n",
        "  )\n",
        "  synth_phrase_pairs2=create_parralel_phrases_synth(synth_sentences2,\n",
        "                                                    0.9)\n",
        "  synth_dataset2=produce_dataset(synth_phrase_pairs2,\n",
        "                                to_shuffle=TO_SHUFFLE)\n",
        "  view_data(synth_dataset2)\n",
        "\n",
        "  ##########################3\n",
        "  synth_path3=\"/gdrive/My Drive/JUDEO-ARAB/for_synth/huruf.txt\"\n",
        "  synth_sentences3=preprocess_synth_lines(\n",
        "      load_lines_synth(synth_path3)\n",
        "  )\n",
        "\n",
        "  synth_phrase_pairs3=create_parralel_phrases_synth(synth_sentences3,\n",
        "                                                    0.9)\n",
        "  synth_dataset3=produce_dataset(synth_phrase_pairs3,\n",
        "                                to_shuffle=False)\n",
        "  view_data(synth_dataset3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT4q_5EwyowP"
      },
      "source": [
        "##gen_all_synth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbugLUuw2Ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a537696-d7b8-4068-b0e1-6d38244740d3"
      },
      "source": [
        "CELL_NAME=\"GEN_ALL_SYNTH\"\n",
        "import glob\n",
        "\n",
        "#all_synth_lines=synth_sentences0+synth_sentences1+synth_sentences2+synth_sentences3\n",
        "all_synth_lines=[]\n",
        "\n",
        "print(len(all_synth_lines))\n",
        "\n",
        "for file_path in glob.glob(\"/gdrive/My Drive/JUDEO-ARAB/for_synth/*.txt\"):\n",
        "  print(file_path)\n",
        "  print(len(all_synth_lines))\n",
        "  all_synth_lines+=preprocess_synth_lines(\n",
        "    load_lines_synth(file_path))\n",
        "print(\"after loop\")\n",
        "def gen_all_synth(keep):\n",
        "  all_synth_phrase_pairs=create_parralel_phrases_synth(all_synth_lines,\n",
        "                                                      keep)\n",
        "  all_synth_dataset=produce_dataset(all_synth_phrase_pairs,\n",
        "                                   to_shuffle=TO_SHUFFLE)\n",
        "  view_data(all_synth_dataset)\n",
        "  return all_synth_dataset"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "/gdrive/My Drive/JUDEO-ARAB/for_synth/ibn_sina_ilhyat.txt\n",
            "0\n",
            "/gdrive/My Drive/JUDEO-ARAB/for_synth/daruri-IR.txt\n",
            "21322\n",
            "/gdrive/My Drive/JUDEO-ARAB/for_synth/farabi-tahsil.txt\n",
            "26932\n",
            "/gdrive/My Drive/JUDEO-ARAB/for_synth/huruf.txt\n",
            "29206\n",
            "after loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPInoOKrP3fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b4f93c-58b3-4b3a-b665-677abcd0009b"
      },
      "source": [
        "\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# WARNING:tensorflow:From <ipython-input-39-fe0c13a5ddfd>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
        "# Instructions for updating:\n",
        "# Use `tf.config.list_physical_devices('GPU')` instead."
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada6828d-0e8a-4fad-be9e-e1635fdbf67c"
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL\"\n",
        "\n",
        "#BASED ON THE MODEL FROM https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units, \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=STATEFUL)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-40-fe0c13a5ddfd>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX"
      },
      "source": [
        "CELL_NAME=\"BUILD MODEL1\"\n",
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "#model=rebuild()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mdbbN5XLlGa"
      },
      "source": [
        "if INSPECT:\n",
        "  model=rebuild()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE"
      },
      "source": [
        "##CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_"
      },
      "source": [
        "CELL_NAME=\"DEFINE CHECKPOINT\"\n",
        "\n",
        "checkpoint_path='/gdrive/My Drive/checkpoints/'+this_time+\"/ckpt\"\n",
        "def save_checkpoint(massage,ckp_path=checkpoint_path):\n",
        "  print_log_screen(\"saving checkpoing at \"+checkpoint_path)\n",
        "  model.save_weights(checkpoint_path)\n",
        "  f_check= open(checkpoint_path+\".txt\",\"a+\")  \n",
        "  f_check.write(\"saving chekcpoing at epoch\"+ str(GLOBAL_epoch) +'\\n')\n",
        "  f_check.write(massage+'\\n')\n",
        "  f_check.close()\n",
        "  \n",
        "def load_checkpoint(checkpoint_path=checkpoint_path):\n",
        "  checkpoint_path=\"/gdrive/My Drive/checkpoints/\"+checkpoint_path+\"/ckpt\"\n",
        "  print_log_screen(\"loading checkpoing from \"+checkpoint_path)\n",
        "  model1=rebuild()\n",
        "  model1.load_weights(checkpoint_path)\n",
        "  return model1\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo"
      },
      "source": [
        "## forward run single letters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb"
      },
      "source": [
        "CELL_NAME=\"TEST single letters\"\n",
        "def test_single_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print_log(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "        if pred_distr[ii]>0.001:\n",
        "          print(LTRchar,\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      \n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print(\"####max is the blank symbole\")\n",
        "      print_log_screen('-'*10)\n",
        "\n",
        "#test_single_letters()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B8G14-fL8Ez"
      },
      "source": [
        "if INSPECT:\n",
        "  #model=load_checkpoint(\"2020-05-07 15:51:26.100845\")\n",
        "  test_single_letters()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF"
      },
      "source": [
        "##forward run text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM2_GVYmOwdT"
      },
      "source": [
        "CELL_NAME=\"DEF test__CTC_word_multiline\"\n",
        "\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def forward_text(lines,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_JA(l)\n",
        "    l = double_hebrew(l)\n",
        "    v= encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "\n",
        "  #PADD HORIZANTALY REST OF LINES TO FILL BATCH\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "  res=[]\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  \n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  print(inputs_len)\n",
        "  print(num_of_paths)\n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "\n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print_log(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_arr(dense[t])\n",
        "      print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''כמאלא\n",
        "כמאלא'''\n",
        "if INSPECT:\n",
        "  print_log_screen(forward_text(lines.split('\\n'),3,BATCH_SIZE))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K6i5QWGO4DS"
      },
      "source": [
        "##TEST KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09dff4c6-b20b-4623-efe5-af0a61fdfa41"
      },
      "source": [
        "CELL_NAME=\"DEF TEST KFIR\"\n",
        "#JA_lines should be allready with doubling\n",
        "#indexes - word index to be tested (if testing only on a certain word in the every line)\n",
        "def test_text_kfir(JA_lines,arr_lines,indexes=None,num_of_paths=1,SHOW_PRINT=False):   \n",
        "  num_of_lines=len(JA_lines)\n",
        "  num_of_letters= 0\n",
        "  assert(num_of_lines==len(arr_lines))\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in JA_lines:\n",
        "    l = preprocess_JA(l)\n",
        "    #l = double_hebrew(l)\n",
        "    v=encode_JA(l)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "\n",
        "\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "  predict_ltrs=model(inputs)\n",
        "\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  \n",
        "  total_res=[]\n",
        "  total_edit_dist=0\n",
        "  total_normalized_edit_dist=0\n",
        "  line_counter=1\n",
        "  for t in range(num_of_lines):\n",
        "    real=arr_lines[t]\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=decode_arr(dense[t]).strip() \n",
        "      # print_log(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "      total_res.append(prediction)\n",
        "    if indexes:\n",
        "      real=real.split()[indexes[t]]\n",
        "      # print(real)          \n",
        "      # print(indexes[t])\n",
        "      # print(prediction)\n",
        "      prediction=prediction.split()[indexes[t]]\n",
        "    ed_dist=editdistance.eval(real, prediction)\n",
        "    num_of_letters+=len(real)\n",
        "\n",
        "    normalized_ed_dist=ed_dist/len(real)\n",
        "    real,prediction=show_diff(real,prediction,'red')\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"({0})\".format(line_counter),LTRchar,undouble_hebrew(JA_lines[t]),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(ed_dist))\n",
        "    line_counter+=1\n",
        "    total_normalized_edit_dist+=normalized_ed_dist\n",
        "    total_edit_dist+=ed_dist\n",
        "  return total_res,total_normalized_edit_dist,num_of_lines,total_edit_dist,num_of_letters\n",
        "  \n",
        "JA_lines='''ייככ''אאללףף\n",
        "עעלליי\n",
        "ההדד''הה\n",
        "אאללאאממאאננהה\n",
        "ווללאא'''\n",
        "\n",
        "JA_lines='''ייככ''אאללףף\n",
        "אאללאאממאאננהה ששללווםם'''\n",
        "\n",
        "# arr_lines='''يخالف\n",
        "# على\n",
        "# هذه\n",
        "# الأمانة\n",
        "# ولا'''\n",
        "\n",
        "arr_lines='''يخالف\n",
        "الأمانة الأمانة'''\n",
        "\n",
        "indexes=[0,1]\n",
        "indexes=None\n",
        "\n",
        "model=rebuild()\n",
        "test_text_kfir(JA_lines.split('\\n'),arr_lines.split('\\n'),indexes,SHOW_PRINT=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 8)            384       \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (128, None, 2048)        6352896   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (128, None, 2048)        18886656  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (128, None, 2048)        18886656  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (128, None, 2048)        18886656  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 67)           137283    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,150,531\n",
            "Trainable params: 63,150,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(1) ‫ יכ'אלף | \u001b[1m\u001b[31mيخالف\u001b[0m | \u001b[1m\u001b[31m0;ئص\u001b[0m | 5.0000\n",
            "(2) ‫ אלאמאנה שלום | \u001b[1m\u001b[31mالأما\u001b[0mن\u001b[1m\u001b[31mة الأمانة\u001b[0m | \u001b[1m\u001b[31m!H0و7زص\u001b[0mن\u001b[1m\u001b[31m0\u001b[0m | 14.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['0;ئص', '!H0و7زصن0'], 1.9333333333333333, 2, 19, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5e5-59jCxY"
      },
      "source": [
        "CELL_NAME=\"DEF test_kfir1\"\n",
        "#data_path : kfir_kuzari_test/kfir_rasag_test\n",
        "def test_kfir(data_path,SHOW_PRINT=False,indexes_filepath=None,replace_GAIN=False):\n",
        "  lines=preprocess_lines(load_lines(data_path))\n",
        "  pairs = create_parralel_phrases(lines)\n",
        "  \n",
        "  if indexes_filepath:\n",
        "    indexes=[]\n",
        "    f_indexes=open(indexes_filepath,'r')\n",
        "    ind_lines=f_indexes.readlines()\n",
        "    assert(len(ind_lines)==len(lines))\n",
        "    for il in ind_lines:\n",
        "      indexes.append(int(il)) #need to iterate beacuase of the casting to int\n",
        "    f_indexes.close()    \n",
        "  if replace_GAIN:    \n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs]\n",
        "  \n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "\n",
        "  num_of_lines=len(pairs)\n",
        "  index=0\n",
        "  total_num_of_examples=0\n",
        "  total_num_of_letters=0\n",
        "  total_sum_e_d_normalized=0\n",
        "  total_sum_e_d=0\n",
        "  while index<=num_of_lines:\n",
        "    batch_hebrew=hebrew_lines[index:index+BATCH_SIZE]\n",
        "    batch_arab=arab_lines[index:index+BATCH_SIZE]\n",
        "    if indexes_filepath:\n",
        "      batch_indexes=indexes[index:index+BATCH_SIZE]\n",
        "    else:\n",
        "      batch_indexes=None\n",
        "    _,sum_of_e_d_normalized,num_of_examples,sum_of_e_d,num_of_letters=test_text_kfir(batch_hebrew,\n",
        "                                                                                     batch_arab,\n",
        "                                                                                     batch_indexes,\n",
        "                                                                                     SHOW_PRINT=SHOW_PRINT)\n",
        "    if SHOW_PRINT:\n",
        "      print_log_screen(\"BATCH (sum_of_e_d_normalized,num_of_examples): \",sum_of_e_d_normalized,num_of_examples)\n",
        "      print_log_screen(\"BATCH (sum_of_e_d,num_of_letters): \",sum_of_e_d,num_of_letters)\n",
        "    total_num_of_examples+=num_of_examples\n",
        "    total_num_of_letters+=num_of_letters\n",
        "    total_sum_e_d+=sum_of_e_d\n",
        "    total_sum_e_d_normalized+=sum_of_e_d_normalized\n",
        "    index+=BATCH_SIZE\n",
        "\n",
        "  print_log_screen(\"#examples:\",total_num_of_examples,\", accuracy:\",1-total_sum_e_d_normalized/total_num_of_examples)\n",
        "  print_log_screen(\"#letters:\",total_num_of_letters,\", accuracy1:\",1-total_sum_e_d/total_num_of_letters)\n",
        "  return total_sum_e_d_normalized/total_num_of_examples,total_sum_e_d/total_num_of_letters\n",
        "\n",
        "if INSPECT:\n",
        "  test_kfir(kfir_kuzari_test_SWITCH,SHOW_PRINT=False)\n",
        "  test_kfir(kfir_kuzari_test,SHOW_PRINT=False)\n",
        "  test_kfir(kfir_rasag_test_SWITCH,SHOW_PRINT=False)\n",
        "  test_kfir(kfir_rasag_test,SHOW_PRINT=False)\n",
        "  #test_kfir(kfir_rasag_test,SHOW_PRINT=True,replace_GAIN=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyZ4mSjvzKxX"
      },
      "source": [
        "##COMPARE with/without CONTEXT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qhhb8OaD1QT",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8794889-dbe4-47b6-b344-131acd89aa5e"
      },
      "source": [
        "CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR\"\n",
        "\n",
        "def switch_gim_ghayn(ja):\n",
        "  return ja.replace(\"ג\",\"ג'\").replace(\"ג''\",\"ג\")\n",
        "\n",
        "#adjust test kfir to comp edits only for index word!\n",
        "EMUNOT=True\n",
        "\n",
        "if EMUNOT:\n",
        "  file_all = haemunot\n",
        "  file_singels = kfir_rasag_test\n",
        "else:\n",
        "  file_all = haemunot  #there's no use in testing with lines from kuzari since I'm training on it.\n",
        "  file_singels = kfir_kuzari_test_SWITCH\n",
        "\n",
        "full_lines=preprocess_lines(load_lines(file_all))\n",
        "single_words=preprocess_lines(load_lines(file_singels))\n",
        "\n",
        "\n",
        "def grep(reg,ja):\n",
        "  random.shuffle(full_lines) \n",
        "  JA_dont_match=False\n",
        "  for line in full_lines:\n",
        "      line=remove_arab_nikud(line) #TODO: do preprocessing jointly    \n",
        "      JA_line,arab_line=line.split('\\t')\n",
        "      if re.search(reg, arab_line):\n",
        "          full_line=line\n",
        "          arab_words=arab_line.split()\n",
        "          JA_words=JA_line.split()\n",
        "          reg_ind=arab_words.index(reg.strip(\" $^\"))\n",
        "          single_words=ja+'\\t'+arab_words[reg_ind]\n",
        "          assert(len(JA_words)==len(arab_words)) #MAYBE NOT ALWAYS THE CASE!!!          \n",
        "          if (not JA_words[reg_ind]==ja):\n",
        "            JA_dont_match=True\n",
        "            print(\"ja:\"+ja+ \"JA_words[reg_ind]:\" +JA_words[reg_ind])            \n",
        "            print(\"continue searching...\")\n",
        "            continue          \n",
        "          return full_line,single_words,reg_ind\n",
        "  if JA_dont_match:\n",
        "    return full_line.replace(JA_words[reg_ind],ja),single_words,reg_ind\n",
        "  return\n",
        "          \n",
        "def highlight(text,word_index):\n",
        "  words=text.split()\n",
        "  res=[]  \n",
        "  for i in range(len(words)):\n",
        "    if i==word_index:\n",
        "      res.append(colored(words[i],'red'))\n",
        "    else:\n",
        "      res.append(words[i])      \n",
        "  return ' '.join(res)\n",
        "\n",
        "def highlight_grep_output(ja_arr_line,word_index):\n",
        "  ja,arr=ja_arr_line.split('\\t')\n",
        "  return highlight(ja,word_index)+'\\t'+highlight(arr,word_index)\n",
        "\n",
        "\n",
        "not_found_counter=0\n",
        "\n",
        "full_f=open(\"with_context.txt\",'w+')\n",
        "single_f=open(\"no_context.txt\",'w+')\n",
        "index_f=open(\"context_idexes.txt\",'w+')\n",
        "if INSPECT:\n",
        "  for l in single_words:  \n",
        "    arab=l.split('\\t')[1].strip()\n",
        "    ja=l.split('\\t')[0].strip()\n",
        "    print(l)\n",
        "    # if not DO_EMUNOT:\n",
        "    #   ja=ja.replace(\"ג\",\"ג'\").replace(\"ג''\",\"ג\")    ##only for kuzari - change to regular (or won't grep)\n",
        "    arab=' '+arab+' ' #first try to find greped in a middle of a line if exists\n",
        "    found=grep(arab,ja)\n",
        "    if not found:\n",
        "      print(colored(\"###SEARCH FIRST WORD\",\"red\",\"on_yellow\"))\n",
        "      found=grep(\"^\"+arab.lstrip(),ja)  #TODO regex for both this and next reg by | , so I don't discremanate start of line to end of line\n",
        "      if not found:\n",
        "        print(colored(\"###SEARCH LAST WORD\",\"red\",\"on_yellow\"))\n",
        "        found=grep(arab.rstrip()+\"$\",ja)\n",
        "        if not found:\n",
        "          not_found_counter+=1\n",
        "    if not found:\n",
        "      print(colored(\"not found:\"+arab,\"blue\",\"on_green\"))\n",
        "    else:\n",
        "      print(highlight_grep_output(found[0],found[2]))\n",
        "      single_f.write(found[1]+'\\n')\n",
        "      full_f.write(found[0]+'\\n')\n",
        "      index_f.write(str(found[2])+'\\n')\n",
        "\n",
        "  single_f.close()\n",
        "  full_f.close()\n",
        "  index_f.close()\n",
        "\n",
        "\n",
        "\n",
        "  print(\"#not founds:\"+str(not_found_counter))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\n",
            "len(lines) 10358\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_ORIGINAL.txt\n",
            "len(lines) 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNrQSIk2f9iG"
      },
      "source": [
        "# #RETURS ERROR!!1\n",
        "# CELL_NAME=\"FOR NACHUM - CONTEXT ON KFIR 1\"\n",
        "\n",
        "# PRINT_DETAILED=True\n",
        "\n",
        "# #model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-04 12:32:58.607342/ckpt')  #THIS IS THE FIRST ACTIVATIION, BY WHICH I PRODUCED THE EXCEL FOR NACHUM\n",
        "\n",
        "# #model=load_checkpoint('/gdrive/My Drive/checkpoints/2020-05-05 13:01:42.636420/ckpt')\n",
        "\n",
        "# #model=load_checkpoint(\"2020-05-07 15:51:26.100845\") THIS IS NOW\n",
        "\n",
        "# if model.stateful:\n",
        "#   model.reset_states()\n",
        "\n",
        "# test_kfir(\"with_context.txt\",SHOW_PRINT=PRINT_DETAILED,indexes_filepath=\"context_idexes.txt\",replace_GAIN=True)\n",
        "\n",
        "# #print(\"=\"*200)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGDdAym7t8m7"
      },
      "source": [
        "#ERROR!\n",
        "# if model.stateful:\n",
        "#   model.reset_states()\n",
        "\n",
        "# test_kfir(\"no_context.txt\",SHOW_PRINT=PRINT_DETAILED,replace_GAIN=True)\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU"
      },
      "source": [
        "CELL_NAME=\"TEST_BASELINE\"\n",
        "\n",
        "def baseline(this_dataset=test_dataset_double_kuzari,print_only_first_in_bath=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_batch, target_batch, inputs_len,targets_len in this_dataset:\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_batch[i])\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=decode_arr(target_batch[i],targets_len[i].numpy()).strip(BLANK)\n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if print_only_first_in_bath and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"baseline accuracy: \",1-total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 1-total_accuracy\n",
        "#baseline(limit=3)\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WehJRO5foBi"
      },
      "source": [
        "if INSPECT:\n",
        "  baseline(limit=3)\n",
        "# print_log_screen(\"KUZARI TEST\")\n",
        "# baseline(test_dataset_double_kuzari)\n",
        "# print_log_screen(\"RASAG TEST\")\n",
        "# baseline(test_dataset_double_rasag)\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWMVLTywwIW"
      },
      "source": [
        "##TEST BASELINE KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrcOFoswu-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7162bdae-7e27-47b6-8f8e-57989cb2cbc2"
      },
      "source": [
        "CELL_NAME=\"DEF TEST_BASELINE_KFIR\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#only once per file\n",
        "kfir_rasag_lines=preprocess_lines(\n",
        "    load_lines(kfir_rasag_test_SWITCH)\n",
        ")\n",
        "kfir_rasag_phrases=create_parralel_phrases(kfir_rasag_lines,1)\n",
        "\n",
        "kfir_kuzari_lines=preprocess_lines(\n",
        "    load_lines(kfir_kuzari_test_SWITCH)\n",
        ")\n",
        "kfir_kuzari_phrases=create_parralel_phrases(kfir_kuzari_lines,1)\n",
        "\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def baseline_kfir(pairs,replace_GAIN=False,SHOW_PRINT=False): \n",
        "  if replace_GAIN:\n",
        "    print_log_screen(\"replaceing gimel with jain to match my train convention\")\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs] #ייננבבגג''יי\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "  \n",
        "  total_examples=len(pairs)\n",
        "  total_loss=0\n",
        "  sum_of_e_dist=0\n",
        "  num_of_letters=0\n",
        "  for l in arab_lines:\n",
        "    num_of_letters+=len(l)\n",
        "  total_accuracy=0\n",
        "  line_counter=1\n",
        "  for heb_input,real in zip(hebrew_lines,arab_lines):\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)                \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                normalized_accuracy=accuracy/len(real)\n",
        "                total_accuracy+=normalized_accuracy\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                sum_of_e_dist+=accuracy\n",
        "                if SHOW_PRINT:\n",
        "                  print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",str(accuracy))\n",
        "                line_counter+=1\n",
        "\t\t\t\t\t\t\t   \n",
        "  total_accuracy/=total_examples\n",
        "  sum_of_e_dist/num_of_letters\n",
        "  print_log_screen(\"accuracy: \",1-total_accuracy)\n",
        "  print_log_screen(\"accuracy1: \",1-sum_of_e_dist/num_of_letters)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy,sum_of_e_dist/num_of_letters\n",
        "if INSPECT:\n",
        "  #baseline(limit=3)\n",
        "  #baseline_kfir(kfir_kuzari_phrases)\n",
        "  baseline_kfir(kfir_kuzari_phrases,False,True)\n",
        "  #baseline_kfir(kfir_rasag_phrases)\n",
        "  baseline_kfir(kfir_rasag_phrases,False,True)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 50\n",
            "loading text: /gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test_SWITCH_GIM_GHAYN.txt\n",
            "len(lines) 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74"
      },
      "source": [
        "##test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk"
      },
      "source": [
        "CELL_NAME=\"DEF general TEST_LOSS\"\n",
        "\n",
        "\n",
        "def test(this_dataset=test_dataset_double_kuzari,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=decode_JA(input_example_batch[i])\n",
        "\n",
        "                heb_input=undouble_hebrew(heb_input).strip(BLANK)\n",
        "                prediction=decode_arr(dense[i]).strip() #SHOULD BE STRING(BLANKS)?\n",
        "                real=decode_arr(target_example_batch[i],targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print_log_screen(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print_log_screen(\"LER (label error rate): \",total_accuracy)\n",
        "  #print_log(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "\n",
        "#test(limit=3)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpW54oxAkxCo"
      },
      "source": [
        "if INSPECT:\n",
        "  test(test_dataset_double_kuzari,limit=3)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGjAhEUfxbRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d9bdde-ba3e-460b-eac5-a8aaf59a9afc"
      },
      "source": [
        "CELL_NAME=\"GUIDE TEXT\"\n",
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "###TODO : change hebrew insertion to \"H\"\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guide_text='''כנת איהא אלתלמיד' אלעזיז עברית-ר' עברית-יוסף עברית-ש\"צ עברית-ב\"ר \n",
        "עברית-יהודה עברית-נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "guide_text=re.sub(r'עברית-[^\\s]+', 'H',guide_text)\n",
        "guide_lines=guide_text.split('\\n')\n",
        "\n",
        "for l in guide_lines:\n",
        "  print(LTRchar+l)\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר עברית-פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‫כנת איהא אלתלמיד' אלעזיז H H H H \n",
            "‫H H למא מת'לת ענדי וקצדת\n",
            "‫ מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
            "‫ אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
            "‫אלאסכנדריה קבל אן אמתחן\n",
            "‫תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
            "‫קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
            "‫זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
            "‫עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
            "‫פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
            "‫וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
            "‫אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
            "‫פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT"
      },
      "source": [
        "CELL_NAME=\"DEF TEST_GUIDE\"\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return forward_text(guide_lines,num_of_paths,BATCH_SIZE)\n",
        "if INSPECT:\n",
        "  print_log_screen(test_guide())"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb"
      },
      "source": [
        "CELL_NAME=\"DEF pretrain_letters\"\n",
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    if STATEFUL:\n",
        "      hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)       \n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test_single_letters()\n",
        "    print_log_screen(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJQQCzYQPWy0"
      },
      "source": [
        "BASIC_MODEL=\"/gdrive/My Drive/checkpoints/BASIC_MODEL_PRETRAIN\"\n",
        "\n",
        "if INSPECT:\n",
        "  #optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001) DONT WORK\n",
        "  optimizer = tf.optimizers.RMSprop(0.001)\n",
        "\n",
        "  #model=load_checkpoint(BASIC_MODEL)\n",
        "  pretrain_letters(10)\n",
        "\n",
        "  #save_checkpoint(\"\",ckp_path=BASIC_MODEL)\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-"
      },
      "source": [
        "CELL_NAME=\"TRAIN SINGLE LETTERS AND TEST LETTERS\"\n",
        "if INSPECT:\n",
        "  model=rebuild()\n",
        "  #test_single_letters()\n",
        "  #optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "  optimizer = tf.optimizers.RMSprop(0.001)\n",
        "  pretrain_letters(10,BATCH_SIZE)\n",
        "  test_single_letters()\n",
        "  #save_checkpoint(\"testing1\")\n",
        "  #model=rebuild()\n",
        "  #test_single_letters()\n",
        "  #model=load_checkpoint()\n",
        "  #test_single_letters()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HuuE4HgPav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e70fbe2-8ac2-4d6d-8294-a76863283b7e"
      },
      "source": [
        "test_single_letters()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidate:***( )***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(3)***0.015\n",
            "----------\n",
            "candidate:***(!)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ذ)***0.015\n",
            "----------\n",
            "candidate:***(\")***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ب)***0.015\n",
            "----------\n",
            "candidate:***(')***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ص)***0.015\n",
            "----------\n",
            "candidate:***(()***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ه)***0.015\n",
            "----------\n",
            "candidate:***())***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***())***0.015\n",
            "----------\n",
            "candidate:***(,)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***( )***0.015\n",
            "----------\n",
            "candidate:***(-)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(إ)***0.015\n",
            "----------\n",
            "candidate:***(.)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٌ)***0.015\n",
            "----------\n",
            "candidate:***(0)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ه)***0.015\n",
            "----------\n",
            "candidate:***(1)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ا)***0.015\n",
            "----------\n",
            "candidate:***(2)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ب)***0.015\n",
            "----------\n",
            "candidate:***(3)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ف)***0.015\n",
            "----------\n",
            "candidate:***(4)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ا)***0.015\n",
            "----------\n",
            "candidate:***(5)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(8)***0.015\n",
            "----------\n",
            "candidate:***(6)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(;)***0.015\n",
            "----------\n",
            "candidate:***(7)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ا)***0.015\n",
            "----------\n",
            "candidate:***(8)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ه)***0.015\n",
            "----------\n",
            "candidate:***(9)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ّ)***0.015\n",
            "----------\n",
            "candidate:***(:)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ف)***0.015\n",
            "----------\n",
            "candidate:***(;)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ؤ)***0.015\n",
            "----------\n",
            "candidate:***(?)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٍ)***0.015\n",
            "----------\n",
            "candidate:***(H)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(غ)***0.015\n",
            "----------\n",
            "candidate:***([)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ء)***0.015\n",
            "----------\n",
            "candidate:***(])***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ب)***0.015\n",
            "----------\n",
            "candidate:***(א)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ا)***0.015\n",
            "----------\n",
            "candidate:***(ב)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(7)***0.015\n",
            "----------\n",
            "candidate:***(ג)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(!)***0.015\n",
            "----------\n",
            "candidate:***(ד)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ّ)***0.015\n",
            "----------\n",
            "candidate:***(ה)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(7)***0.015\n",
            "----------\n",
            "candidate:***(ו)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ش)***0.015\n",
            "----------\n",
            "candidate:***(ז)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ش)***0.015\n",
            "----------\n",
            "candidate:***(ח)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ق)***0.015\n",
            "----------\n",
            "candidate:***(ט)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٍ)***0.015\n",
            "----------\n",
            "candidate:***(י)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٍ)***0.015\n",
            "----------\n",
            "candidate:***(כ)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٍ)***0.015\n",
            "----------\n",
            "candidate:***(ל)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(H)***0.015\n",
            "----------\n",
            "candidate:***(מ)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٍ)***0.015\n",
            "----------\n",
            "candidate:***(נ)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(2)***0.015\n",
            "----------\n",
            "candidate:***(ס)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(7)***0.015\n",
            "----------\n",
            "candidate:***(ע)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ٌ)***0.015\n",
            "----------\n",
            "candidate:***(פ)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ا)***0.015\n",
            "----------\n",
            "candidate:***(צ)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ن)***0.015\n",
            "----------\n",
            "candidate:***(ק)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(H)***0.015\n",
            "----------\n",
            "candidate:***(ר)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(إ)***0.015\n",
            "----------\n",
            "candidate:***(ש)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ز)***0.015\n",
            "----------\n",
            "candidate:***(ת)***\n",
            "‫ 0.015( )  ‫ 0.015(!)  ‫ 0.015(\")  ‫ 0.015(')  ‫ 0.015(()  ‫ 0.015())  ‫ 0.015(,)  ‫ 0.015(-)  ‫ 0.015(.)  ‫ 0.015(0)  ‫ 0.015(1)  ‫ 0.015(2)  ‫ 0.015(3)  ‫ 0.015(4)  ‫ 0.015(5)  ‫ 0.015(6)  ‫ 0.015(7)  ‫ 0.015(8)  ‫ 0.015(9)  ‫ 0.015(:)  ‫ 0.015(;)  ‫ 0.015(?)  ‫ 0.015(H)  ‫ 0.015([)  ‫ 0.015(])  ‫ 0.015(ء)  ‫ 0.015(آ)  ‫ 0.015(أ)  ‫ 0.015(ؤ)  ‫ 0.015(إ)  ‫ 0.015(ئ)  ‫ 0.015(ا)  ‫ 0.015(ب)  ‫ 0.015(ة)  ‫ 0.015(ت)  ‫ 0.015(ث)  ‫ 0.015(ج)  ‫ 0.015(ح)  ‫ 0.015(خ)  ‫ 0.015(د)  ‫ 0.015(ذ)  ‫ 0.015(ر)  ‫ 0.015(ز)  ‫ 0.015(س)  ‫ 0.015(ش)  ‫ 0.015(ص)  ‫ 0.015(ض)  ‫ 0.015(ط)  ‫ 0.015(ظ)  ‫ 0.015(ع)  ‫ 0.015(غ)  ‫ 0.015(ف)  ‫ 0.015(ق)  ‫ 0.015(ك)  ‫ 0.015(ل)  ‫ 0.015(م)  ‫ 0.015(ن)  ‫ 0.015(ه)  ‫ 0.015(و)  ‫ 0.015(ى)  ‫ 0.015(ي)  ‫ 0.015(ً)  ‫ 0.015(ٌ)  ‫ 0.015(ٍ)  ‫ 0.015(ّ)  ‫ 0.015(ٱ)  0.015 <blank>\n",
            "prediction***(ل)***0.015\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_"
      },
      "source": [
        "CELL_NAME=\"DEF main TRAIN_LOOP\"\n",
        "\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train(cur_dataset=train_dataset_double_kuzari,stop_loop=10000000000):\n",
        "  global GLOBAL_epoch\n",
        "  if STATEFUL:\n",
        "    hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:            \n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?\n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print_log_screen(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhSQaqd3ye9g"
      },
      "source": [
        "if INSPECT:\n",
        "  dataset_double_synt=gen_all_synth(1).concatenate(train_dataset_double_kuzari).shuffle(500)\n",
        "  count=0\n",
        "  for input,target,input_lens,target_lens in dataset_double_synt:\n",
        "    #print(decode_JA(input[0],input_lens[0]))\n",
        "    count+=1\n",
        "    #print(count)\n",
        "  print(count)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPaH-g2wziSg"
      },
      "source": [
        "if INSPECT:\n",
        "  test_kfir(kfir_kuzari_test_SWITCH,True,None,False)  \n",
        "  test_kfir(kfir_rasag_test_SWITCH,True,None,False)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test(test_dataset_double_rasag)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)\n",
        "\n",
        "\n",
        "  guide_result=test_guide()\n",
        "\n",
        "  #TODO SAVE CHECKPOINT\n",
        "  if all_accuracy<BEST_ACCURACY:\n",
        "    save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "    BEST_ACCURACY=all_accuracy\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMO0v8usWqmZ",
        "outputId": "7678948d-eec9-45f2-b03a-1a52d33c4888"
      },
      "source": [
        "#@title forward text\n",
        "use_checkpoint = \"2021-11-04 09:37:50.447676\" #@param [\"NONE\", \"2021-11-04 09:37:50.447676\", \"2021-11-09 14:27:55.044957\"]\n",
        "#lines = \"PGPID_5502_MS\" #@param [\"PGPID_5502_MS\", \"PGPID_5485_MS\", \"PGPID_5550_MS\", \"PGPID_5544_MS\"]\n",
        "lines1='''ואמא קולה H H פהו פעל מאץ',\n",
        " ולו קאל H H H לקד כאן ימכן\n",
        "  מן יסתחסן אלמכ'רקה \n",
        "  אן יתעלק בד'לך,\n",
        "   אמא מנד' קאל H ל אנה אמר \n",
        "   קד ג'רא, וד'לך אנה יצף H H H'''\n",
        "# lines='''וחדת'ני ואלדי אן קבל הד'ה אלקצה\n",
        "# בנחו כ'מסה' עשר סנה או עשרין\n",
        "# קאמוא אקואם עלמא אכ'יאר פי מדינה' קרטבה\n",
        "# '''\n",
        "\n",
        "\n",
        "lines2='''\n",
        "ולמא וצלנא מן הד'ה אלמקאלה\n",
        "אלי הד'א אלמוצ'ע \n",
        "והו כאן גאיה' אלקצד בהא,\n",
        "וראינא אנהא עאדמה' אלפאידה באלכלייה,\n",
        "לאנהא לא תתצ'מן אלא תכראר\n",
        "מא קד ד'כר פי שרח אלמשנה ופי אלחבור,\n",
        "וזיאדה' ביאן לכל קציר אלפהם או מתהאפת,\n",
        "ראינא אן לא נכ'ליהא מן אסתג'דאד פאידה,\n",
        "ואן נתכלם עלי מסאלתין לאיקתין בהד'א אלמעני.\n",
        "'''\n",
        "#  ואן פי אכ'ר אלאמר יקום קאימא יאתי\n",
        "#   בדין ישבה אלדין אלחק, ויאתי\n",
        "#   ''' \n",
        " # בכתאב וכ'טאב, ויתכלם בעט'אים,  '''\n",
        "#    יעני אנה נזל עליה כתאב וכ'טאב,\n",
        "#     ויתכלם בעט'אים, יעני אנה נזל עליה כתאב,\n",
        "#      ואנה קאל אללה לה, וקאל הו ללה,\n",
        "#       וגיר ד'לך מן כת'רה' כלאמה.\n",
        "# '''\n",
        "#forward_text(lines.split('\\n'),3,BATCH_SIZE)\n",
        "\n",
        "#C:\\Users\\TERNER-PC\\Downloads\\PGPID_5502_MS-TS-00008-00019-000-00001.xml (14 hits)\n",
        "#TS NS J 5\n",
        "#See TS 24 66\n",
        "PGPID_5502_MS='''ללשיך מצמון וואחדה ללשיך יוסף וואחדה לי ועוד אלמוגנז אלדי\n",
        "לא גיר דלך וקר אנפד ענדה במא לא כטר לה והוין ניה סכר\n",
        "וקנינה זביב ברסם אלצריאן אללה תעאלי יחכסהם עליך וטלם מולאי\n",
        "בדינאר זרניך וליס פו אלכלד מנה שי ולא מן אללאדן ואמא אלקרטאם קד\n",
        "שרא לה עבדה בדינאר קרטאם אלורקאעלה וצדר אליה נצף דטלכח\n",
        "ושר רטל צמג ושף רטל זאג והו מן עבדה מא לה קומה ואמא דינא\n",
        "אלקדטאם תצופה אלי הסאב עבדך פי אלזנגביו ואלפופל אלדי סלם באלשמה\n",
        "וישתרי מן לאי מא סהל פי בעץ אלמראסב אלואצלה ומולאי פלא\n",
        "יעלם בראו זכץ טוידי עזיז נפסה מן עבדה באפצל אלסלאםות\n",
        "עני אלזלדין אלסיידין באפצל אלסלאם ואלשיך כמה מנצוץ אלסלחם\n",
        "ואן תסהל אלי מן אלהול אנפד בה צחבה מא אתפקשראה ושלום\n",
        "'''  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "#C:\\Users\\TERNER-PC\\Downloads\\PGPID_5544_MS-TS-K-00025-00285-000-00002.xml (46 hits)\n",
        "PGPID_5544_MS='''בש רח\n",
        "נתכלם איצא פי אלרד עלי מן יקול אן אלשפחה אדא לם\n",
        "תעתק ואדא לם תתגייר אן אלקידושין אלתי קדסת בהא\n",
        "פאסדא ותחתאג אלי קידוסין אן תאניה ותלך אלקידוסין\n",
        "אלאולה אלתי קבצ(ת)הא והי עלי תלך אלחאלה קבל דכולהא\n",
        "ללמדהב לם תאתר פיהא שיא ולא בד מן קידוסין\n",
        "אן תאניה נסתדל בעון אללה עלי פסאד קולה פי דלך\n",
        "מן קולהם עליהם אלסלאם האומר לאשה הרי את\n",
        "מקודשת לי לאחר שאתגייר לאחר שתתגיירי לאחר\n",
        "שאשתחרר לאחר שתשתחרירי הרי זו מקודשת\n",
        "תפסיר דלך באכתסר ונקצד מא נטלבה בגיר\n",
        "תטויל מתאל דלך אנסאן יהודי גא אלי אמה גיר\n",
        "מעתוקה או גיר מגיירה קאל להא אנת מקדסא לי\n",
        "בהדה אלקידושין אלתי דפעתהא לך ⟦עס⟧ אלסעה פי\n",
        "חאלתך הדה ואנת אלסעה ממלוכה או ואנת עלי גיר\n",
        "מדהב ישראל אדא תסלמת מנה תלך אלקידוסין\n",
        "תם אנעתקת בעד דלך ואתגיירת לם ↑ אלי קידושין\n",
        "נחתאג\n",
        "אכרי בל אלקידוסין אלאולה קד חאזתהא לה ותשבתת\n",
        "בהא וצארת בתלך אלקידושין זוגתה בלא מחאלה ולא\n",
        "צן ולא שך ועלי אנה סלם להא אלקידושין והי עלי מדהב\n",
        "אל כפר או והי ממלוכה פקד צארת זוגתה לא מחאלה\n",
        "בתלך אלקידושין אלתי כאנת בינהמא מן אלאול והי עאדהא\n",
        "לם תדכל פי אלדין פכל מן קאל כלאף דלך ליס קולה צחיח\n",
        "פלמא פעלת מא אשרטה עליהא אלדי הו לאחר שתתגיירי\n",
        "ולאחר שתשתחרירי תם דלך אלעקד בינהמא תמאם\n",
        "אן כאמל ולא כאד אן ינפסך מנה שיא וכאנת זוגתה\n",
        "והו זוגהא למא אנהם עליהם אלסלאם עללו דלך בעלה\n",
        "קולהם מפני שלא הטעתו ולמא דכלת בעד דלך פי אל\n",
        "מדהב וכאנת קד קבצת אלקידוסין מן אלאול אעני מן\n",
        "קבל דכולהא פי אלמדהב צח אנהא תמת באלשרט\n",
        "אלתי שרטה מקדסהא עליהא ולמא כאן דלך כדלך צח\n",
        "אנהא לא הטעתו ובדלך מלכהא וצארת זוגתה ולא\n",
        "נחוגהא קידושין תאניה אלבתה פאן קאל קאיל\n",
        "ועארצנא מעארץ וקאל אן הדא אלאמה אלתי קדסהא\n",
        "ראובן פי בלד אלהנד לם יצח לנא אנהא כאנת פי\n",
        "תלך סאעה תקדיסהא אנהא טבלת ודכלת פי אלדין\n",
        "פנחתאג אלסעה אן נעיד אלתקדיס תאניה בעד\n",
        "דכולה פי אל מדהב קלנא לה עלי טריק אל מתאבעה\n",
        "יאיהו אלמעארץ לנא בקולך הדא הב אנהא כמא\n",
        "קלת עלי טריק מתאבעתנא פי מנאטרתך יגב\n",
        "אן יכון חכמהא כחכם מא נצתה אלמשנה פי האומר\n",
        "לאשה הרי את מקודשת לי לאחר שאתגייר לאחר\n",
        "שתתגיירי לאחר שאשתחרר לאחר שתשתחרירי\n",
        "הרי זו מקודשת פליס תתבת לה בעד דלך חגה\n",
        "וקד צח דלך ואתצח ובאללה אל תופיק\n",
        "'''\n",
        "#C:\\Users\\TERNER-PC\\Downloads\\PGPID_5550_MS-TS-NS-J-00021-000-00002.xml (26 hits)\n",
        "PGPID_5550_MS='''ידכר ל[\n",
        "ג אבהרה גיר רבע זנגביל לאן מולאי דכ[\n",
        "ואן אלנאכדא גלב יאכד עליה אלנול אלא מן גאנבה וכתאב מולאי[\n",
        "ענד עבדה מן עאם אול בהדא אלחדית ואלדי רמי מן אלשפארה\n",
        "פקד קסט עלי אלחמל פלא אדרי אן מולאי נסי אן ידכר דלך בכתבה\n",
        "אם כיף אלכבר לאן כתב מולאי אלדי מן עאם אול ענד עבדה מחתפץ\n",
        "]בהא לאן צח קימה אלחריר בעד אלמונה יז מתקאל ונצף שרית לעבדך\n",
        "אבהרה גיר רבע זנגביל ביא מתקאל סער ארבע מת⟦ל⟧אקיל אלבהאר אלבאקי\n",
        "לעבדך סתה מתאקיל ונצף כרג מנהא מתקאל תפאות אלחדיד ↑ מן אלעאם\n",
        "אלדי\n",
        "אלאול תבקא לעבדך ה מתאקיל ונצף שרית לעבדך נצף בהאר פלפל באלכביר\n",
        "גיר קיראט והדא אלדי דכרת לעבדך באלשפארה פי עאם וצל כתאב\n",
        "מולאי ענדי בדלך פינצר מולאי מא לחקני מן אלתקסיט ואיש מא\n",
        "תחצל לעבדך קימה אלכל כנת תשתרי מא קסם אללה תעאלי מן אלפלפל\n",
        "וגירה ותנפדה לי בבעץ אלמראכב אלמתכלצה אול אלזמאן ואמא\n",
        "מן אגל אלהיל אלדי ענד אלכארדאר לענה אללה פאני תכלמת מע\n",
        "בעץ אלנאס בדלך פדכר לי אן אלהיל לכאצתך ומא לנא פיה שי\n",
        "ואנמא כאן בינך ובין אלכארדאר מעאמלה ואנכסר עליה\n",
        "הדא אלשי פרדיתה לנא לאן עבדך ינפד יעול עליך בשרא שי\n",
        "הו וסואה ושי לא יחתאג אלא מעארצה ולא אלי תקדמה אלא שי נאץ\n",
        "פאן סהל אלשרא ואלא תרך לאן מן ירסל אליך מא ידכר לך אן\n",
        "תקדם לה עלי שי אלא ידכר אן תשתרי במא קסם אללה ורזק ותנפד\n",
        "בה ואנה יא מולאי אולא בגמיע אלאשיא וכאן וצל הדה אלסנה\n",
        "אלשיך אבן אסחאק בן יוסף ודכר אן אכוך מבשר וצל אלי דיאר מצר\n",
        "והו יטלב אלמגי אלי ענדך פתעלם דלך ואמא אלמאלח ואלקצאע אלדי\n",
        "]סאלם פאנה כאן נזל פי אלבחר אלי מרכב\n",
        "'''\n",
        "\n",
        "\n",
        "#C:\\Users\\TERNER-PC\\Downloads\\PGPID_5485_MS-OR-01081-J-00006-000-00001.xml (13 hits)\n",
        "PGPID_5485_MS='''אלממלוך\n",
        "בשם רחמן שמריה בן דויד רית\n",
        "יכץ אלחצרה אלעאליה אלסאמיה אלשיכיה\n",
        "אלאגליה אלמאלכה אלמנעמה אלמתפצלה\n",
        "אלמופקה אלסעידה אדאם אללה עזהא\n",
        "וגדד סעדהא וכבת צדהא וכצהא\n",
        "בגזיל אלסלאם וסהל סרעה לקאיהא\n",
        "אן שא אללה תעאלי וינהי אליהא\n",
        "אדאם אללה סעאדתהא תקדם\n",
        "כתבה אליהא פי אלכארם אלמבארך\n",
        "ופיהא מא יסתגני אלממלוך ען\n",
        "אעאדתה פי הדא אלכתאב ואנא\n",
        "'''\n",
        "model=load_checkpoint(use_checkpoint)\n",
        "  \n",
        "\n",
        "print_log_screen(\"\\nPGPID_5502_MS\")\n",
        "lines=PGPID_5502_MS\n",
        "print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace(\"'\",''))\n",
        "\n",
        "print_log_screen(\"\\nPGPID_5544_MS\")\n",
        "lines=PGPID_5544_MS\n",
        "print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace(\"'\",''))\n",
        "\n",
        "\n",
        "print_log_screen(\"\\nPGPID_5550_MS\")\n",
        "lines=PGPID_5550_MS\n",
        "print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace(\"'\",''))\n",
        "\n",
        "\n",
        "print_log_screen(\"\\nPGPID_5485_MS\")\n",
        "lines=PGPID_5485_MS\n",
        "print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace(\"'\",''))\n",
        "\n",
        "\n",
        "# if use_checkpoint!=\"NONE\":\n",
        "#   model=load_checkpoint(use_checkpoint)\n",
        "#   print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace(\"'\",''))\n",
        "#   #for google translate\n",
        "#   print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace('\\n','').replace('  ',''))\n",
        "\n",
        "if INSPECT:\n",
        "  #model might not be defined\n",
        "  print_log_screen(forward_text(lines.split('\\n'),1,BATCH_SIZE).replace('\\n','').replace('  ',''))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading checkpoing from /gdrive/My Drive/checkpoints/2021-11-04 09:37:50.447676/ckpt\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (128, None, 8)            384       \n",
            "                                                                 \n",
            " bidirectional_20 (Bidirecti  (128, None, 2048)        6352896   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_21 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_22 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_23 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_5 (Dense)             (128, None, 67)           137283    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,150,531\n",
            "Trainable params: 63,150,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "PGPID_5502_MS\n",
            "[105, 92, 107, 114, 99, 99, 116, 98, 95, 106, 94, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "1\n",
            "\n",
            "للشيك مضمّون واحدة للشيك يوسف واحدة لي وعود الموجنز الذي         \n",
            "لا غير ذلك وقرّ أنفد عنده بما لا خطر له وهوين نيّة سكر           \n",
            "وقنينه زبيب برسم الصريان الله تعالي يحكسهم عليك وظلم مولئ        \n",
            "بدينار زرنيك وليس فو الكلد منه شيء ولا من اللادن وأمّا القرطام قد\n",
            "شراً له عبده بدينار قرطام الورقاعلة وصدر إليه نصف دطلكح          \n",
            "وشرّ رطل صمج وشف رطل زاج وهو من عبده ما له قومه وأمّا دينا       \n",
            "القدظام تصوّره إلى هساب عبدك في الزنجبيو والفوفل الذي سلم بالشمه \n",
            "ويشترى من لأي ما سهل في بعض المراسب الواصلة ومولاي فلا           \n",
            "يعلّم برأو زخص طويدي عزيز نفسه من عبده بأفصل السلاموت            \n",
            "عني الزلدين السييدين بأفصل السلام والشيك كمه منصوص السلحم        \n",
            "وإن تسهل إلى من الهول أنفذ به صحبه ما اتّفقشراه وشلوم            \n",
            "                                                                 \n",
            "\n",
            "PGPID_5544_MS\n",
            "[9, 84, 84, 89, 92, 74, 81, 71, 76, 67, 66, 74, 74, 75, 78, 70, 76, 10, 81, 81, 88, 79, 92, 89, 84, 74, 79, 82, 79, 82, 71, 78, 75, 72, 77, 75, 75, 70, 77, 71, 66, 76, 71, 68, 68, 58, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "1\n",
            "\n",
            "بش رح                                              \n",
            "نتكلّم أيضاً في الرد على من يقول إنّ الشفحة إذا لم \n",
            "تعتق وإدا لم تتغيير أنّ القيدشين التي قدّست بها    \n",
            "فاسداً وتحتاج إلى قيدوسين أن تانيه وتلك القيدوسين  \n",
            "الأولة التي قبص ثنها وهي على تلك الحالّة قبل دخولها\n",
            "للمدهب لم تأثر فيها شيئاً ولا بدّ من قيدوسين       \n",
            "أن تانية نستدلّ بعون الله على فساد قوله في ذلك     \n",
            "من قولهم عليهم السلام هأؤمر لاشه هرى ات            \n",
            "مقودشت لي لأحر شاتغيير لأحر شتتغييريّ لأحر         \n",
            "شاشتحرّر لأحر شتشتحريريّ هريّ زو مقودشت            \n",
            "تفسير ذلك باكتسر ونقصد ما نطلبه بغير               \n",
            "تطويل مثّال ذلك إنسان يهوديّ جاء إلى أمّة غير      \n",
            "معتوقة أو غير مغييره قال لها أنت مقدّساً لي        \n",
            "بهذه القيدشين التي دفعتها لك عس السعة في           \n",
            "حالتك هذه وأنت السعة مملوكة أو وانت على غير        \n",
            "مدهب إسرائيل\" إذا تسلّمت منه تلك القيدوسين         \n",
            "ثمّ انعتقت بعد ذلك واتغييرت لم إلى قيدشين          \n",
            "نحتاج                                              \n",
            "أخرى بل القيدوسين الأولة قد حازتها له وتشبتت       \n",
            "بها وصارت بتلك القيدسين زوجته بلا محالة ولا        \n",
            "صن ولا شكّ وعلى أنّه سلم لها القيدسين وهي على مدهب \n",
            "ال كفر أو وهي مملوكة فقد صارت زوجته لا محالة       \n",
            "بتلك القيدسين التي كانت بينهما من الأوّل وهي عادها \n",
            "لم تدخل في الذين فكلّ من قال خلاف ذلك ليس قوله صحيح\n",
            "فلما فعلت ما أشرطه عليها الذي هو لاحر شتتغييري     \n",
            "ولاحر شتشتحريري ثمّ ذلك العقد بينهما تمام          \n",
            "إنّ كامل ولا كاد أن ينفسك منه شيئاً وكانت زوجته    \n",
            "وهو زوجها لما أنّهم عليهم السلام عللو ذلك بعله     \n",
            "قولهم مفنى شلاً هطعتو ولما دكلت بعد ذلك في ال      \n",
            "مدهب وكانت قد قبصت القيدوسين من الأوّل أعني من     \n",
            "قبل دخولها في المدهب صحّ أنّها تمت بالشرط          \n",
            "التي شرطه مقدّسها عليها ولمّا كان ذلك كذلك صح      \n",
            "أنّها لا هطعتو وبذلك ملكها وصارت زوجته ولا         \n",
            "نحوجها قيدشين ثانية البته فإن قال قائل             \n",
            "وعارضنا معارض وقال إنّ هذا الأمّة التي قدّسها      \n",
            "رأوبن في بلد الهند لم يصحّ لنا أنّها كانت في       \n",
            "تلك ساعة تقديسها أنّها طبلت ودخلت في الدين         \n",
            "فنحتاج السعة أن نعيد التقديس ثانية بعد             \n",
            "دخوله في ال مدهب قلنا له على طريق ال متابعة        \n",
            "يايهو المعارض لنا بقولك هذا هب أنّها كما           \n",
            "قلت على طريق متابعتنا في مناظرتك يجب               \n",
            "أن يكون حكمها كحكم ما نصته المشنه في هؤمر          \n",
            "لاشه هرى ات مقودشت لي لأحر شاتغيير لأحر            \n",
            "شتتغييريّ لأحر شاشتحرّر لأحر شتشتحريري             \n",
            "هري زو مقودشت فليس تثبت له بعد ذلك حجّه            \n",
            "وقد صحّ ذلك واتّصح وبالله أل توفيق                 \n",
            "                                                   \n",
            "\n",
            "PGPID_5550_MS\n",
            "[13, 65, 104, 94, 102, 102, 107, 112, 107, 8, 115, 100, 100, 105, 98, 98, 95, 87, 92, 102, 94, 100, 96, 104, 106, 65, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "1\n",
            "\n",
            "يذكر ل                                                           \n",
            "ج أبهره غير ربع زنجبيل لأنّ مولاي ذك                             \n",
            "وأنّ الناكذا جلب يأخذ عليه النول إلا من جانبه وكتاب مولائ        \n",
            "عند عبده من عام أوّل بهذا الحديث والذي رمي من الشفارة            \n",
            "فقد قسط على الحمل فلا أدري أنّ مولئ نسي أن يذكر ذلك بكتبة        \n",
            "أم كيف الكبر لأن كتب مولاي الذي من عام أوّل عند عبده محتفص       \n",
            "بها لأنّ صحّ قيمه الحرير بعد المونة يز متقال ونصف شريت لعبدك     \n",
            "أبهره غير ربع زنجبيل بياً متقال سعر أربع متلاقيل البهار الباقي   \n",
            "لعبدك سته متاقيل ونصف خرج منها متقال تفاوت الحديد من العام       \n",
            "الذي                                                             \n",
            "الأوّل تبقاً لعبدك ه متاقيل ونصف شريت لعبدك نصف بهار فلفل بالكبير\n",
            "غير قيراط وهذا الذي ذكرت لعبدك بالشفارة في عام وصل كتاب          \n",
            "مولاي عندي بذلك فينصر مولائ ما لحقني من التقسيط وأيش ما          \n",
            "تحصل لعبدك قيمة الكلّ كنت تشترى ما قسم الله تعالي من الفلفل      \n",
            "وغيره وتنفذه لي ببعض المراكب المتخلّصة أول الزمان وأمّا          \n",
            "من أجل الهيل الذي عند الكاردار لعنه الله فإنّي تكلّمت مع         \n",
            "بعض الناس بذلك فذكر لي أنّ الهيل لخاصتك وما لنا فيه شي           \n",
            "وإنّما كان بينك وبين الكاردار معاملة وانكسر عليه                 \n",
            "هذا الشيء فرديّته لنا لأنّ عبدك ينفذ يعول عليك بشراً شي          \n",
            "هو وسواه وشيء لا يحتاج إلاّ معارضة ولا إلى تقدّمه إلا شيء ناص    \n",
            "فإنّ سهل الشرا وألا ترك لأنّ من يرسل إليك ما يذكر لك أنّ         \n",
            "تقدّم له على شيء ألا يذكر أن تشترى بما قسم الله ورزق وتنفذ       \n",
            "به وأنّه يا مولاي أولاً بجميع الأشياء وكان وصل هذه السنة         \n",
            "الشيك أبن إسحاق بن يوسف وذكر أن أكوك مبشر وصل إلى ديار مصر       \n",
            "وهو يطلب المجي إلى عندك فتعلّم ذلك وأمّا المالح والقضاع الذي     \n",
            "سالم فإنّه كان نزل في البحر إلى مركّب                            \n",
            "                                                                 \n",
            "\n",
            "PGPID_5485_MS\n",
            "[14, 47, 64, 61, 56, 48, 54, 51, 41, 52, 48, 46, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "1\n",
            "\n",
            "المملوك                           \n",
            "بشم رحمن شمرية بن دويد ريت        \n",
            "يخص الحضرة العالية السامية الشيكية\n",
            "الأجلية المالكة المنعمة المتفصّلة \n",
            "الموفقة السعيدة أدام الله عزها    \n",
            "وجدد سعدها وكبت صدّها وخصها       \n",
            "بجزيل السلام وسهل سرعة لقائها     \n",
            "إن شاء الله تعالى وينهي إليها     \n",
            "أدام الله سعادتها تقدّم           \n",
            "كتبة إليها في الكارم المبارك      \n",
            "وفيها ما يستغني المملوك عن        \n",
            "إعادته في هذا الكتاب وأنا         \n",
            "                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec01aa6-c763-48bd-d4e4-bcb5b8af858f"
      },
      "source": [
        "#@title MAIN { display-mode: \"form\" }\n",
        "num_of_pretrain_epocs =  10#@param {type:\"integer\"}\n",
        "use_synthetic_data = True #@param {type:\"boolean\"}\n",
        "dropout_percent = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "subject_of_mail = \"Princeton: add synt on and continue with last checkpoint\" #@param {type:\"string\"}\n",
        "use_checkpoint = \"2021-11-09 14:27:55.044957\" #@param [\"NONE\", \"2021-11-04 09:37:50.447676\", \"2021-11-09 14:27:55.044957\"]\n",
        "kuzari_data = False #@param {type:\"boolean\"}\n",
        "rasag_data = False #@param {type:\"boolean\"}\n",
        "CELL_NAME=\"MAIN\"\n",
        "print(\"start\")\n",
        "##############################################\n",
        "####RUN PARAMETERS:\n",
        "mail_subject=subject_of_mail\n",
        "mail_subject=this_time+\":\"+mail_subject\n",
        "pretrain_letter=num_of_pretrain_epocs\n",
        "synth=use_synthetic_data\n",
        "keep_percent=1-dropout_percent\n",
        "if use_checkpoint==\"NONE\":\n",
        "  USE_CHECKPOINT=False\n",
        "  model=rebuild()\n",
        "else:\n",
        "  USE_CHECKPOINT=True\n",
        "  model=load_checkpoint(use_checkpoint)\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print(\"desc\")\n",
        "print_log_screen(description)\n",
        "#################################\n",
        "#INIT\n",
        "BEST_ACCURACY=1\n",
        "f= open(\"my_log.txt\",\"w+\") #attach to mail summary of tests\n",
        "init_random()\n",
        "\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "#optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "optimizer = tf.optimizers.RMSprop(0.001)\n",
        "\n",
        "####PRETRAIN\n",
        "if not USE_CHECKPOINT and pretrain_letter>0:\n",
        "  print_log_screen(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print_log_screen('-'*200)\n",
        "\n",
        "print_log_screen(\"START TRAIN\")\n",
        "\n",
        "for jjj in range(6): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(5): #iter without sendmail and only partial test    \n",
        "    print(\"KEEP PRECENT\",keep_percent)\n",
        "    if keep_percent<1:\n",
        "        train_dataset_double_kuzari= produce_dataset(\n",
        "            create_parralel_phrases(kuzari_lines_train,keep_percent),\n",
        "            to_shuffle=TO_SHUFFLE)        \n",
        "    view_data(train_dataset_double_kuzari)\n",
        "    if synth:\n",
        "      dataset_double_synt=gen_all_synth(keep_percent).concatenate(train_dataset_double_kuzari).shuffle(500)\n",
        "      view_data(dataset_double_synt)\n",
        "      loss=train(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train(train_dataset_double_kuzari)\n",
        "    \n",
        "    #total_test_loss,total_accuracy=test(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    total_test_loss,total_accuracy=test(limit=5)\n",
        "    test(this_dataset=test_dataset_double_rasag,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print_log_screen('-'*200)\n",
        "    test_kfir(kfir_kuzari_test_SWITCH,False)\n",
        "    test_kfir(kfir_rasag_test_SWITCH,False)\n",
        "    print_log_screen('-'*200)\n",
        "    \n",
        "  print_log('FULL STATISTICS')\n",
        "  print_log('='*200)\n",
        "  test_kfir(kfir_kuzari_test_SWITCH,True,None,False)  \n",
        "  test_kfir(kfir_rasag_test_SWITCH,True,None,False)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test(test_dataset_double_rasag)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  if all_accuracy<BEST_ACCURACY:\n",
        "    save_checkpoint(\"improvement in accuracy. Current LER on KUZARI test data: \"+str(all_accuracy))\n",
        "    BEST_ACCURACY=all_accuracy\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print_log(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print_log(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print_log(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  \n",
        "  f.write(\"EPOCH \"+str(GLOBAL_epoch)+'\\n')\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  \n",
        "  log_flush()\n",
        "  f.flush()\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "\n",
        "\n",
        "  \n",
        "f.close()\n",
        "close_log()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "loading checkpoing from /gdrive/My Drive/checkpoints/2021-11-09 14:27:55.044957/ckpt\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (128, None, 8)            384       \n",
            "                                                                 \n",
            " bidirectional_24 (Bidirecti  (128, None, 2048)        6352896   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_25 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_26 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_27 (Bidirecti  (128, None, 2048)        18886656  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_6 (Dense)             (128, None, 67)           137283    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,150,531\n",
            "Trainable params: 63,150,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "desc\n",
            "\n",
            "pretrain: 10\n",
            "with synth data\n",
            "dropout:0.7\n",
            "\n",
            "init random to 1\n",
            "START TRAIN\n",
            "KEEP PRECENT 0.7\n",
            "‫ אע_מ מ_ אע__אדהמ א_חד_  |  أعظم من اعتقادهم الحدث\n",
            "‫ _לי __ה אל_רג_ א_ר_ח_ניה  |  إلى هذه الدرجة الروحانية\n",
            "‫ אלאל_ה_ אלדי רא_ קרבה  |  الإلهيّ الذي رام قربه\n",
            "‫ . _לא א__קל_ מ_ אלמאל  |  . ولا التقلّل من المال\n",
            "‫ , ות_ת_ בר_י_ אלנור  |  , وتلتذّ برؤية النور\n",
            "‫ __ א_נפ_ . וה_ה _לתי  |  في النفس . وهذه التي\n",
            "‫ וג_ __צורו_ כיפיתה ,  |  وجه يتصورون كيفيته ,\n",
            "‫ ___ מנ אלמי_ _י חכ_ חכמ  |  شخص من الميل في حكم حكم\n",
            "‫ אל_דמ __לקא פ__א __תמל  |  العدم مطلقا فيما يحتمل\n",
            "‫ : פי _נ אלמ_א_יר אער__  |  : في أن المقادير أعراض\n",
            "‫ _ ו_במא ___ _ל__ל מדה  |  , وربما عرف العقل مدة\n",
            "‫ , וה__ א_מעני ל_ס מ_ק___  |  , وهذا المعنى ليس معقولاً\n",
            "‫ אלמ_כב_ _ ויגעל פי  |  المركّبة , ويجعل في\n",
            "‫ , _מנ חית __ כ_יה ,  |  , ومن حيث هي كلّيّة ,\n",
            "‫ נרתא_ _י ___דל ענד פ_ספתנ_  |  نرتاض في الجدل عند فلسفتنا\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1447: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1430: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n",
            "Epoch 1 Batch 0 Loss 15.2040\n",
            "Epoch 1 Batch 10 Loss 14.6574\n",
            "Epoch 1 Batch 20 Loss 12.3987\n",
            "Epoch 1 Batch 30 Loss 12.0262\n",
            "Epoch 1 Batch 40 Loss 10.3956\n",
            "Epoch 1 Batch 50 Loss 10.3828\n",
            "Epoch 1 Batch 60 Loss 10.0493\n",
            "Epoch 1 Batch 70 Loss 10.6050\n",
            "Epoch 1 Batch 80 Loss 9.5670\n",
            "Epoch 1 Batch 90 Loss 10.1939\n",
            "Epoch 1 Batch 100 Loss 9.6903\n",
            "Epoch 1 Batch 110 Loss 10.2891\n",
            "Epoch 1 Batch 120 Loss 10.4507\n",
            "Epoch 1 Batch 130 Loss 10.3588\n",
            "Epoch 1 Batch 140 Loss 9.7470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwpwDinoeIu5"
      },
      "source": [
        "#MAIN OUTPUT (ABOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM"
      },
      "source": [
        "# test()\n",
        "#test(test_dataset_double_rasag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH"
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2"
      },
      "source": [
        "# for i in range(50):\n",
        "#   train()\n",
        "#   test(single_words_test_dataset,limit=5)\n",
        "#   test(limit=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX"
      },
      "source": [
        "  # test(only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05"
      },
      "source": [
        "test(this_dataset=test_dataset_double_kuzari,only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02"
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb"
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double_rasag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA"
      },
      "source": [
        "test(this_dataset=test_dataset_double_rasag,only_first=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zcZ9IdHq23e"
      },
      "source": [
        "#OLD STAFF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM"
      },
      "source": [
        "# CELL_NAME=\"SMALL TRY\"\n",
        "\n",
        "# this_string='وأهل الأديان ثمّ على'\n",
        "# this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "# this_string='ثمّ'\n",
        "\n",
        "# #this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string=normalize_unicode(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "# print_log_screen(len(this_string))\n",
        "# this_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2"
      },
      "source": [
        "# CELL_NAME=\"TRY SYNTH\"\n",
        "\n",
        "# #ACTIVATE\n",
        "# ibnsina_text=load_lines_synth()\n",
        "\n",
        "# #STATSTICS OF SYNTH TEXT\n",
        "# sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "# print_log(\"NOT IN LETTER LIST:\")\n",
        "# for c in sina_vocab:\n",
        "#    if c not in targ_lang.char2idx:\n",
        "#       print_log(\"(\",c,\")\")\n",
        "\n",
        "# print_log(\"\\nLETTER COUNTS\")\n",
        "# for i in range(len(sina_vocab)):\n",
        "#   print_log(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   \n",
        "\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "# #sina_words=ibnsina_text.split(\" \")\n",
        "# #ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "# sina_words=ibnsina_text.split() #for removing also newlines\n",
        "# print_log(ibnsina_text[:100])\n",
        "# sina_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA"
      },
      "source": [
        "##Shuffled test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli"
      },
      "source": [
        "# CELL_NAME=\"SHUFFLED TEST\"\n",
        "\n",
        "# def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "#   val_inputs_list=[]\n",
        "#   val_outputs_list=[]\n",
        "\n",
        "#   for i,j,l1,l2 in test_dataset:\n",
        "#     for tt in range(BATCH_SIZE):\n",
        "#      # print_log(i[tt],j[tt])\n",
        "#       i_prediction=decode_JA(tf.constant(i[tt]),l1[tt])\n",
        "#       j_prediction=decode_arr(tf.constant(j[tt]),l2[tt])      \n",
        "#       if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "#         val_inputs_list+=i_prediction.split()\n",
        "#         val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "#   print_log(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "#   word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "#   print_log(\"BEFORE SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   random.shuffle(word_pairs)\n",
        "#   print_log(\"AFTER SHUFFLE\")\n",
        "#   for i in word_pairs[:5]:\n",
        "#     print_log(i)\n",
        "#   return word_pairs\n",
        "\n",
        "# #TESTING\n",
        "# #word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag.take(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L"
      },
      "source": [
        "# CELL_NAME=\"DEF GET SHUFFLE DATA\"\n",
        "# def get_shuffled_data(word_pairs):\n",
        "\n",
        "#   accum=0\n",
        "#   heb_acum=\"\"\n",
        "#   arab_acum=\"\"\n",
        "#   results_line=[]\n",
        "#   for i,j in word_pairs:\n",
        "#     if accum>19:\n",
        "#       results_line.append(undouble_hebrew(heb_acum)+'\\t'+arab_acum)\n",
        "#       assert(len(i)%2==0)\n",
        "#       accum=len(i)/2\n",
        "#       heb_acum=i\n",
        "#       arab_acum=j\n",
        "#     else:\n",
        "#       heb_acum+=\" \"+i\n",
        "#       arab_acum+=\" \"+j \n",
        "#       assert(len(i)%2==0)\n",
        "#       accum += len(i)/2 + 1;\n",
        "#   results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "#   print_log(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "#   input_tensor_shuffle, target_tensor_shuffle \\\n",
        "#   ,input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "#   print_log(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "#   print_log(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "#   BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "#   shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "#                                                             target_tensor_shuffle,\n",
        "#                                                             input_lenghts_shuffle,\n",
        "#                                                             target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#   shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#   return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD"
      },
      "source": [
        "CELL_NAME=\"GEN SHUFFLED DATA\"\n",
        "\n",
        "# word_pairs=get_shuffled_word_pairs(test_dataset_double_kuzari)\n",
        "# shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "# view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "# word_pairs1=get_shuffled_word_pairs(test_dataset_double_rasag)\n",
        "# shuffle_test_dataset_double_rasag=get_shuffled_data(word_pairs1)\n",
        "# view_data(shuffle_test_dataset_double_rasag)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG"
      },
      "source": [
        "# CELL_NAME=\"DEF TEST_SHUFFLE\"\n",
        "# def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "#   return test(this_dataset=data,limit=limit)\n",
        "# #shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6"
      },
      "source": [
        "\n",
        "##Single words test (NOT USED NOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu"
      },
      "source": [
        "# CELL_NAME=\"GEN SINGEL WORDS\"\n",
        "\n",
        "# # results_line=[]\n",
        "# # for i,j in word_pairs:\n",
        "# #     results_line.append(undouble_hebrew(i)+'\\t'+j)\n",
        "\n",
        "# # print_log(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "# # input_tensor_shuffle, target_tensor_shuffle \\\n",
        "# # , input_lenghts_shuffle,target_lengths_shuffle = create_data_tensors(create_parralel_phrases(results_line))\n",
        "\n",
        "# # print_log(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "# # BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "# # single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "# #                                                                 target_tensor_shuffle,\n",
        "# #                                                                 input_lenghts_shuffle,\n",
        "# #                                                                 target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# # single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# # view_data(single_words_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}