{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 8 march",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ori1234/JA-RNN/blob/master/Thesis_8_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFP8V9qDldY",
        "colab_type": "text"
      },
      "source": [
        "https://webcache.googleusercontent.com/search?q=cache:viNLSTwuTS0J:https://www.reddit.com/r/datascience/comments/bkrzah/google_colab_how_to_avoid_timeoutdisconnect_issues/+&cd=2&hl=en&ct=clnk&gl=il\n",
        "\n",
        "Go to the google Colab console (ctrl+shift+i) :\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
        "\n",
        "THIS console.log(\"Working\");document.querySelector(\"colab-connect-button\")\n",
        "\n",
        "Dont exit the console until you get \"Working\" as the output in the console window. It would keep on clicking the page and prevent it from disconnecting.\n",
        "\n",
        "\n",
        "\n",
        "Note: Although I did the same thing, I forgot abt it for 12 hours and got my GPU privileges suspended temporarily. Make sure you dont run anything for more than 12 hrs on Colab!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "function ClickConnect(){console.log(\"Working\");if (document.querySelector(\"paper-button#ok\")!=null){document.querySelector(\"paper-button#ok\").click()}}val=setInterval(ClickConnect,60000)\n",
        "\n",
        "clearInterval(val)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltiajqo3ptE",
        "colab_type": "text"
      },
      "source": [
        "**SUMMERY**\n",
        "\n",
        "\n",
        "\n",
        "this is based on https://colab.research.google.com/drive/1m6VuABiVrkKmhUGitHFDl99r6-qe_ZoJ#scrollTo=OHn4Dct23jEm at tirza's account titled:\n",
        "FINAL 4CURRENT__CLEAN_Copy_of_CTC_UnEquel_Input_Output_heb_to_ar.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF7hxxLw2gZp",
        "colab_type": "text"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUeDrwE2cb7",
        "colab_type": "code",
        "outputId": "90030d55-9adc-4adb-8a15-caff5855fe27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5b0420cd036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lTDTAb9VqY",
        "colab_type": "text"
      },
      "source": [
        "Global Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ8uN3dj9Uhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64*2 #original 64*2 but don't have enough data right now\n",
        "\n",
        "this_notebook=\"/gdrive/My Drive/RMSPROP Back to Thesis.ipynb\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH_iDyGn0mWh",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roia04jL0jCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "#The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.\n",
        "#We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMwuVCRK0upw",
        "colab_type": "text"
      },
      "source": [
        "Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gEIvtr0znU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "#tf.random.set_seed(1)\n",
        "tf.compat.v1.set_random_seed(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9DB__L2M9g",
        "colab_type": "text"
      },
      "source": [
        "Arabic Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tqrGT52Nrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arab_nikud=[u\"\\u0652\",u\"\\u0650\", u\"\\u064F\",u\"\\u064E\", ]#sukuun,kasra, Damma,# fatHa\n",
        "tanween=[u\"\\u064B\", # fatHatayn\n",
        "         u\"\\u064C\", # Dammatayn\n",
        "         u\"\\u064D\", ]\n",
        "shada=u\"\\u0651\"\n",
        "\n",
        "hamza_on_line=u\"\\u0621\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZBFNnRFscK",
        "colab_type": "text"
      },
      "source": [
        "#UTILS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l17b5XJR_5Mj",
        "colab_type": "text"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrphwz1_7Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LTRchar=u'\\u202B'\n",
        "#https://stackoverflow.com/questions/50975763/how-right-to-left-rtl-google-colaboratory\n",
        "#https://stackoverflow.com/questions/51576756/display-render-an-html-file-inside-jupyter-notebook-on-google-colab-platform\n",
        "#https://stackoverflow.com/questions/42556063/right-to-left-and-left-to-right-printed-nicely\n",
        "\n",
        "\n",
        "##HELPERS\n",
        "def print_by_idx_CTC(idx,dict,leng=-1):\n",
        "     # print(len(idx))\n",
        "      if leng==-1:\n",
        "       # print(len(idx))\n",
        "        leng=len(idx)\n",
        "        \n",
        "      result=\"\"\n",
        "      for i in idx[:leng]:\n",
        "        result += dict[i.numpy()]\n",
        "   #   print(result,\"#\"+str(leng))\n",
        "      return result\n",
        "\n",
        "def view_data(data):\n",
        "  for i,j,l1,l2 in data.take(3):\n",
        "    print(LTRchar,print_by_idx_CTC(i[0],inp_lang.idx2char,l1[0]),\" | \",print_by_idx_CTC(j[0],targ_lang.idx2char,l2[0]))\n",
        "\n",
        "\n",
        "#s is a sentence\n",
        "#dict as a dictionary that maps chars to ints\n",
        "# def vectorize(s,dict):\n",
        "#   return [dict[c] for c in s]\n",
        "def vectorize(s,dict):  \n",
        "  #return [dict[c] for c in s]\n",
        "  res=[]\n",
        "  for c in s:\n",
        "    if c not in dict:\n",
        "      print(LTRchar,s,\":\", c,\"not in dict\")\n",
        "    else:\n",
        "      res.append(dict[c])  \n",
        "  return res\n",
        "\n",
        "def un_double_letters(s):\n",
        "  res=\"\"\n",
        "  words=s.split()\n",
        "  for w in words:\n",
        "    for i in range(0,len(w),2):\n",
        "      res+=w[i]\n",
        "    res+=' '\n",
        "  return res.strip()\n",
        "\n",
        "def init_log():\n",
        "  global fLog\n",
        "  fLog= open(\"NEW_all_log.txt\",\"w+\")\n",
        "\n",
        "\n",
        "def print_to_log(s):  \n",
        "  print(s)\n",
        "  fLog.write(s+\"\\n\")\n",
        "  fL.flush\n",
        "\n",
        "BLANK=\"_\"\n",
        "def clear_blank(s):\n",
        "  return s.replace(\"_\",\"\")\n",
        "\n",
        "def clear_arab_punctuation(s): \n",
        "  return s.replace(\"،\",\",\").replace(\"؛\",\";\").replace(\"؟\",\"?\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EONT-4FvdF",
        "colab_type": "text"
      },
      "source": [
        "send mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YGN7tvFu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NEED TO ALLOW LESS SECURE APPS AT:  \n",
        "#https://myaccount.google.com/lesssecureapps?utm_source=google-account&utm_medium=web\n",
        "\n",
        "#Send Alert Email at finish with GMail\n",
        "##ref: https://webcache.googleusercontent.com/search?q=cache:peuNIUcC5eAJ:https://rohitmidha23.github.io/Colab-Tricks/+&cd=1&hl=en&ct=clnk&gl=il\n",
        "#https://www.google.com/search?safe=strict&rlz=1C1SQJL_iwIL818IL818&sxsrf=ACYBGNQn05BVmX0bKCQOdxEZsOV8sylztA%3A1568909507810&ei=w6iDXeKYMZLSxgO1qYSICg&q=smtplib.smtp+sendmail+attachment&oq=smtplib.smtp+sendmail+att&gs_l=psy-ab.3.0.33i21j33i160.1435.2378..3438...0.2..0.188.632.0j4......0....1..gws-wiz.......0i71j0j0i22i30.7MbuYV36t10\n",
        "####how to define app password see: https://kinsta.com/knowledgebase/free-smtp-server/\n",
        "\n",
        "import smtplib\n",
        "from os import path\n",
        "from os.path import basename\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "\n",
        "def send_results(subject,description):\n",
        "  THISTHIS=\"qczvfrlypitxxsfc\"\n",
        "\n",
        "  server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "  #server = smtplib.SMTP('localhost')\n",
        "  server.starttls()\n",
        "  server.login(\"kuti.sulimani@gmail.com\", THISTHIS)\n",
        "  #msg = \"COLAB WORK FINISH ALERT!\"\n",
        "  msg = MIMEMultipart()\n",
        "  msg['From'] = \"sender_gmail_here@gmail.com\"\n",
        "  msg['To'] = COMMASPACE.join([\"oriterner@gmail.com\"])\n",
        "  msg['Date'] = formatdate(localtime=True)\n",
        "  msg['Subject'] = subject\n",
        "\n",
        "\n",
        "  msg.attach(MIMEText(description))\n",
        "  #files=[\"/content/sample_data/README.md\",\"/content/train.png\"]  #list of graphs to send or logs....\n",
        "  files=[\"/content/train.png\",\"/content/test.png\",\"/content/accuracys.png\",\"/content/my_log.txt\"]  #list of graphs to send or logs....\n",
        "  #files.append(this_notebook)\n",
        "  for f in files or []:\n",
        "      if not path.exists(f):\n",
        "        continue\n",
        "      with open(f, \"rb\") as fil:\n",
        "          part = MIMEApplication(\n",
        "              fil.read(),\n",
        "              Name=basename(f)\n",
        "          )\n",
        "      # After the file is closed\n",
        "      part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
        "      msg.attach(part)\n",
        "\n",
        "\n",
        "  #server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg)\n",
        "  server.sendmail(\"sender_gmail_here@gmail.com\", \"oriterner@gmail.com\", msg.as_string())\n",
        "  server.quit()\n",
        "#send_results(\"test\",\"test body\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg25sCsGdYP",
        "colab_type": "text"
      },
      "source": [
        "plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDNZ49DGcgi",
        "colab_type": "code",
        "outputId": "a0b36e88-8d7a-4651-8fcd-acd8af7565c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "losses=[1,2,3]\n",
        "def my_plot_save(data_series,save_name,decor='r--'):\n",
        "  t = range(0, len(data_series))\n",
        "  plt.plot(t, data_series, decor)\n",
        "  plt.savefig(save_name) #\"/content/foo.png\"\n",
        "  plt.show()\n",
        "my_plot_save(losses,\"train.png\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCn2zeq_2sGE",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEx-7TJ2uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hakuzari=\"/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt\"\n",
        "haemunot=\"/gdrive/My Drive/thesis-data/haemunot_vedeot/for_ctc_train22_FRIDBERG5.txt\"\n",
        "kfir_kuzari_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_kuzari_test.txt\"\n",
        "kfir_rasag_test=\"/gdrive/My Drive/thesis-data/kfir1/kfir_rasag_test.txt\"\n",
        "\n",
        "def load_lines(input_file=hakuzari):\n",
        "  with open(input_file, 'rb') as f:\n",
        "    text = f.read().decode(encoding='utf-8')\n",
        "    text=text.replace('ֿ',\"'\")   ##I ADDED THIS. than did the replacement in the file uploaded to drive\n",
        "    text=text.replace(\"&nbsp\",\"\")\n",
        "  lines=text.strip().split('\\n') \n",
        "  print(lines)\n",
        "  print(len(lines)) # 10923 kuzari 10358 haemunot\n",
        "  return lines\n",
        "\n",
        "#lines=load_lines(haemunot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtssRgtEvv1"
      },
      "source": [
        "###letter mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMPZNXh4Zoz",
        "colab_type": "code",
        "outputId": "1ea3959d-2208-4c12-ff16-f0cdc8274758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#all letters\n",
        "\n",
        "\n",
        "\n",
        "#arab_letters=\"آأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي\" \n",
        "#heb_by_order=\"אאואיאבה'תת'ג'חכ'דד'רזסשצצ'טט'עגפקכלמנהויי\"\n",
        "#TODO EDIT: simple map will take first char in maped list for each hebrew letter\n",
        "\n",
        "\n",
        "tag=\"'\"\n",
        "additional_letters=\".H,?:;[]()!-\\\" 0123456789\"\n",
        "\n",
        "\n",
        "letter_dict={   #make sure all are here\n",
        "    \"א\": \"اإآأ\", # mising alif wasla  \n",
        "    \"ב\":\"ب\" ,\n",
        "    \"ג\":\"غ\",\n",
        "    \"ג\"+tag:\"ج\",\n",
        "    \"ד\":\"د\",\n",
        "    \"ד\"+tag:\"ذ\",\n",
        "    \"ה\":\"ه\",\n",
        "    \"ה\"+tag:\"ة\",\n",
        "    \"ו\":\"وؤ\",\n",
        "    \"ז\":\"ز\",\n",
        "    \"ח\":\"ح\",\n",
        "    \"ט\":\"ط\",\n",
        "    \"ט\"+tag:\"ظ\",\n",
        "    \"י\":\"يىئ\",\n",
        "    \"כ\":\"ك\",\n",
        "    \"כ\"+tag:\"خ\",\n",
        "    \"ל\":\"ل\",\n",
        "    \"מ\":\"م\",\n",
        "    \"נ\":\"ن\",\n",
        "    \"ס\":\"س\",\n",
        "    \"ע\":\"ع\",\n",
        "    \"פ\":\"ف\",\n",
        "    \"צ\":\"ص\",\n",
        "    \"צ\"+tag:\"ض\",\n",
        "    \"ק\":\"ق\",\n",
        "    \"ר\":\"ر\",\n",
        "    \"ש\":\"ش\",\n",
        "    \"ת\":\"ت\",\n",
        "    \"ת\"+tag:\"ث\",\n",
        "}\n",
        "\n",
        "for i in additional_letters:\n",
        "  letter_dict[i]=i\n",
        "\n",
        "arab_heb_maping={}\n",
        "heb_arab_maping={}\n",
        "for heb,arr in letter_dict.items():\n",
        "  heb_arab_maping[heb]=arr[0]\n",
        "  for a in arr:\n",
        "    arab_heb_maping[a]=heb\n",
        "\n",
        "\n",
        "print(len(arab_heb_maping))\n",
        "print(arab_heb_maping)\n",
        "print(len(heb_arab_maping))\n",
        "print(heb_arab_maping)\n",
        "\n",
        "\n",
        "\n",
        "# arab_letters+=additional_letters\n",
        "# heb_by_order+=additional_letters\n",
        "\n",
        "\n",
        "# heb_by_order_tag=[]\n",
        "# iterator = iter(range(len(heb_by_order)))\n",
        "\n",
        "# for i in iterator:\n",
        "#   if i+1!=len(heb_by_order) and heb_by_order[i+1]==tag:\n",
        "#     heb_by_order_tag.append(heb_by_order[i]+tag)\n",
        "#     next(iterator, None)\n",
        "#   else:\n",
        "#     heb_by_order_tag.append(heb_by_order[i])\n",
        "\n",
        "# len(arab_letters)\n",
        "\n",
        "\n",
        "# arab_heb_maping=dict(zip(arab_letters,heb_by_order_tag))\n",
        "\n",
        "# print(len(arab_heb_maping))\n",
        "# print(arab_heb_maping)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59\n",
            "{'ا': 'א', 'إ': 'א', 'آ': 'א', 'أ': 'א', 'ب': 'ב', 'غ': 'ג', 'ج': \"ג'\", 'د': 'ד', 'ذ': \"ד'\", 'ه': 'ה', 'ة': \"ה'\", 'و': 'ו', 'ؤ': 'ו', 'ز': 'ז', 'ح': 'ח', 'ط': 'ט', 'ظ': \"ט'\", 'ي': 'י', 'ى': 'י', 'ئ': 'י', 'ك': 'כ', 'خ': \"כ'\", 'ل': 'ל', 'م': 'מ', 'ن': 'נ', 'س': 'ס', 'ع': 'ע', 'ف': 'פ', 'ص': 'צ', 'ض': \"צ'\", 'ق': 'ק', 'ر': 'ר', 'ش': 'ש', 'ت': 'ת', 'ث': \"ת'\", '.': '.', 'H': 'H', ',': ',', '?': '?', ':': ':', ';': ';', '[': '[', ']': ']', '(': '(', ')': ')', '!': '!', '-': '-', '\"': '\"', ' ': ' ', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9'}\n",
            "53\n",
            "{'א': 'ا', 'ב': 'ب', 'ג': 'غ', \"ג'\": 'ج', 'ד': 'د', \"ד'\": 'ذ', 'ה': 'ه', \"ה'\": 'ة', 'ו': 'و', 'ז': 'ز', 'ח': 'ح', 'ט': 'ط', \"ט'\": 'ظ', 'י': 'ي', 'כ': 'ك', \"כ'\": 'خ', 'ל': 'ل', 'מ': 'م', 'נ': 'ن', 'ס': 'س', 'ע': 'ع', 'פ': 'ف', 'צ': 'ص', \"צ'\": 'ض', 'ק': 'ق', 'ר': 'ر', 'ש': 'ش', 'ת': 'ت', \"ת'\": 'ث', '.': '.', 'H': 'H', ',': ',', '?': '?', ':': ':', ';': ';', '[': '[', ']': ']', '(': '(', ')': ')', '!': '!', '-': '-', '\"': '\"', ' ': ' ', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adVEVZ_vYdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TODO need to edit this. this is for baseline\n",
        "\n",
        "def simple_letter_map(heb_str): \n",
        "  res=[]\n",
        "  tag=\"'\"\n",
        "  iterator = iter(range(len(heb_str)))\n",
        "  for i in iterator:\n",
        "    if i+1!=len(heb_str) and heb_str[i+1]==tag:     \n",
        "      if heb_str[i]+tag in heb_arab_maping:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]+tag]\n",
        "        res.append(ar_leter)\n",
        "      else:\n",
        "        ar_leter=heb_arab_maping[heb_str[i]]\n",
        "        res.append(ar_leter)\n",
        "        res.append(tag)\n",
        "      next(iterator, None)\n",
        "    else:      \n",
        "      ar_leter=heb_arab_maping[heb_str[i]]\n",
        "      res.append(ar_leter)\n",
        "  return \"\".join(res)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjUhtjL2-31",
        "colab_type": "text"
      },
      "source": [
        "##preprocess sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqPPcio2_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    s = s.rstrip().strip()#.translate(str.maketrans('', '', \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"))\n",
        "    return ''.join(c for c in unicodedata.normalize('NFC', s))\n",
        "        #if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_arab_nikud(s):\n",
        "  return ''.join(c for c in  s  if c not in arab_nikud)\n",
        "\n",
        "def standard_nunization(s):\n",
        "  return s.replace(\"ًا\",\"اً\")\n",
        "\n",
        "arr=\"بأفراد كانوا لباباً \"\n",
        "assert(standard_nunization(\"بيتًا\")==\"بيتاً\")\n",
        "\n",
        "\n",
        "def preprocess_hebrew(w):\n",
        "    w = unicode_to_ascii(w.strip())    \n",
        "    w = w.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ')\n",
        "    w = w.replace('ֿ',\"'\")\n",
        "    return w\n",
        "\n",
        "def double_hebrew(w):    \n",
        "    res=\"\"\n",
        "    for i in w:\n",
        "      res+=i\n",
        "      if not i==\" \":  ##THIS 2 LINES IS THE CHANGE THAT WAS ADDED AT THE LAST MINUTE \n",
        "        res+=i    \n",
        "    return res\n",
        "\n",
        "#    Takes a file of <heb, arab> phrases separated by tab\n",
        "#    Return phares pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(lines,num_examples=200000000):\n",
        "    word_pairs=[]\n",
        "    for l in lines[:num_examples]:\n",
        "      splited=l.split('\\t')\n",
        "      heb=splited[0]\n",
        "      heb=unicode_to_ascii(heb)\n",
        "      heb=preprocess_hebrew(heb)\n",
        "      heb=double_hebrew(heb)\n",
        "      heb=clear_blank(clear_arab_punctuation(heb))\n",
        "      arr=splited[1]\n",
        "      arr=remove_arab_nikud(arr)\n",
        "      arr=standard_nunization(arr)\n",
        "      arr=clear_blank(clear_arab_punctuation(arr))\n",
        "      arr=unicode_to_ascii(arr) \n",
        "      word_pairs.append([heb,arr])        \n",
        "    return word_pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWRQ3jGAesuv",
        "colab_type": "text"
      },
      "source": [
        "small example for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrBodSVerlM",
        "colab_type": "code",
        "outputId": "3bf327e8-1dea-4ea3-ad58-120bfd7c3894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "this_string='وأهل الأديان ثمّ على'\n",
        "this_string='سُئِلْتُ عمّا عنديَ من الاحتجاج'\n",
        "#this_string='كان عند مَلِك الخَزَرِ الداخل'\n",
        "print(len(this_string))\n",
        "this_string=unicode_to_ascii(remove_arab_nikud(this_string)) #new!!!!\n",
        "\n",
        "print(len(this_string))\n",
        "this_string"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'سئلت عمّا عندي من الاحتجاج'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftz4BjtfDnq",
        "colab_type": "text"
      },
      "source": [
        "languageIndex class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmOvLIryMPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ALTERNATIVE language index\n",
        "\n",
        "# This class creates a char -> index mapping (e.g,. \"d\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"d\") for each language,\n",
        "\n",
        "#this class takes a corpus of lines (lang) and extract the vocab\n",
        "# (letters and signs), stores the corpus and the vocab (with revers map)\n",
        "# it also addes the BLANK symbol to the vocab. (makes sure that BLANK is not in the corpus)\n",
        "BLANK=\"_\"\n",
        "class LanguageIndex():\n",
        "  def __init__(self, allowed_letters):\n",
        "    self.allowed_letters = allowed_letters\n",
        "    self.char2idx = {}\n",
        "    self.idx2char = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for c in self.allowed_letters:\n",
        "      #for c in phrase:     #for the meantime don't habdle the diatrics in- hebrew (the tag) and hope the ctc will handle...wishfully\n",
        "        self.vocab.update(c)\n",
        "      #for c in additional_letters:\n",
        "      #  self.vocab.update(c)\n",
        "    self.vocab = sorted(self.vocab)\n",
        "    print(\"vocab: \",self.vocab)   # maps id (i.e. map index) to char\n",
        "    \n",
        "    \n",
        "    for index, char in enumerate(self.vocab): #reverse map: char to id\n",
        "      self.char2idx[char] = index\n",
        "    print(\"len(self.vocab)\",len(self.vocab))\n",
        "    assert(BLANK not in self.char2idx)\n",
        "    self.char2idx[BLANK] = len(self.vocab)   #add BLANK to reverse map\n",
        "    print(\"len(self.char2idx)\",len(self.char2idx)) #should print successor of privous print\n",
        "    print(self.char2idx[BLANK],BLANK)\n",
        "    \n",
        "    \n",
        "    for char, index in self.char2idx.items():  #this is a map equal to the array vocab, but with BLANK\n",
        "      self.idx2char[index] = char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LaMY0igcWT",
        "colab_type": "text"
      },
      "source": [
        "load_dataset method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kFSbZTPfe0m",
        "colab_type": "code",
        "outputId": "8e8cbc35-f6f9-4ed9-bb5c-b011f780ba9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "#inp_lang = LanguageIndex(heb_by_order)\n",
        "inp_lang = LanguageIndex(\"\".join(heb_arab_maping.keys()))\n",
        "#targ_lang = LanguageIndex(arab_letters+u\"\\u0651\"+\"\".join(tanween)+ u\"\\u0621\")# shadda and hamza on line  (arab_nikud)\n",
        "targ_lang = LanguageIndex(\"\".join(arab_heb_maping.keys())+\"\".join(tanween)+ shada + hamza_on_line)\n",
        "\n",
        "\n",
        "def load_dataset(pairs):\n",
        "    # creating cleaned input, output pairs    \n",
        "   # pairs = create_dataset(lines,num_examples)  \n",
        "  \n",
        "    input_tensor = [vectorize(heb,inp_lang.char2idx) for heb, arr in pairs]\n",
        "    input_lenghts=[len(heb) for heb,arr in pairs]\n",
        "    # English sentences\n",
        "    target_tensor = [vectorize(arr,targ_lang.char2idx) for heb, arr in pairs]\n",
        "    target_lengths = [len(arr)  for heb,arr in pairs]\n",
        "    print()\n",
        "    print(LTRchar,pairs[0])\n",
        "    print(input_lenghts[0])\n",
        "    print(target_lengths[0])\n",
        "    print(input_tensor[0])\n",
        "    print(target_tensor[0])\n",
        "\n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    max=max_length(input_tensor)\n",
        "    if max>70:\n",
        "      max=70\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "                                                                 maxlen=max,\n",
        "                                                                 padding='post',\n",
        "                                                                  value=inp_lang.char2idx[BLANK])\n",
        "    max=max_length(target_tensor)\n",
        "    if max>70:\n",
        "      max=70    \n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max,\n",
        "                                                                  padding='post',\n",
        "                                                                  value=targ_lang.char2idx[BLANK])\n",
        "    \n",
        "    print()\n",
        "    print(LTRchar,pairs[0])\n",
        "    print(input_lenghts[0])\n",
        "    print(target_lengths[0])\n",
        "    print(input_tensor[0])\n",
        "    print(len(input_tensor[0]))\n",
        "    print(target_tensor[0])\n",
        "    print(len(target_tensor[0]))\n",
        "\n",
        "    return input_tensor, target_tensor ,input_lenghts,target_lengths"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab:  [' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'H', '[', ']', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'כ', 'ל', 'מ', 'נ', 'ס', 'ע', 'פ', 'צ', 'ק', 'ר', 'ש', 'ת']\n",
            "len(self.vocab) 47\n",
            "len(self.char2idx) 48\n",
            "47 _\n",
            "vocab:  [' ', '!', '\"', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'H', '[', ']', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ً', 'ٌ', 'ٍ', 'ّ']\n",
            "len(self.vocab) 64\n",
            "len(self.char2idx) 65\n",
            "64 _\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GlQJhoggxw3",
        "colab_type": "text"
      },
      "source": [
        "call load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofru0hLdgwVc",
        "colab_type": "code",
        "outputId": "32abb925-b82a-49e5-ec3a-3f534a7f58c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "lines=load_lines()\n",
        "#num_examples = 2000000000  #don't limit\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor, target_tensor ,input_lenghts,target_lengths = load_dataset(pairs)\n",
        "lines=load_lines(haemunot)\n",
        "pairs = create_dataset(lines)  \n",
        "input_tensor1, target_tensor1 ,input_lenghts1,target_lengths1 = load_dataset(pairs)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-176a35a4ad42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#num_examples = 2000000000  #don't limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0minput_lenghts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhaemunot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0d244b416517>\u001b[0m in \u001b[0;36mload_lines\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhakuzari\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ֿ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m##I ADDED THIS. than did the replacement in the file uploaded to drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/My Drive/thesis-data/for_ctc_train22_FRIDBERG3.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWENYC4QPozE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# lines=load_lines(kfir_kuzari_test)\n",
        "# pairs = create_dataset(lines)  \n",
        "# input_tensor3, target_tensor3 ,input_lenghts3,target_lengths3 = load_dataset(pairs)\n",
        "\n",
        "# lines=load_lines(kfir_rasag_test)\n",
        "# pairs = create_dataset(lines)  \n",
        "# input_tensor4, target_tensor4 ,input_lenghts4,target_lengths4 = load_dataset(pairs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkyeHLzi_sN",
        "colab_type": "text"
      },
      "source": [
        "generate the data tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ovSfOPxoy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_data(input_tensor, target_tensor,input_lenghts,target_lengths,_test_size=0.2):\n",
        "  if _test_size<1:\n",
        "    input_tensor_train, input_tensor_val, \\\n",
        "    target_tensor_train, target_tensor_val, \\\n",
        "    input_lengths_train, input_lengths_val, \\\n",
        "    target_lengths_train, target_lengths_val = train_test_split(input_tensor,\n",
        "                                                                target_tensor,\n",
        "                                                                input_lenghts,\n",
        "                                                                target_lengths, test_size=_test_size)\n",
        "    print(len(input_tensor_train), \n",
        "          len(target_tensor_train), \n",
        "          len(input_tensor_val), \n",
        "          len(target_tensor_val))\n",
        "    \n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    #N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "    #vocab_inp_size = len(inp_lang.char2idx)\n",
        "    #vocab_tar_size = len(targ_lang.char2idx)\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, \n",
        "                                                  target_tensor_train,\n",
        "                                                  input_lengths_train,\n",
        "                                                  target_lengths_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, \n",
        "                                                      target_tensor_val,\n",
        "                                                      input_lengths_val,\n",
        "                                                      target_lengths_val)).shuffle(BUFFER_SIZE)                                                  \n",
        "    \n",
        "    dataset_double=dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  else:\n",
        "    print(len(input_tensor), \n",
        "          len(target_tensor))\n",
        "    \n",
        "    \n",
        "    BUFFER_SIZE = len(input_tensor)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor,\n",
        "                                                       target_tensor,\n",
        "                                                       input_lenghts,\n",
        "                                                       target_lengths)).shuffle(BUFFER_SIZE)                                                  \n",
        "    dataset_double=0\n",
        "  test_dataset_double=test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return dataset_double,test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0elYUMb8Ls",
        "colab_type": "text"
      },
      "source": [
        "##activate gen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DklcYhg_b6Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ACTIVATE\n",
        "dataset_double,test_dataset_double=gen_data(input_tensor, target_tensor,input_lenghts,target_lengths)\n",
        "view_data(test_dataset_double)\n",
        "dataset_double1,test_dataset_double1=gen_data(input_tensor1, target_tensor1,input_lenghts1,target_lengths1)\n",
        "view_data(test_dataset_double1)\n",
        "\n",
        "# _,test_dataset_double3=gen_data(input_tensor3, target_tensor3,input_lenghts3,target_lengths3,_test_size=1)\n",
        "# view_data(test_dataset_double3)\n",
        "# _,test_dataset_double4=gen_data(input_tensor4, target_tensor4,input_lenghts4,target_lengths4,_test_size=1)\n",
        "# view_data(test_dataset_double4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMQxNPpy_MXA",
        "colab_type": "text"
      },
      "source": [
        "##Shuffled test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjB8kq4zAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shuffled_word_pairs(test_dataset):\n",
        "  \n",
        "  val_inputs_list=[]\n",
        "  val_outputs_list=[]\n",
        "\n",
        "  for i,j,l1,l2 in test_dataset:\n",
        "    for tt in range(BATCH_SIZE):\n",
        "     # print(i[tt],j[tt])\n",
        "      i_prediction=print_by_idx_CTC(tf.constant(i[tt]),inp_lang.idx2char,l1[tt])\n",
        "      j_prediction=print_by_idx_CTC(tf.constant(j[tt]),targ_lang.idx2char,l2[tt])      \n",
        "      if (len(i_prediction.split())==len(j_prediction.split())):\n",
        "        val_inputs_list+=i_prediction.split()\n",
        "        val_outputs_list+=j_prediction.split()\n",
        "    \n",
        "  print(\"len(val_inputs_list),len(val_outputs_list)\",len(val_inputs_list),len(val_outputs_list)) #10836 10836\n",
        "\n",
        "  word_pairs=list(zip(val_inputs_list,val_outputs_list))\n",
        "  print(\"BEFORE SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print(i)\n",
        "  random.shuffle(word_pairs)\n",
        "  print(\"AFTER SHUFFLE\")\n",
        "  for i in word_pairs[:5]:\n",
        "    print(i)\n",
        "  return word_pairs\n",
        "\n",
        "#TESTING\n",
        "#word_pairs1=get_shuffled_word_pairs(test_dataset_double1.take(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXrW_yjRZS_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shuffled_data(word_pairs):\n",
        "\n",
        "  accum=0\n",
        "  heb_acum=\"\"\n",
        "  arab_acum=\"\"\n",
        "  results_line=[]\n",
        "  for i,j in word_pairs:\n",
        "    if accum>19:\n",
        "      results_line.append(un_double_letters(heb_acum)+'\\t'+arab_acum)\n",
        "      assert(len(i)%2==0)\n",
        "      accum=len(i)/2\n",
        "      heb_acum=i\n",
        "      arab_acum=j\n",
        "    else:\n",
        "      heb_acum+=\" \"+i\n",
        "      arab_acum+=\" \"+j \n",
        "      assert(len(i)%2==0)\n",
        "      accum += len(i)/2 + 1;\n",
        "  results_line.append(heb_acum+'\\t'+arab_acum)  #needed?\n",
        "\n",
        "  print(\"len(results_line)\",len(results_line)) # 2175 before was: 2185 lines \n",
        "\n",
        "\n",
        "  input_tensor_shuffle, target_tensor_shuffle \\\n",
        "  ,input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "  print(\"len(input_tensor_shuffle), len(input_lenghts_shuffle)\",len(input_tensor_shuffle), len(input_lenghts_shuffle))\n",
        "  print(\"len(target_tensor_shuffle),  len(target_lengths_shuffle)\",len(target_tensor_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "  BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "  shuffle_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "                                                            target_tensor_shuffle,\n",
        "                                                            input_lenghts_shuffle,\n",
        "                                                            target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "  shuffle_test_dataset_double=shuffle_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  return shuffle_test_dataset_double\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QPdoa03o6gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ACTIVATE\n",
        "word_pairs=get_shuffled_word_pairs(test_dataset_double)\n",
        "shuffle_test_dataset_double=get_shuffled_data(word_pairs)\n",
        "view_data(shuffle_test_dataset_double)\n",
        "\n",
        "\n",
        "word_pairs1=get_shuffled_word_pairs(test_dataset_double1)\n",
        "shuffle_test_dataset_double1=get_shuffled_data(word_pairs1)\n",
        "view_data(shuffle_test_dataset_double1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLoKpwg9rJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Single words test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSr5HALl9qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_line=[]\n",
        "for i,j in word_pairs:\n",
        "    results_line.append(un_double_letters(i)+'\\t'+j)\n",
        "\n",
        "print(\"len(results_line)\",len(results_line)) \n",
        "\n",
        "\n",
        "input_tensor_shuffle, target_tensor_shuffle \\\n",
        ", input_lenghts_shuffle,target_lengths_shuffle = load_dataset(create_dataset(results_line))\n",
        "\n",
        "print(\"len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle)\",len(input_tensor_shuffle), len(target_tensor_shuffle), len(input_lenghts_shuffle), len(target_lengths_shuffle))\n",
        "\n",
        "BUFFER_SIZE = len(target_lengths_shuffle)\n",
        "\n",
        "single_words_test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_shuffle,\n",
        "                                                                target_tensor_shuffle,\n",
        "                                                                input_lenghts_shuffle,\n",
        "                                                                target_lengths_shuffle)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "single_words_test_dataset=single_words_test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "view_data(single_words_test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_4H-rpFyk8",
        "colab_type": "text"
      },
      "source": [
        "##SYNTHETIC DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOenDCDtDfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO edit when time permits\n",
        "\n",
        "def load_text_for_synth(ibnsina=\"/gdrive/My Drive/JUDEO-ARAB/ibn_sina_ilhyat.txt\"):\n",
        "  with open(ibnsina, 'rb') as f:\n",
        "      ibnsina_text = f.read().decode(encoding='utf-8')\n",
        "\n",
        "\n",
        "  ibnsina_text=clear_arab_punctuation(ibnsina_text)\n",
        "  #TODO somthing with new line - this indicates new content!!!\n",
        "\n",
        "  #add space before and after punctuation signs\n",
        "  import re\n",
        "  ibnsina_text = re.sub(r\"([:;?.!,¿])\", r\" \\1 \", ibnsina_text)\n",
        "  ibnsina_text = re.sub(r'[\" \"]+', \" \", ibnsina_text)    \n",
        "  \n",
        "  return ibnsina_text\n",
        "\n",
        "\n",
        "#ACTIVATE\n",
        "ibnsina_text=load_text_for_synth()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBvUvPHKLCIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ibnsina_text[:250]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBvz3vMxLIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STATSTICS OF SYNTH TEXT\n",
        "sina_vocab=sorted(set(ibnsina_text))\n",
        "\n",
        "print(\"NOT IN LETTER LIST:\")\n",
        "for c in sina_vocab:\n",
        "   if c not in targ_lang.char2idx:\n",
        "      print(\"(\",c,\")\")\n",
        "\n",
        "print(\"\\nLETTER COUNTS\")\n",
        "for i in range(len(sina_vocab)):\n",
        "  print(LTRchar,i,'\"',sina_vocab[i],'\"',ibnsina_text.count(sina_vocab[i])) #64 is shadda   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM7xIKZBLOd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\" \\r\\n \") #TODO rethink this\n",
        "#sina_words=ibnsina_text.split(\" \")\n",
        "#ibnsina_text=ibnsina_text.replace(\"\\r\\n\\r\\n\",\"\\r\\n\").replace(\"\\r\\n\",\". \") #TODO rethink this\n",
        "sina_words=ibnsina_text.split() #for removing also newlines\n",
        "print(ibnsina_text[:100])\n",
        "sina_words[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kHwSPQLfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_synth_sentences(sina_words):\n",
        "\n",
        "  #SENTENCE_LIMIT=random.randint(1,50) #this didn't work. try random with range1,10\n",
        "  SENTENCE_LIMIT=20\n",
        "  sentences=[]\n",
        "  char_count=0\n",
        "  res=[]\n",
        "  for w in sina_words:\n",
        "  #  w=w.rstrip(\" \").strip(\" \")\n",
        "    char_count+=len(w)+1 #len of word + space afterwards\n",
        "    res.append(w)\n",
        "    if char_count>SENTENCE_LIMIT:    \n",
        "      sentences.append(unicode_to_ascii(remove_arab_nikud(\" \".join(res))))\n",
        "      res=[]\n",
        "      char_count=0\n",
        "    #  SENTENCE_LIMIT=random.randint(1,50)          \n",
        "  return sentences\n",
        "\n",
        "#ACTIVATE\n",
        "sentences=gen_synth_sentences(sina_words)\n",
        "len(sentences)\n",
        "sentences[:5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMt2VQPrMtIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# arab_setences=[]\n",
        "# heb_sentences=[]\n",
        "\n",
        "def gen_dropout(sentences,keep_prob=0.90):\n",
        "  arab_setences=[]\n",
        "  heb_sentences=[]\n",
        "  for arr in sentences:\n",
        "    arab_setences.append(arr)        \n",
        "    heb=[]\n",
        "    for c in arr:\n",
        "      if c in arab_heb_maping:\n",
        "        if (np.random.binomial(1,keep_prob)) or c==\" \":\n",
        "          heb.append(arab_heb_maping[c])\n",
        "        else:\n",
        "          heb.append(BLANK)\n",
        "\n",
        "    #print(\"\".join(heb))\n",
        "    heb_sentences.append(double_hebrew(preprocess_hebrew(\"\".join(heb))))\n",
        "\n",
        "  print(heb_sentences[:5]) \n",
        "  print(arab_setences[:5])\n",
        "\n",
        "  print(len(arab_setences))\n",
        "  input_tensor_synth, target_tensor_synth, \\\n",
        "  input_lenghts_synth,target_lengths_synth = load_dataset(list(zip(heb_sentences,arab_setences)))\n",
        "  \n",
        "  # Show length\n",
        "  print(len(input_tensor_synth), len(target_tensor_synth))\n",
        "  print(len(input_lenghts_synth), len(target_lengths_synth))\n",
        "\n",
        "  BUFFER_SIZE = len(input_tensor_synth)\n",
        "\n",
        "  dataset_synth = tf.data.Dataset.from_tensor_slices((input_tensor_synth,\n",
        "                                                      target_tensor_synth,\n",
        "                                                      input_lenghts_synth,\n",
        "                                                      target_lengths_synth)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "  dataset_double_synt=dataset_synth.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  #dataset_double_synt=dataset_double_synt.concatenate(dataset_double).shuffle(BUFFER_SIZE)\n",
        "  return dataset_double_synt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8r6AS2jMtQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # def remove_chars_not_in_dict(s,dict):\n",
        "# #   #print(s)\n",
        "# #   res=''\n",
        "# #   for c in s:\n",
        "# #     if c in dict:\n",
        "# #       res+=c\n",
        "# #   return res\n",
        "\n",
        "# def print_show(i):\n",
        "#     print(len(heb_sentences),len(arab_setences))\n",
        "#     print(heb_sentences[i],arab_setences[i])\n",
        "#     print(input_lenghts[i])\n",
        "#     print(target_lengths[i])\n",
        "#     print(input_tensor[i])\n",
        "#     print(target_tensor[i])\n",
        "\n",
        "\n",
        "# def load_dataset_synth(arab_setences,heb_sentences):\n",
        "#     # creating cleaned input, output pairs    \n",
        "#     print(heb_sentences[0])\n",
        "#     input_tensor = [vectorize(heb,inp_lang.char2idx) for heb in heb_sentences]\n",
        "#     print(input_tensor[0])\n",
        "#     input_lenghts=[len(heb) for heb in heb_sentences]\n",
        "  \n",
        "#     target_tensor = [vectorize(arr,targ_lang.char2idx) for arr in arab_setences]\n",
        "#     target_lengths = [len(arr)  for  arr in arab_setences]\n",
        "#     print(len(arab_setences))\n",
        "#     print(len(target_tensor))\n",
        "\n",
        "#     # Padding the input and output tensor to the maximum length\n",
        "#     input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
        "#                                                                  maxlen=max_length(input_tensor),\n",
        "#                                                                  padding='post',\n",
        "#                                                                   value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "#     target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "#                                                                   maxlen=max_length(target_tensor), \n",
        "#                                                                   padding='post',\n",
        "#                                                                   value=targ_lang.char2idx[BLANK])\n",
        "#     print(len(target_tensor))\n",
        "#    # print_show(0)\n",
        "\n",
        "#     return input_tensor, target_tensor , input_lenghts,target_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcBdNP-Ms-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_dropout_all(keep=1):\n",
        "  synth_text=load_text_for_synth()\n",
        "  sentences=gen_synth_sentences(synth_text.split())\n",
        "  dataset_double_synt=gen_dropout(sentences,keep)\n",
        "\n",
        "  path1=\"/gdrive/My Drive/JUDEO-ARAB/daruri-IR.txt\"\n",
        "  synth_text1=load_text_for_synth(path1)\n",
        "  sentences1=gen_synth_sentences(synth_text1.split())\n",
        "  dataset_double_synt1=gen_dropout(sentences1,keep).concatenate(dataset_double_synt)\n",
        "\n",
        "  path2=\"/gdrive/My Drive/JUDEO-ARAB/farabi-tahsil.txt\"\n",
        "  synth_text2=load_text_for_synth(path2)\n",
        "  sentences2=gen_synth_sentences(synth_text2.split())\n",
        "  dataset_double_synt2=gen_dropout(sentences2,keep).concatenate(dataset_double_synt1)\n",
        "\n",
        "\n",
        "  path3=\"/gdrive/My Drive/JUDEO-ARAB/huruf.txt\"\n",
        "  synth_text3=load_text_for_synth(path3)\n",
        "  sentences3=gen_synth_sentences(synth_text3.split())\n",
        "  dataset_double_synt3=gen_dropout(sentences2,keep).concatenate(dataset_double_synt2)\n",
        "\n",
        "\n",
        "  return dataset_double_synt3.concatenate(dataset_double).shuffle(BUFFER_SIZE)\n",
        "view_data(gen_dropout_all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2BCYUxIVv4",
        "colab_type": "text"
      },
      "source": [
        "#MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLpB4zfAY7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NETWORK PARAMS\n",
        "# The embedding dimension\n",
        "embedding_dim = 8 #256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 #1024\n",
        "\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  #rnn = tf.keras.layers.CuDNNGRU\n",
        "  rnn=tf.compat.v1.keras.layers.CuDNNGRU\n",
        "  #rnn = tf.keras.layers.LSTM #see https://stackoverflow.com/questions/55761337/module-tensorflow-python-keras-api-v2-keras-layers-has-no-attribute-cudnnlst\n",
        "\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
        "  \n",
        "#FUNCTION TO BUILD MODEL\n",
        "def build_model(vocab_size_heb1,vocab_size_ar, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size_heb1, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "   tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation. this improve the results because the net can predict end of word, and know when to put OTIOT SOFIOT \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    \n",
        "  \n",
        "\n",
        "    tf.keras.layers.Bidirectional(rnn(rnn_units,     #I added the bidirectianl myself on top of https://www.tensorflow.org/tutorials/sequences/text_generation\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True)),\n",
        "    tf.keras.layers.Dense(vocab_size_ar\n",
        "                         )\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DAfj8zQGhKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rebuild():\n",
        "  #BUILD MODEL\n",
        "  model = build_model(\n",
        "    vocab_size_ar = len(targ_lang.char2idx),\n",
        "    vocab_size_heb1 = len(inp_lang.char2idx),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "model=rebuild()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK87dh5brMUG",
        "colab_type": "text"
      },
      "source": [
        "#TESTING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWhnKORfyw8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from termcolor import colored\n",
        "import difflib\n",
        "\n",
        "def show_diff(t1,t2,col):\n",
        "    \"\"\"Unify operations between two compared strings\n",
        "seqm is a difflib.SequenceMatcher instance whose a & b are strings\"\"\"\n",
        "    seqm= difflib.SequenceMatcher(None,t1,t2)   \n",
        "    output1=[]\n",
        "    output2= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        \n",
        "        if opcode == 'equal':            \n",
        "            output1.append(seqm.a[a0:a1])\n",
        "            output2.append(seqm.b[b0:b1])\n",
        "        elif opcode == 'insert':            \n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        elif opcode == 'delete':\n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "        elif opcode == 'replace':            \n",
        "            output1.append(colored(seqm.a[a0:a1],col,attrs=['bold']))\n",
        "            output2.append(colored(seqm.b[b0:b1],col,attrs=['bold']))\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output1),''.join(output2)\n",
        "\n",
        "#USEAGE:\n",
        "s1=\"لامة النصارى واستحقو\" \n",
        "s2=\"لأمّة النصارى واستحقوّا\"\n",
        "a,b=show_diff(s1,s2,'blue')\n",
        "#print(\"\".join(b))\n",
        "print(a,\"|\",b)\n",
        "\n",
        "#print('\\x1b[31mّ\\x1b[0m')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13p5-wog3JMo",
        "colab_type": "text"
      },
      "source": [
        "## forward run each letter seperatly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwSqhhfqy6Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test__CTC_letters(): \n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "\n",
        "  all_heb_letters=inp_lang.vocab\n",
        "  \n",
        "  num_of_letters=len(inp_lang.vocab)\n",
        "  num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#  print(num_of_letters)\n",
        "  letters_as_int=[]\n",
        "  tag=inp_lang.char2idx[\"'\"]\n",
        "  for t in range(num_of_letters):\n",
        "    letters_as_int.append([t]*2)\n",
        "  for t in range(BATCH_SIZE-num_of_letters):\n",
        "    letters_as_int.append([0,0])    \n",
        "\n",
        "  letters_tensor=tf.convert_to_tensor(letters_as_int)\n",
        "  \n",
        "  predict_ltrs=model(letters_tensor)\n",
        "  \n",
        "  \n",
        "  for jj in range(num_of_letters):\n",
        "      print(\"candidate:***({0})***\".format(inp_lang.vocab[jj]))\n",
        "      \n",
        "      pred_distr=predict_ltrs[jj][1]\n",
        "      pred_distr=tf.nn.softmax(pred_distr)\n",
        "      for ii in range(num_of_arab_letters):\n",
        "       print(\"{0:.3f}({1})  \".format(pred_distr[ii],targ_lang.vocab[ii]),end = '')\n",
        "      print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "      maximum=tf.argmax(pred_distr).numpy()\n",
        "      max_score=tf.math.reduce_max(pred_distr).numpy()\n",
        "      if (maximum<num_of_arab_letters):\n",
        "        print(\"prediction***({0})***{1:.3f}\".format(targ_lang.vocab[maximum],max_score))\n",
        "      else:\n",
        "        print(\"####max is the blank symbole\")\n",
        "      print('-'*10)\n",
        "  \n",
        "#test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9vM09mq0ZSF",
        "colab_type": "text"
      },
      "source": [
        "##forward run a sentence (with top n beam search results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM2_GVYmOwdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    l = double_hebrew(l)\n",
        "    v=vectorize(l,inp_lang.char2idx)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "#  inp=[] \n",
        "#  double=[]\n",
        "  #num_of_letters=0\n",
        "  #for c in word_str:\n",
        "  #  double.append(inp_lang.char2idx[c])\n",
        "  #  num_of_letters+=1\n",
        "  #  if not c==' ':\n",
        "  #    double.append(inp_lang.char2idx[c])\n",
        "  #    num_of_letters+=1\n",
        "  #inp.append(double)\n",
        "  #inp.append([inp_lang.char2idx[c] for c in word_str])\n",
        "  #for i in range(_BATCH_SIZE-1):\n",
        "  #  inp.append([0]*num_of_letters)\n",
        "  \n",
        " # num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "  #print(num_of_letters)\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  #print(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "  #inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  # if print_deteils:\n",
        "  #   print('#####deteils')\n",
        "  # for jj in range(num_of_letters):\n",
        "  #     if print_deteils:\n",
        "  #       print(word_str[jj])\n",
        "      \n",
        "  #     pred_distr=predict_ltrs[0][jj]\n",
        "  #     pred_distr=tf.nn.softmax(pred_distr)\n",
        "  #     if print_deteils:\n",
        "  #       for ii in range(num_of_arab_letters):\n",
        "  #         print(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "  #       print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "  #       print()\n",
        "  #     maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "  #     if print_deteils:\n",
        "  #       print(maximum)\n",
        "  #     if (maximum<42):\n",
        "  #       if print_deteils:\n",
        "  #         print(targ_lang.vocab[maximum])\n",
        "  #       res.append(targ_lang.vocab[maximum])\n",
        "  #     else:\n",
        "  #       if print_deteils:\n",
        "  #         print(\"####max is the blank symbole\")\n",
        "  #       res.append(\"-\")\n",
        "  #     if print_deteils:\n",
        "  #       print('-'*10)\n",
        "  # print(word_str)\n",
        "  #print(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "  total_res=\"\"\n",
        "  for t in range(num_of_lines):\n",
        "    print(lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=print_by_idx_CTC(dense[t],targ_lang.idx2char)\n",
        "      print(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "    total_res+='\\n'+prediction\n",
        "  return total_res\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''כמאלא\n",
        "כמאלא'''\n",
        "\n",
        "print(test__CTC_word_multiline(lines,3,BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K6i5QWGO4DS",
        "colab_type": "text"
      },
      "source": [
        "###test kfir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqzEV2rtpgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "#TODO edit code when time permits\n",
        "\n",
        "def test__CTC_word_multiline_kfir(word_str,targ_str,num_of_paths=1,_BATCH_SIZE=BATCH_SIZE,SHOW_PRINT=False): \n",
        "  lines=word_str.split('\\n')\n",
        "  targ_lines=targ_str.split('\\n')\n",
        "  num_of_lines=len(lines)\n",
        "  assert(num_of_lines==len(targ_lines))\n",
        "  assert(num_of_lines<=BATCH_SIZE)\n",
        "\n",
        "  inputs=[]\n",
        "  inputs_len=[]\n",
        "  for l in lines:\n",
        "    l = preprocess_hebrew(l)\n",
        "    #l = double_hebrew(l)\n",
        "    v=vectorize(l,inp_lang.char2idx)\n",
        "    inputs.append(v)\n",
        "    inputs_len.append(len(v))\n",
        "  #max_len=max_len(inputs)\n",
        "  #PADD HORIZANTALY\n",
        "  for i in range(BATCH_SIZE-num_of_lines):\n",
        "    inputs.append([0])\n",
        "    inputs_len.append(1)\n",
        "\n",
        "\n",
        "\n",
        "  res=[]\n",
        "#  inp=[] \n",
        "#  double=[]\n",
        "  #num_of_letters=0\n",
        "  #for c in word_str:\n",
        "  #  double.append(inp_lang.char2idx[c])\n",
        "  #  num_of_letters+=1\n",
        "  #  if not c==' ':\n",
        "  #    double.append(inp_lang.char2idx[c])\n",
        "  #    num_of_letters+=1\n",
        "  #inp.append(double)\n",
        "  #inp.append([inp_lang.char2idx[c] for c in word_str])\n",
        "  #for i in range(_BATCH_SIZE-1):\n",
        "  #  inp.append([0]*num_of_letters)\n",
        "  \n",
        " # num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "  #print(num_of_letters)\n",
        "\n",
        "  #PADDDDDD\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
        "                                                         maxlen=max_length(inputs),\n",
        "                                                         padding='post',\n",
        "                                                         value=inp_lang.char2idx[BLANK])\n",
        "    \n",
        "  inputs=tf.convert_to_tensor(inputs)\n",
        "    \n",
        "  predict_ltrs=model(inputs)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "  \n",
        "  decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      inputs,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "  \n",
        "  total_res=[]\n",
        "  total_edit_dist=0\n",
        "  line_counter=1\n",
        "  for t in range(num_of_lines):\n",
        "    #print(lines[t])\n",
        "    # print(\"real:\",targ_lines[t])\n",
        "    for i in reversed(range(num_of_paths)):\n",
        "      dense=tf.sparse.to_dense(decoded[i])   \n",
        "      prediction=print_by_idx_CTC(dense[t],targ_lang.idx2char).strip() \n",
        "      # print(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "      total_res.append(prediction)\n",
        "    # print(len(prediction))\n",
        "    #print(len(targ_lines[t]))\n",
        "    ed_dist=editdistance.eval(targ_lines[t], prediction)\n",
        "    # print(\"edit_dist:\",ed_dist)\n",
        "    ed_dist/=len(targ_lines[t])\n",
        "    real,prediction=show_diff(targ_lines[t],prediction,'red')\n",
        "    if SHOW_PRINT:\n",
        "      print(\"({0})\".format(line_counter),LTRchar,un_double_letters(lines[t]),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(ed_dist))\n",
        "    line_counter+=1\n",
        "\n",
        "\n",
        "    total_edit_dist+=ed_dist\n",
        "  return total_res,total_edit_dist,num_of_lines\n",
        "  \n",
        "#IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "\n",
        "lines='''יכ'אלף\n",
        "עלי\n",
        "הד'ה\n",
        "אלאמאנה\n",
        "ולא'''\n",
        "\n",
        "lines='''ייככ''אאללףף\n",
        "עעלליי\n",
        "ההדד''הה\n",
        "אאללאאממאאננהה\n",
        "ווללאא'''\n",
        "\n",
        "targ_lines='''يخالف\n",
        "على\n",
        "هذه\n",
        "الأمانة\n",
        "ولا'''\n",
        "\n",
        "print(test__CTC_word_multiline_kfir(lines,targ_lines,1,BATCH_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5e5-59jCxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "def test_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False):\n",
        "\n",
        "  lines=load_lines(data_path)\n",
        "  pairs = create_dataset(lines)  \n",
        "  # #input_tensor4, target_tensor4 ,input_lenghts4,target_lengths4 = load_dataset(pairs)\n",
        "\n",
        "  if replace_GAIN:\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs]\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "\n",
        "  num_of_lines=len(pairs)\n",
        "  index=0\n",
        "  total_num_of_examples=0\n",
        "  total_sum_e_d=0\n",
        "  while index<=num_of_lines:\n",
        "    batch_hebrew=hebrew_lines[index:index+BATCH_SIZE]\n",
        "    batch_arab=arab_lines[index:index+BATCH_SIZE]\n",
        "    _,sum_of_e_d,num_of_examples=test__CTC_word_multiline_kfir('\\n'.join(batch_hebrew),'\\n'.join(batch_arab),1,BATCH_SIZE,SHOW_PRINT)\n",
        "    if SHOW_PRINT:\n",
        "      print(\"BATCH (sum_of_e_d,num_of_examples): \",sum_of_e_d,num_of_examples)\n",
        "    total_num_of_examples+=num_of_examples\n",
        "    total_sum_e_d+=sum_of_e_d\n",
        "    index+=BATCH_SIZE\n",
        "\n",
        "  print(\"total_num_of_examples:total_sum_e_d:ratio\",total_num_of_examples,total_sum_e_d,1-total_sum_e_d/total_num_of_examples)\n",
        "  return total_sum_e_d/total_num_of_examples\n",
        "\n",
        "test_kfir(kfir_kuzari_test,True)\n",
        "#test_kfir(kfir_kuzari_test)\n",
        "test_kfir(kfir_rasag_test,True)\n",
        "\n",
        "#test_kfir(kfir_rasag_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brMFHDJVlwhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr=[1,2,3,4]\n",
        "arr[0:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQTXhvMjads",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl24mg1r2EKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# #TODO edit code when time permits\n",
        "\n",
        "# def test__CTC_word(word_str,num_of_paths=5,_BATCH_SIZE=BATCH_SIZE,print_deteils=False): \n",
        "#   res=[]\n",
        "#   inp=[] \n",
        "#   double=[] \n",
        "#   num_of_letters=0\n",
        "#   for c in word_str:\n",
        "#     double.append(inp_lang.char2idx[c])\n",
        "#     num_of_letters+=1\n",
        "#     if not c==' ':\n",
        "#       double.append(inp_lang.char2idx[c])\n",
        "#       num_of_letters+=1\n",
        "#   inp.append(double)\n",
        "#   #inp.append([inp_lang.char2idx[c] for c in word_str])\n",
        "#   for i in range(_BATCH_SIZE-1):\n",
        "#     inp.append([0]*num_of_letters)\n",
        "  \n",
        "#   num_of_arab_letters=len(targ_lang.vocab)\n",
        "\n",
        "#   #print(num_of_letters)\n",
        "  \n",
        "#   letters_tensor=tf.convert_to_tensor(inp)\n",
        "    \n",
        "#   predict_ltrs=model(letters_tensor)\n",
        "#   #print(predict_ltrs.shape)\n",
        "  \n",
        "  \n",
        "#   inputs_len=[num_of_letters]*_BATCH_SIZE\n",
        "#   inputs=tf.transpose(predict_ltrs,perm=[1,0,2]) #[max_time, batch_size, num_classes]\n",
        "  \n",
        "#   decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "#                       inputs,\n",
        "#                       inputs_len,top_paths=num_of_paths) \n",
        "#   if print_deteils:\n",
        "#     print('#####deteils')\n",
        "#   for jj in range(num_of_letters):\n",
        "#       if print_deteils:\n",
        "#         print(word_str[jj])\n",
        "      \n",
        "#       pred_distr=predict_ltrs[0][jj]\n",
        "#       pred_distr=tf.nn.softmax(pred_distr)\n",
        "#       if print_deteils:\n",
        "#         for ii in range(num_of_arab_letters):\n",
        "#           print(\"{0:.3f}\".format(pred_distr[ii]),targ_lang.vocab[ii])\n",
        "#         print(\"{0:.3f} <blank>\".format(pred_distr[num_of_arab_letters]))\n",
        "#         print()\n",
        "#       maximum=tf.argmax(predict_ltrs[0][jj]).numpy()\n",
        "#       if print_deteils:\n",
        "#         print(maximum)\n",
        "#       if (maximum<42):\n",
        "#         if print_deteils:\n",
        "#           print(targ_lang.vocab[maximum])\n",
        "#         res.append(targ_lang.vocab[maximum])\n",
        "#       else:\n",
        "#         if print_deteils:\n",
        "#           print(\"####max is the blank symbole\")\n",
        "#         res.append(\"-\")\n",
        "#       if print_deteils:\n",
        "#         print('-'*10)\n",
        "#   print(word_str)\n",
        "#   print(\"ARGMAX PREDICTION: \",\"\".join(res))  \n",
        "#   for i in reversed(range(num_of_paths)):\n",
        "#     dense=tf.sparse.to_dense(decoded[i])   \n",
        "#     prediction=print_by_idx_CTC(dense[0],targ_lang.idx2char)\n",
        "#     print(\"#####PREDICTION\"+str(i)+\": \",prediction)\n",
        "#   return prediction\n",
        "  \n",
        "# #IMPORTED OBSERV: it seems that ctc decodes make much more than just argmax over sequences see https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7\n",
        "# ###  ا-ل-ط-ب-ي-ع-ي-ة  و-ك-ا-ل-اا   -م---ع-ع-ا-ا   ---> الطبيعية كمالاً مستعدّاً\n",
        "\n",
        "# #test__CTC_word(\"כמאלא\",3,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjJv0g7qoF5L",
        "colab_type": "text"
      },
      "source": [
        "##test baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZGDfiE4brU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "def test_loss_baseline(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "\t\t\t\n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0:\n",
        "                    continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "          total_examples+=BATCH_SIZE\n",
        "\t\t\t\t\t\t\t \n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print(\"LER (label error rate): \",total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy\n",
        "#test_loss_baseline(limit=3)\n",
        "test_loss_baseline(test_dataset_double)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWMVLTywwIW",
        "colab_type": "text"
      },
      "source": [
        "##TEST BASELINE KFIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrcOFoswu-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "\n",
        "#data_path options:\n",
        "# kfir_kuzari_test\n",
        "# kfir_rasag_test\n",
        "\n",
        "#replace_GAIN - replace JAIN with GIMEL\n",
        "\n",
        "def test_loss_baseline_kfir(data_path,replace_GAIN=False,SHOW_PRINT=False):\n",
        "  lines=load_lines(data_path) #THIS IS DOUBLED ALLREADY\n",
        "  pairs = create_dataset(lines)  \n",
        "  \n",
        "  if replace_GAIN:\n",
        "    hebrew_lines=[heb.replace(\"גג\",\"גג''\").replace(\"גג''''\",\"גג\") for heb, arr in pairs] #ייננבבגג''יי\n",
        "  else:\n",
        "    hebrew_lines=[heb for heb, arr in pairs]\n",
        "  arab_lines=[arr for heb, arr in pairs]\n",
        "  total_examples=len(pairs)\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  line_counter=1\n",
        "  for heb_input,real in zip(hebrew_lines,arab_lines):\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=simple_letter_map(heb_input)                \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                if SHOW_PRINT:\n",
        "                  print(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\t\t\t\t\t\t\t   \n",
        "  total_accuracy/=total_examples\n",
        "  print(\"test_loss_baseline_kfir: LER (label error rate): \",1-total_accuracy)\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "  return 0,total_accuracy\n",
        "#test_loss_baseline(limit=3)\n",
        "#test_loss_baseline_kfir(kfir_kuzari_test)\n",
        "test_loss_baseline_kfir(kfir_kuzari_test,True)\n",
        "#test_loss_baseline_kfir(kfir_rasag_test)\n",
        "test_loss_baseline_kfir(kfir_rasag_test,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVLQxjcHkq74",
        "colab_type": "text"
      },
      "source": [
        "##test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pus1Jr4rLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import editdistance\n",
        "def test_loss(this_dataset=test_dataset_double,only_first=True,limit=False):\n",
        "  num_of_paths=1\n",
        "  total_loss=0\n",
        "  total_accuracy=0\n",
        "  total_examples=0\n",
        "  line_counter=1\n",
        "  if limit:\n",
        "    this_dataset=this_dataset.take(limit)\n",
        "  for input_example_batch, target_example_batch, inputs_len,targets_len in this_dataset:\n",
        "          predictions = model(input_example_batch)                 \n",
        "          logits=tf.transpose(predictions,perm=[1,0,2])    \n",
        "          #loss=tf.nn.ctc_loss_v2(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          loss=tf.nn.ctc_loss(target_example_batch,logits, targets_len,inputs_len,blank_index=targ_lang.char2idx[BLANK])\n",
        "          cost = tf.reduce_mean(loss)\n",
        "          total_loss+=cost \n",
        "          \n",
        "          \n",
        "          #decoded, log_probabilities=tf.nn.ctc_beam_search_decoder_v2(\n",
        "          decoded, log_probabilities=tf.nn.ctc_beam_search_decoder(\n",
        "                      logits,\n",
        "                      inputs_len,top_paths=num_of_paths) \n",
        "          dense=tf.sparse.to_dense(decoded[0])\n",
        "            \n",
        "          for i in range(BATCH_SIZE):\n",
        "                heb_input=print_by_idx_CTC(input_example_batch[i],inp_lang.idx2char)\n",
        "\n",
        "                heb_input=un_double_letters(heb_input).strip(BLANK)\n",
        "                prediction=print_by_idx_CTC(dense[i],targ_lang.idx2char).strip()\n",
        "                real=print_by_idx_CTC(target_example_batch[i],targ_lang.idx2char,targets_len[i].numpy()).strip(BLANK)  \n",
        "                accuracy=editdistance.eval(real, prediction)\n",
        "                accuracy/=len(real)\n",
        "                total_accuracy+=accuracy\n",
        "                if only_first and i!=0: \n",
        "                  continue\n",
        "                real,prediction=show_diff(real,prediction,'red')\n",
        "                print(\"({0})\".format(line_counter),LTRchar,heb_input.strip(BLANK),\"|\",real,\"|\",prediction,\"|\",\"{0:.4f}\".format(accuracy))\n",
        "                line_counter+=1\n",
        "\n",
        "          total_examples+=BATCH_SIZE\n",
        "  #total_loss/=total_examples\n",
        "  if total_accuracy!=0:\n",
        "    total_accuracy/=total_examples\n",
        "    print(\"LER (label error rate): \",total_accuracy)\n",
        "#  print(\"total_test loss: \",total_loss.numpy())\n",
        "  return total_loss.numpy(),total_accuracy\n",
        "#test_loss(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcIFcI-ghO2f",
        "colab_type": "text"
      },
      "source": [
        "##test guide perplex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DGjAhEUfxbRc",
        "colab": {}
      },
      "source": [
        "#NOTICE:there's a mix up compared to the arab translitartaion by attai in the 5 6 raw mark here in brackets\n",
        "\n",
        "#THIS IS THE ORIGNAL FROM THE GNIZA WEBSITE\n",
        "guid_text='''כנת איהא אלתלמיד' אלעזיז ר' יוסף ש\"צ ב\"ר \n",
        "יהודה נ\"ע למא מת'לת ענדי וקצדת\n",
        " מן אקאצי אלבלאד ללקראה עלי , עט'ם שאנך ענדי לשדהֿ חרצך עלי \n",
        " אלטלב ולמא ראיתה פי אשעארך מן שדה' אלאשתיאק ללאמור אלנט'ריה וכאן ד'לך מנד' וצלתני רסאילך ומקאמאתך מן\n",
        "אלאסכנדריה קבל אן אמתחן\n",
        "תצורך וקלת לעל שוקה אקוי מן אדראכה פלמא קראת עלי מא קד\n",
        "קראתה מן עלם אלהיאה ומא תקדם לך ממא לא בד מנה תוטיה להא מן אלתעאלים \n",
        "זדת בך גבטה לג'ודה' ד'הנך וסרעה' תצורך וראית שוקך ללתעאלים \n",
        "עט'ימא פתרכתך ללארתיאץ' פיהא לעלמי במאלך.   \n",
        "פלמא קראת עלי מא קד קראתה מן צנאעה' אלמנטק תעלקת אמאלי בך \n",
        "וראיתך אהלא לתכשף לך אסראר אלכתב אלנבויה חתי תטלע מנהא עלי מא ינבגי \n",
        "אן יטלע עליה אלכאמלון פאכ'ד'ת אן אלוח לך תלויחאת ואשיר לך באשאראת\n",
        "פראיתך תטלב מני אלאזדיאד וסמתני אן אבין לך אשיא מן אלאמור'''\n",
        "\n",
        "#AND THIS IS FROM THE SECOND PAGE ON (in attai book)\n",
        "#      אלאלאהיה ואן אכ'ברך בהד'ה מקאצד\n",
        "# אלמתכלמין והל תלך אלטרק ברהאניה ואן לם תכן פמן אי צנאעה הי\n",
        "# וראיתך קד שדות שיא מן ד'לך עלי גירי ואנת חאיר קד בדתך אלדהשה\n",
        "# ונפסך אלשריפה תטאלבך למצא דברי חפץ פלם אזל אדפעך ען ד'לך\n",
        "# ואמרך אן תאכ'ד' אלאשיא עלי תרתיב קצדא מני אן יצח לך אלחק\n",
        "# בטרקה לא אן יקע אליקין באלערץ' ולם אמתנע טאל אג'תמאעך בי אד'א\n",
        "# מא ד'כר פסוק או נץ מן נצוץ אלחכמים פיה תנביה עלי מעני גריב מן\n",
        "# תביין ד'לך לך . פלמא קדר אללה באלאפתראק ותוג'הת אלי חית' תוג'הת\n",
        "# את'ארת מני תלך אלאג'תמאעאת עזימה קד כאנת פתרת וחרכתני גיבתך\n",
        "# לוצ'ע הד'ה אלמקאלה אלתי וצ'עתהא לך ולאמת'אלך וקלילא מא הם\n",
        "# וג'עלתהא פצולא מנת'ורה וכל מא אנכתב מנהא פהו יצלך אולא אולא\n",
        "# חית' כנת ואנת סאלם'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbZb2SwOhSxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# guid_lines=guid_text.split('\\n')\n",
        "\n",
        "# def test_guide(limit=1000000,num_of_paths=1):\n",
        "#   line_res=[]\n",
        "#   count=0\n",
        "#   for l in guid_lines:    \n",
        "#     if count>limit:\n",
        "#       break\n",
        "#     l = l.replace('ם','מ').replace('ן','נ').replace('ץ','צ').replace('ף','פ').replace('ך','כ').replace('\"',\"\").replace('ֿ',\"'\")\n",
        "#     l_res=test__CTC_word(l,num_of_paths=num_of_paths)\n",
        "#     line_res.append(l_res)\n",
        "#     count+=1\n",
        "#   print('\\n'.join(line_res))\n",
        "#   return '\\n'.join(line_res)\n",
        "\n",
        "# print(test_guide(limit=1))\n",
        "# #print(test_guide())\n",
        "\n",
        "def test_guide(limit=1000000,num_of_paths=5):\n",
        "  return test__CTC_word_multiline(guid_text,num_of_paths,BATCH_SIZE)\n",
        "print(test_guide())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jI08UDJhkS9",
        "colab_type": "text"
      },
      "source": [
        "##test shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSpgkEQxhmFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_shuffle(data=shuffle_test_dataset_double,limit=False):\n",
        "  return test_loss(this_dataset=data,limit=limit)\n",
        "#shuffle_loss,shuffle_accuracy=test_shuffle(limit=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeiuwvnOySt",
        "colab_type": "text"
      },
      "source": [
        "#TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCrkXujeClQ",
        "colab_type": "text"
      },
      "source": [
        "##pre-train letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9adhW06Rjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train only non-tag letters with cross_entropy\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "LEN=10\n",
        "\n",
        "def pretrain_letters(EPOCHS=10000,_BATCH_SIZE=BATCH_SIZE):\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss=0\n",
        "    hidden = model.reset_states()  #needed?\n",
        "    for batch_n in range(30):\n",
        "        inp=[]\n",
        "        target=[]\n",
        "        for i in range(_BATCH_SIZE):\n",
        "          #draw hebrew letter with tag or not. translate to ints   ###SHOULD USE THE DICT #arab_heb_maping\n",
        "          heb_res=[]\n",
        "          arab_res=[]\n",
        "          for jj in range(LEN):\n",
        "            choosen_arr=random.choice(list(arab_heb_maping.keys()))            \n",
        "            choosen_heb=arab_heb_maping[choosen_arr]\n",
        "            if len(choosen_heb)==2:\n",
        "              heb_res.append(choosen_heb[0])\n",
        "            else:\n",
        "              heb_res.append(choosen_heb)\n",
        "            arab_res.append(choosen_arr)\n",
        "          # print(heb_res)\n",
        "          # print(arab_res)\n",
        "          heb_choosen_int=[inp_lang.char2idx[cr] for cr in heb_res]\n",
        "          arab_choosen_int=[targ_lang.char2idx[cr] for cr in arab_res]              \n",
        "          inp.append(heb_choosen_int)          \n",
        "          target.append(arab_choosen_int)    \n",
        "\n",
        "        inp=tf.convert_to_tensor(inp)\n",
        "        target=tf.convert_to_tensor(target)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(inp)   \n",
        "            #cost = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            cost = tf.compat.v1.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    template = 'Epoch {} Loss {:.4f}'\n",
        "    #test__CTC_letters()\n",
        "    print(template.format(epoch+1,  total_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMwJyqa6ZA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model=rebuild()\n",
        "# optimizer = tf.train.AdamOptimizer()  #need to reset oprimazer between trains????\n",
        "# pretrain_letters(10,BATCH_SIZE)\n",
        "# test__CTC_letters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaFWkYAG99Du",
        "colab_type": "text"
      },
      "source": [
        "##train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZNb9x9W6bS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBAL_epoch=0\n",
        "\n",
        "#A SINGLE EPOCH\n",
        "def train_loop(cur_dataset=dataset_double,stop_loop=10000000000):\n",
        "  # Training step  \n",
        "  global GLOBAL_epoch\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initially hidden is None\n",
        "  hidden = model.reset_states()\n",
        "  total_loss=0\n",
        "  for (batch_n, (inp, target,input_lens,target_lens)) in enumerate(cur_dataset):\n",
        "        if batch_n>stop_loop:\n",
        "          break\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feeding the hidden state back into the model\n",
        "            # This is the interesting step\n",
        "            predictions = model(inp)                \n",
        "            #labels=tf.cast(target, tf.int32) #need?  \n",
        "            logits=tf.transpose(predictions,perm=[1,0,2])  \n",
        "            #seqs_lens=logit_lens=[101]*BATCH_SIZE  \n",
        "            #loss=tf.nn.ctc_loss_v2(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            loss=tf.nn.ctc_loss(target,logits, target_lens,input_lens,blank_index=targ_lang.char2idx[BLANK])              \n",
        "            \n",
        "            cost = tf.reduce_mean(loss)\n",
        "            total_loss+=cost\n",
        "\n",
        "        grads = tape.gradient(cost, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if batch_n % 10 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "            print(template.format(GLOBAL_epoch+1, batch_n, cost))\n",
        "  GLOBAL_epoch+=1\n",
        "  return total_loss.numpy()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAxxqXvBekE",
        "colab_type": "text"
      },
      "source": [
        "CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGaUtdVBcj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #DEFINE CHECKPOINT CALLBACK\n",
        "\n",
        "# # Directory where the checkpoints will be saved\n",
        "# checkpoint_dir = './training_checkpoints'\n",
        "# # Name of the checkpoint files\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# # checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "# #     filepath=checkpoint_prefix,\n",
        "# #     save_weights_only=True)\n",
        "\n",
        "\n",
        "# 2)\n",
        "#       #       model.save_weights(checkpoint_prefix.format(epoch=epoch)+'synth')\n",
        "# 3)\n",
        "\n",
        "# # print (tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# # model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# # model.build(tf.TensorShape([BATCH_SIZE, None]))\n",
        "\n",
        "\n",
        "# #ref :https://colab.research.google.com/github/tensorflow/models/blob/master/samples/core/tutorials/keras/save_and_restore_models.ipynb#scrollTo=R7W5plyZ-u9X\n",
        "\n",
        "# # Save the weights\n",
        "# model.save_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# # Restore the weights\n",
        "# model = create_model()\n",
        "# model.load_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# loss,acc = model.evaluate(test_images, test_labels)\n",
        "# print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlmUGP0i-uA",
        "colab_type": "text"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPPcVK2jDyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOBAL_epoch=0\n",
        "np.random.seed(1)\n",
        "#tf.set_random_seed(1)\n",
        "#tf.random.set_seed(1)\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "mail_subject=\"pretrain letters. synt DROPOUT 0.85. STANDARD TANWIN\"\n",
        "mail_subject=\"JUST_TESTING\"+mail_subject\n",
        "\n",
        "pretrain_letter=15\n",
        "synth=True\n",
        "keep_percent=0.85\n",
        "\n",
        "description=\"\\n\"+\"pretrain: \"+str(pretrain_letter)+\"\\n\"+ \\\n",
        "    (\"no synth\" if not synth else \"with synth data\")+ \\\n",
        "    \"\\n\"+\"dropout:\"+str(keep_percent) +\"\\n\"\n",
        "print(description)\n",
        "\n",
        "#LOG Stats\n",
        "losses=[]\n",
        "test_losses=[]\n",
        "accuracys=[]\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer()  #need to reset oprimazer between trains????\n",
        "\n",
        "model=rebuild()\n",
        "#optimizer = tf.train.RMSPropOptimizer(0.001)\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(0.001)\n",
        "if pretrain_letter>0:\n",
        "  #optimizer = tf.train.AdamOptimizer() #adam is a bit better for this #didn't work\n",
        "  print(\"PRETRAIN\")\n",
        "  pretrain_letters(pretrain_letter,BATCH_SIZE)\n",
        "  print('-'*200)\n",
        "\n",
        "print(\"START TRAIN\")\n",
        "#optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "\n",
        "#dataset_double_synt=gen_dropout(keep_percent)\n",
        "for jjj in range(10): #after each of this iterations - send mail and calc full test\n",
        "  for i in range(5): #iter without sendmail and only partial test\n",
        "    if synth:\n",
        "      dataset_double_synt=gen_dropout_all(keep_percent)\n",
        "      loss=train_loop(dataset_double_synt,stop_loop=350)\n",
        "    else:\n",
        "      loss=train_loop(dataset_double)\n",
        "    #total_test_loss,total_accuracy=test_loss(single_words_test_dataset,limit=5)\n",
        "    \n",
        "    #total_test_loss,total_accuracy=test_loss(limit=5)\n",
        "    #test_loss(this_dataset=test_dataset_double1,limit=5)\n",
        "\n",
        "    #losses.append(loss)\n",
        "    #test_losses.append(total_test_loss)\n",
        "    #accuracys.append(total_accuracy)\n",
        "\n",
        "\n",
        "    # print ('Epoch {} Loss {:.4f} Test Loss {:.4f} accuracy {:.4f}' \\\n",
        "    #        .format(GLOBAL_epoch, loss, total_test_loss,total_accuracy))    \n",
        "       \n",
        "    print('-'*200)\n",
        "    test_kfir(kfir_kuzari_test,True)\n",
        "    #test_kfir(kfir_kuzari_test)\n",
        "    test_kfir(kfir_rasag_test,True)\n",
        "    #test_kfir(kfir_rasag_test)\n",
        "    print('-'*200)\n",
        "  print('FULL STATISTICS')\n",
        "  print('='*200)\n",
        "  test_kfir(kfir_kuzari_test,True,True)\n",
        "  #test_kfir(kfir_kuzari_test)\n",
        "  test_kfir(kfir_rasag_test,True,True)\n",
        "  #test_kfir(kfir_rasag_test)\n",
        "  #TESTING ALL EVERY OUTER LOOP \n",
        "  all_test_loss,all_accuracy=test_loss()\n",
        "  #shuffle_loss,shuffle_accuracy=test_shuffle()\n",
        "  all_test_loss1,all_accuracy1=test_loss(test_dataset_double1)  #HAEMUNOT VEHADEOT\n",
        "  #shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)\n",
        "  \n",
        "  \n",
        "  guide_result=test_guide()\n",
        "  \n",
        "  #TODO SAVE CHECKPOINT\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "\n",
        "#  my_plot_save(losses,\"train.png\",decor='r--')\n",
        "#  my_plot_save(test_losses,\"test.png\",decor='b-')\n",
        "#  my_plot_save(accuracys,\"accuracys.png\",decor='g-')\n",
        "  \n",
        "  print(\"full test: loss \",all_test_loss,\" accuracy \",all_accuracy)\n",
        "  print(\"full test (HAEMUNOT): loss \",all_test_loss1,\" accuracy \",all_accuracy1)\n",
        "\n",
        " # print(\"shuffle test (HAEMUNOT): loss \",shuffle_loss1,\" accuracy \",shuffle_accuracy1)\n",
        "  print('='*200)\n",
        "  print('CONTINUE TRAINING')\n",
        "\n",
        "  f= open(\"my_log.txt\",\"w+\")\n",
        "  # for l,t,a in zip(losses,test_losses,accuracys):\n",
        "  #   print(l,t,a)\n",
        "  #   f.write(\"%.3f %.3f %.6f\\r\\n\" % (l,t,a))\n",
        "  f.write(\"full test: loss %.6f accuracy %.6f\\r\\n\" % (all_test_loss,all_accuracy))\n",
        "  f.write(\"full test (HAEMUNOT): loss  %.6f accuracy %.6f\\r\\n\" % (all_test_loss1,all_accuracy1))\n",
        "  #f.write(\"shuffle test (HAEMUNOT): loss %.6f accuracy %.6f\\r\\n\" % (shuffle_loss1,shuffle_accuracy1))\n",
        "  \n",
        "  f.close()\n",
        "\n",
        "\n",
        "\n",
        "  #[(l,t,a) for l,t,a in zip(losses,test_losses,accuracys)]\n",
        "  send_results(mail_subject,str(all_accuracy)+'\\n'+str(all_accuracy1)+description+'\\n\\n'+guide_result)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwpwDinoeIu5",
        "colab_type": "text"
      },
      "source": [
        "#MAIN OUTPUT (ABOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdj5RlXokVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_loss()\n",
        "#test_loss(test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFG_Z9-HjEVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_guide()\n",
        "# test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4qfAwZhuDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXh1zFNmDjX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   train_loop()\n",
        "#   test_loss(single_words_test_dataset,limit=5)\n",
        "#   test_loss(limit=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQWJJJiGMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # test_loss(only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ukMRiAym05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjO2JYZhw02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle_loss,shuffle_accuracy=test_shuffle()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrgk68_hwBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  shuffle_loss1,shuffle_accuracy1=test_shuffle(shuffle_test_dataset_double1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVg68eKuh2eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss(this_dataset=test_dataset_double1,only_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxC8ziVJuTe",
        "colab_type": "text"
      },
      "source": [
        "test_guide(limit=3)TESTTtttt#TODO\n",
        "\n",
        "\n",
        "1.   varied length for data - to makes the system more robust for sentneces with different lengths. can do this with SENTENCE_LIMIT=20 set to random limit when sentences length exceedes current limit\n",
        "\n",
        "2.   abstraction for the testing functions (see comparesment in notpad++)\n",
        "\n",
        "3.   try TPU\n",
        "\n",
        "4.   new idea: input - arab baseline. train network to correct it\n",
        "\n",
        "5.    predict only middle word. input (1 true arab words) - (2 arab baseline word) - (3 true arab words) output - the middle word in corrected arab.\n",
        "\n",
        "or calc results only on middle word(s)\n",
        "\n",
        "6.   transformer (see tf tutorial)\n",
        "\n",
        "7.    NEW AND INTERESTING!!!!!: add space to each line at start and at end\n",
        "so the network knows this is end of word!\n",
        "\n",
        "\n"
      ]
    }
  ]
}